[
  {
    "number": 2,
    "filename": "adr-002-docs-structure.md",
    "title": "ADR 002: SDK Documentation Structure",
    "content": "# ADR 002: SDK Documentation Structure\n\n## Context\n\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\n\nIdeally, we would have:\n\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\n\n## Decision\n\nRe-structure the `/docs` folder of the Cosmos SDK github repo as follows:\n\n```text\ndocs/\n├── README\n├── intro/\n├── concepts/\n│   ├── baseapp\n│   ├── types\n│   ├── store\n│   ├── server\n│   ├── modules/\n│   │   ├── keeper\n│   │   ├── handler\n│   │   ├── cli\n│   ├── gas\n│   └── commands\n├── clients/\n│   ├── lite/\n│   ├── service-providers\n├── modules/\n├── spec/\n├── translations/\n└── architecture/\n```\n\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\n\n* `README`: Landing page of the docs.\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https://github.com/cosmos/sdk-application-tutorial/) will be highlighted, as well as the `godocs`.\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\n* `spec`: Contains specs of modules, and others.\n* `modules`: Contains links to `godocs` and the spec of the modules.\n* `architecture`: Contains architecture-related docs like the present one.\n* `translations`: Contains different translations of the documentation.\n\nWebsite docs sidebar will only include the following sections:\n\n* `README`\n* `intro`\n* `concepts`\n* `clients`\n\n`architecture` need not be displayed on the website.\n\n## Status\n\nAccepted\n\n## Consequences\n\n### Positive\n\n* Much clearer organisation of the Cosmos SDK docs.\n* The `/docs` folder now only contains Cosmos SDK and gaia related material. Later, it will only contain Cosmos SDK related material.\n* Developers only have to update `/docs` folder when they open a PR (and not `/examples` for example).\n* Easier for developers to find what they need to update in the docs thanks to reworked architecture.\n* Cleaner vuepress build for website docs.\n* Will help build an executable doc (cf https://github.com/cosmos/cosmos-sdk/issues/2611)\n\n### Neutral\n\n* We need to move a bunch of deprecated stuff to `/_attic` folder.\n* We need to integrate content in `docs/sdk/docs/core` in `concepts`.\n* We need to move all the content that currently lives in `docs` and does not fit in new structure (like `lotion`, intro material, whitepaper) to the website repository.\n* Update `DOCS_README.md`\n\n## References\n\n* https://github.com/cosmos/cosmos-sdk/issues/1460\n* https://github.com/cosmos/cosmos-sdk/pull/2695\n* https://github.com/cosmos/cosmos-sdk/issues/2611"
  },
  {
    "number": 3,
    "filename": "adr-003-dynamic-capability-store.md",
    "title": "ADR 3: Dynamic Capability Store",
    "content": "# ADR 3: Dynamic Capability Store\n\n## Changelog\n\n* 12 December 2019: Initial version\n* 02 April 2020: Memory Store Revisions\n\n## Context\n\nFull implementation of the [IBC specification](https://github.com/cosmos/ibc) requires the ability to create and authenticate object-capability keys at runtime (i.e., during transaction execution),\nas described in [ICS 5](https://github.com/cosmos/ibc/tree/master/spec/core/ics-005-port-allocation#technical-specification). In the IBC specification, capability keys are created for each newly initialised\nport & channel, and are used to authenticate future usage of the port or channel. Since channels and potentially ports can be initialised during transaction execution, the state machine must be able to create\nobject-capability keys at this time.\n\nAt present, the Cosmos SDK does not have the ability to do this. Object-capability keys are currently pointers (memory addresses) of `StoreKey` structs created at application initialisation in `app.go` ([example](https://github.com/cosmos/gaia/blob/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4/app/app.go#L132))\nand passed to Keepers as fixed arguments ([example](https://github.com/cosmos/gaia/blob/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4/app/app.go#L160)). Keepers cannot create or store capability keys during transaction execution — although they could call `NewKVStoreKey` and take the memory address\nof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional — were this not the case, the keys would be predictable and couldn't serve as object capabilities).\n\nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.\nThis ADR proposes such an interface & mechanism.\n\n## Decision\n\nThe Cosmos SDK will include a new `CapabilityKeeper` abstraction, which is responsible for provisioning,\ntracking, and authenticating capabilities at runtime. During application initialisation in `app.go`,\nthe `CapabilityKeeper` will be hooked up to modules through unique function references\n(by calling `ScopeToModule`, defined below) so that it can identify the calling module when later\ninvoked.\n\nWhen the initial state is loaded from disk, the `CapabilityKeeper`'s `Initialise` function will create\nnew capability keys for all previously allocated capability identifiers (allocated during execution of\npast transactions and assigned to particular modes), and keep them in a memory-only store while the\nchain is running.\n\nThe `CapabilityKeeper` will include a persistent `KVStore`, a `MemoryStore`, and an in-memory map.\nThe persistent `KVStore` tracks which capability is owned by which modules.\nThe `MemoryStore` stores a forward mapping that map from module name, capability tuples to capability names and\na reverse mapping that map from module name, capability name to the capability index.\nSince we cannot marshal the capability into a `KVStore` and unmarshal without changing the memory location of the capability,\nthe reverse mapping in the KVStore will simply map to an index. This index can then be used as a key in the ephemeral\ngo-map to retrieve the capability at the original memory location.\n\nThe `CapabilityKeeper` will define the following types & functions:\n\nThe `Capability` is similar to `StoreKey`, but has a globally unique `Index()` instead of\na name. A `String()` method is provided for debugging.\n\nA `Capability` is simply a struct, the address of which is taken for the actual capability.\n\n```go\ntype Capability struct {\n  index uint64\n}\n```\n\nA `CapabilityKeeper` contains a persistent store key, memory store key, and mapping of allocated module names.\n\n```go\ntype CapabilityKeeper struct {\n  persistentKey StoreKey\n  memKey        StoreKey\n  capMap        map[uint64]*Capability\n  moduleNames   map[string]interface{}\n  sealed        bool\n}\n```\n\nThe `CapabilityKeeper` provides the ability to create *scoped* sub-keepers which are tied to a\nparticular module name. These `ScopedCapabilityKeeper`s must be created at application initialisation\nand passed to modules, which can then use them to claim capabilities they receive and retrieve\ncapabilities which they own by name, in addition to creating new capabilities & authenticating capabilities\npassed by other modules.\n\n```go\ntype ScopedCapabilityKeeper struct {\n  persistentKey StoreKey\n  memKey        StoreKey\n  capMap        map[uint64]*Capability\n  moduleName    string\n}\n```\n\n`ScopeToModule` is used to create a scoped sub-keeper with a particular name, which must be unique.\nIt MUST be called before `InitialiseAndSeal`.\n\n```go\nfunc (ck CapabilityKeeper) ScopeToModule(moduleName string) ScopedCapabilityKeeper {\n\tif k.sealed {\n\t\tpanic(\"cannot scope to module via a sealed capability keeper\")\n\t}\n\n\tif _, ok := k.scopedModules[moduleName]; ok {\n\t\tpanic(fmt.Sprintf(\"cannot create multiple scoped keepers for the same module name: %s\", moduleName))\n\t}\n\n\tk.scopedModules[moduleName] = struct{}{}\n\n\treturn ScopedKeeper{\n\t\tcdc:      k.cdc,\n\t\tstoreKey: k.storeKey,\n\t\tmemKey:   k.memKey,\n\t\tcapMap:   k.capMap,\n\t\tmodule:   moduleName,\n\t}\n}\n```\n\n`InitialiseAndSeal` MUST be called exactly once, after loading the initial state and creating all\nnecessary `ScopedCapabilityKeeper`s, in order to populate the memory store with newly-created\ncapability keys in accordance with the keys previously claimed by particular modules and prevent the\ncreation of any new `ScopedCapabilityKeeper`s.\n\n```go\nfunc (ck CapabilityKeeper) InitialiseAndSeal(ctx Context) {\n  if ck.sealed {\n    panic(\"capability keeper is sealed\")\n  }\n\n  persistentStore := ctx.KVStore(ck.persistentKey)\n  map := ctx.KVStore(ck.memKey)\n  \n  // initialise memory store for all names in persistent store\n  for index, value := range persistentStore.Iter() {\n    capability = &CapabilityKey{index: index}\n\n    for moduleAndCapability := range value {\n      moduleName, capabilityName := moduleAndCapability.Split(\"/\")\n      memStore.Set(moduleName + \"/fwd/\" + capability, capabilityName)\n      memStore.Set(moduleName + \"/rev/\" + capabilityName, index)\n\n      ck.capMap[index] = capability\n    }\n  }\n\n  ck.sealed = true\n}\n```\n\n`NewCapability` can be called by any module to create a new unique, unforgeable object-capability\nreference. The newly created capability is automatically persisted; the calling module need not\ncall `ClaimCapability`.\n\n```go\nfunc (sck ScopedCapabilityKeeper) NewCapability(ctx Context, name string) (Capability, error) {\n  // check name not taken in memory store\n  if capStore.Get(\"rev/\" + name) != nil {\n    return nil, errors.New(\"name already taken\")\n  }\n\n  // fetch the current index\n  index := persistentStore.Get(\"index\")\n  \n  // create a new capability\n  capability := &CapabilityKey{index: index}\n  \n  // set persistent store\n  persistentStore.Set(index, Set.singleton(sck.moduleName + \"/\" + name))\n  \n  // update the index\n  index++\n  persistentStore.Set(\"index\", index)\n  \n  // set forward mapping in memory store from capability to name\n  memStore.Set(sck.moduleName + \"/fwd/\" + capability, name)\n  \n  // set reverse mapping in memory store from name to index\n  memStore.Set(sck.moduleName + \"/rev/\" + name, index)\n\n  // set the in-memory mapping from index to capability pointer\n  capMap[index] = capability\n  \n  // return the newly created capability\n  return capability\n}\n```\n\n`AuthenticateCapability` can be called by any module to check that a capability\ndoes in fact correspond to a particular name (the name can be untrusted user input)\nwith which the calling module previously associated it.\n\n```go\nfunc (sck ScopedCapabilityKeeper) AuthenticateCapability(name string, capability Capability) bool {\n  // return whether forward mapping in memory store matches name\n  return memStore.Get(sck.moduleName + \"/fwd/\" + capability) === name\n}\n```\n\n`ClaimCapability` allows a module to claim a capability key which it has received from another module\nso that future `GetCapability` calls will succeed.\n\n`ClaimCapability` MUST be called if a module which receives a capability wishes to access it by name\nin the future. Capabilities are multi-owner, so if multiple modules have a single `Capability` reference,\nthey will all own it.\n\n```go\nfunc (sck ScopedCapabilityKeeper) ClaimCapability(ctx Context, capability Capability, name string) error {\n  persistentStore := ctx.KVStore(sck.persistentKey)\n\n  // set forward mapping in memory store from capability to name\n  memStore.Set(sck.moduleName + \"/fwd/\" + capability, name)\n\n  // set reverse mapping in memory store from name to capability\n  memStore.Set(sck.moduleName + \"/rev/\" + name, capability)\n\n  // update owner set in persistent store\n  owners := persistentStore.Get(capability.Index())\n  owners.add(sck.moduleName + \"/\" + name)\n  persistentStore.Set(capability.Index(), owners)\n}\n```\n\n`GetCapability` allows a module to fetch a capability which it has previously claimed by name.\nThe module is not allowed to retrieve capabilities which it does not own.\n\n```go\nfunc (sck ScopedCapabilityKeeper) GetCapability(ctx Context, name string) (Capability, error) {\n  // fetch the index of capability using reverse mapping in memstore\n  index := memStore.Get(sck.moduleName + \"/rev/\" + name)\n\n  // fetch capability from go-map using index\n  capability := capMap[index]\n\n  // return the capability\n  return capability\n}\n```\n\n`ReleaseCapability` allows a module to release a capability which it had previously claimed. If no\nmore owners exist, the capability will be deleted globally.\n\n```go\nfunc (sck ScopedCapabilityKeeper) ReleaseCapability(ctx Context, capability Capability) err {\n  persistentStore := ctx.KVStore(sck.persistentKey)\n\n  name := capStore.Get(sck.moduleName + \"/fwd/\" + capability)\n  if name == nil {\n    return error(\"capability not owned by module\")\n  }\n\n  // delete forward mapping in memory store\n  memoryStore.Delete(sck.moduleName + \"/fwd/\" + capability, name)\n\n  // delete reverse mapping in memory store\n  memoryStore.Delete(sck.moduleName + \"/rev/\" + name, capability)\n\n  // update owner set in persistent store\n  owners := persistentStore.Get(capability.Index())\n  owners.remove(sck.moduleName + \"/\" + name)\n  if owners.size() > 0 {\n    // there are still other owners, keep the capability around\n    persistentStore.Set(capability.Index(), owners)\n  } else {\n    // no more owners, delete the capability\n    persistentStore.Delete(capability.Index())\n    delete(capMap[capability.Index()])\n  }\n}\n```\n\n### Usage patterns\n\n#### Initialisation\n\nAny modules which use dynamic capabilities must be provided a `ScopedCapabilityKeeper` in `app.go`:\n\n```go\nck := NewCapabilityKeeper(persistentKey, memoryKey)\nmod1Keeper := NewMod1Keeper(ck.ScopeToModule(\"mod1\"), ....)\nmod2Keeper := NewMod2Keeper(ck.ScopeToModule(\"mod2\"), ....)\n\n// other initialisation logic ...\n\n// load initial state...\n\nck.InitialiseAndSeal(initialContext)\n```\n\n#### Creating, passing, claiming and using capabilities\n\nConsider the case where `mod1` wants to create a capability, associate it with a resource (e.g. an IBC channel) by name, then pass it to `mod2` which will use it later:\n\nModule 1 would have the following code:\n\n```go\ncapability := scopedCapabilityKeeper.NewCapability(ctx, \"resourceABC\")\nmod2Keeper.SomeFunction(ctx, capability, args...)\n```\n\n`SomeFunction`, running in module 2, could then claim the capability:\n\n```go\nfunc (k Mod2Keeper) SomeFunction(ctx Context, capability Capability) {\n  k.sck.ClaimCapability(ctx, capability, \"resourceABC\")\n  // other logic...\n}\n```\n\nLater on, module 2 can retrieve that capability by name and pass it to module 1, which will authenticate it against the resource:\n\n```go\nfunc (k Mod2Keeper) SomeOtherFunction(ctx Context, name string) {\n  capability := k.sck.GetCapability(ctx, name)\n  mod1.UseResource(ctx, capability, \"resourceABC\")\n}\n```\n\nModule 1 will then check that this capability key is authenticated to use the resource before allowing module 2 to use it:\n\n```go\nfunc (k Mod1Keeper) UseResource(ctx Context, capability Capability, resource string) {\n  if !k.sck.AuthenticateCapability(name, capability) {\n    return errors.New(\"unauthenticated\")\n  }\n  // do something with the resource\n}\n```\n\nIf module 2 passed the capability key to module 3, module 3 could then claim it and call module 1 just like module 2 did\n(in which case module 1, module 2, and module 3 would all be able to use this capability).\n\n## Status\n\nProposed.\n\n## Consequences\n\n### Positive\n\n* Dynamic capability support.\n* Allows CapabilityKeeper to return same capability pointer from go-map while reverting any writes to the persistent `KVStore` and in-memory `MemoryStore` on tx failure.\n\n### Negative\n\n* Requires an additional keeper.\n* Some overlap with existing `StoreKey` system (in the future they could be combined, since this is a superset functionality-wise).\n* Requires an extra level of indirection in the reverse mapping, since MemoryStore must map to index which must then be used as key in a go map to retrieve the actual capability\n\n### Neutral\n\n(none known)\n\n## References\n\n* [Original discussion](https://github.com/cosmos/cosmos-sdk/pull/5230#discussion_r343978513)"
  },
  {
    "number": 4,
    "filename": "adr-004-split-denomination-keys.md",
    "title": "ADR 004: Split Denomination Keys",
    "content": "# ADR 004: Split Denomination Keys\n\n## Changelog\n\n* 2020-01-08: Initial version\n* 2020-01-09: Alterations to handle vesting accounts\n* 2020-01-14: Updates from review feedback\n* 2020-01-30: Updates from implementation\n\n### Glossary\n\n* denom / denomination key -- unique token identifier.\n\n## Context\n\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https://github.com/cosmos/cosmos-sdk/issues/5467) and [4982](https://github.com/cosmos/cosmos-sdk/issues/4982) for additional context.\n\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\n\n## Decision\n\nBalances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\n\n### Account interface (x/auth)\n\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\nnow be stored in & managed by the bank module.\n\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\naccount balance.\n\nVesting accounts will continue to store original vesting, delegated free, and delegated\nvesting coins (which is safe since these cannot contain arbitrary denominations).\n\n### Bank keeper (x/bank)\n\nThe following APIs will be added to the `x/bank` keeper:\n\n* `GetAllBalances(ctx Context, addr AccAddress) Coins`\n* `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\n* `SetBalance(ctx Context, addr AccAddress, coin Coin)`\n* `LockedCoins(ctx Context, addr AccAddress) Coins`\n* `SpendableCoins(ctx Context, addr AccAddress) Coins`\n\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\ncore functionality or persistence.\n\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\nbut retrieval of all balances for a single account is presumed to be more frequent):\n\n```go\nvar BalancesPrefix = []byte(\"balances\")\n\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\n  if !balance.IsValid() {\n    return err\n  }\n\n  store := ctx.KVStore(k.storeKey)\n  balancesStore := prefix.NewStore(store, BalancesPrefix)\n  accountStore := prefix.NewStore(balancesStore, addr.Bytes())\n\n  bz := Marshal(balance)\n  accountStore.Set([]byte(balance.Denom), bz)\n\n  return nil\n}\n```\n\nThis will result in the balances being indexed by the byte representation of\n`balances/{address}/{denom}`.\n\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\naccount balance by denomination found in the (un)delegation amount. As a result,\nany mutations to the account balance will be made by denomination.\n\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\ndirectly instead of calling `GetCoins()` / `SetCoins()` (which no longer exist).\n\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\naccount balances.\n\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\npossible as to not load the entire account balance.\n\n### Supply module\n\nThe supply module, in order to implement the total supply invariant, will now need\nto scan all accounts & call `GetAllBalances` using the `x/bank` Keeper, then sum\nthe balances and check that they match the expected total supply.\n\n## Status\n\nAccepted.\n\n## Consequences\n\n### Positive\n\n* O(1) reads & writes of balances (with respect to the number of denominations for\nwhich an account has non-zero balances). Note, this does not relate to the actual\nI/O cost, rather the total number of direct reads needed.\n\n### Negative\n\n* Slightly less efficient reads/writes when reading & writing all balances of a\nsingle account in a transaction.\n\n### Neutral\n\nNone in particular.\n\n## References\n\n* Ref: https://github.com/cosmos/cosmos-sdk/issues/4982\n* Ref: https://github.com/cosmos/cosmos-sdk/issues/5467\n* Ref: https://github.com/cosmos/cosmos-sdk/issues/5492"
  },
  {
    "number": 6,
    "filename": "adr-006-secret-store-replacement.md",
    "title": "ADR 006: Secret Store Replacement",
    "content": "# ADR 006: Secret Store Replacement\n\n## Changelog\n\n* July 29th, 2019: Initial draft\n* September 11th, 2019: Work has started\n* November 4th: Cosmos SDK changes merged in\n* November 18th: Gaia changes merged in\n\n## Context\n\nCurrently, a Cosmos SDK application's CLI directory stores key material and metadata in a plain text database in the user’s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\n\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user/computer.\n\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\n\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don’t provide a native secret store.\n\n## Decision\n\nWe recommend replacing the current Keybase backend based on LevelDB with [Keyring](https://github.com/99designs/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\n\nThis appears to fulfill the requirement of protecting both key material and metadata from rogue software on a user’s machine.\n\n## Status\n\nAccepted\n\n## Consequences\n\n### Positive\n\nIncreased safety for users.\n\n### Negative\n\nUsers must manually migrate.\n\nTesting against all supported backends is difficult.\n\nRunning tests locally on a Mac require numerous repetitive password entries.\n\n### Neutral\n\n{neutral consequences}\n\n## References\n\n* #4754 Switch secret store to the keyring secret store (original PR by @poldsam) [__CLOSED__]\n* #5029 Add support for github.com/99designs/keyring-backed keybases [__MERGED__]\n* #5097 Add keys migrate command [__MERGED__]\n* #5180 Drop on-disk keybase in favor of keyring [_PENDING_REVIEW_]\n* cosmos/gaia#164 Drop on-disk keybase in favor of keyring (gaia's changes) [_PENDING_REVIEW_]"
  },
  {
    "number": 7,
    "filename": "adr-007-specialization-groups.md",
    "title": "ADR 007: Specialization Groups",
    "content": "# ADR 007: Specialization Groups\n\n## Changelog\n\n* 2019 Jul 31: Initial Draft\n\n## Context\n\nThis idea was first conceived of in order to fulfill the use case of the\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\nmembers would be elected by a governing community and would fulfill the role of\ncoordinating the community under emergency situations. This thinking\ncan be further abstracted into the conception of \"blockchain specialization\ngroups\".\n\nThe creation of these groups are the beginning of specialization capabilities\nwithin a wider blockchain community which could be used to enable a certain\nlevel of delegated responsibilities. Examples of specialization which could be\nbeneficial to a blockchain community include: code auditing, emergency response,\ncode development etc. This type of community organization paves the way for\nindividual stakeholders to delegate votes by issue type, if in the future\ngovernance proposals include a field for issue type.\n\n## Decision\n\nA specialization group can be broadly broken down into the following functions\n(herein containing examples):\n\n* Membership Admittance\n* Membership Acceptance\n* Membership Revocation\n    * (probably) Without Penalty\n        * member steps down (self-Revocation)\n        * replaced by new member from governance\n    * (probably) With Penalty\n        * due to breach of soft-agreement (determined through governance)\n        * due to breach of hard-agreement (determined by code)\n* Execution of Duties\n    * Special transactions which only execute for members of a specialization\n     group (for example, dCERT members voting to turn off transaction routes in\n     an emergency scenario)\n* Compensation\n    * Group compensation (further distribution decided by the specialization group)\n    * Individual compensation for all constituents of a group from the\n     greater community\n\nMembership admittance to a specialization group could take place over a wide\nvariety of mechanisms. The most obvious example is through a general vote among\nthe entire community, however in certain systems a community may want to allow\nthe members already in a specialization group to internally elect new members,\nor maybe the community may assign a permission to a particular specialization\ngroup to appoint members to other 3rd party groups. The sky is really the limit\nas to how membership admittance can be structured. We attempt to capture\nsome of these possibilities in a common interface dubbed the `Electionator`. For\nits initial implementation as a part of this ADR we recommend that the general\nelection abstraction (`Electionator`) is provided as well as a basic\nimplementation of that abstraction which allows for a continuous election of\nmembers of a specialization group.\n\n``` golang\n// The Electionator abstraction covers the concept space for\n// a wide variety of election kinds.  \ntype Electionator interface {\n\n    // is the election object accepting votes.\n    Active() bool\n\n    // functionality to execute for when a vote is cast in this election, here\n    // the vote field is anticipated to be marshalled into a vote type used\n    // by an election.\n    //\n    // NOTE There are no explicit ids here. Just votes which pertain specifically\n    // to one electionator. Anyone can create and send a vote to the electionator item\n    // which will presumably attempt to marshal those bytes into a particular struct\n    // and apply the vote information in some arbitrary way. There can be multiple\n    // Electionators within the Cosmos-Hub for multiple specialization groups, votes\n    // would need to be routed to the Electionator upstream of here.\n    Vote(addr sdk.AccAddress, vote []byte)\n\n    // here lies all functionality to authenticate and execute changes for\n    // when a member accepts being elected\n    AcceptElection(sdk.AccAddress)\n\n    // Register a revoker object\n    RegisterRevoker(Revoker)\n\n    // No more revokers may be registered after this function is called\n    SealRevokers()\n\n    // register hooks to call when an election actions occur\n    RegisterHooks(ElectionatorHooks)\n\n    // query for the current winner(s) of this election based on arbitrary\n    // election ruleset\n    QueryElected() []sdk.AccAddress\n\n    // query metadata for an address in the election this\n    // could include for example position that an address\n    // is being elected for within a group\n    //\n    // this metadata may be directly related to\n    // voting information and/or privileges enabled\n    // to members within a group.\n    QueryMetadata(sdk.AccAddress) []byte\n}\n\n// ElectionatorHooks, once registered with an Electionator,\n// trigger execution of relevant interface functions when\n// Electionator events occur.\ntype ElectionatorHooks interface {\n    AfterVoteCast(addr sdk.AccAddress, vote []byte)\n    AfterMemberAccepted(addr sdk.AccAddress)\n    AfterMemberRevoked(addr sdk.AccAddress, cause []byte)\n}\n\n// Revoker defines the function required for a membership revocation rule-set\n// used by a specialization group. This could be used to create self revoking,\n// and evidence based revoking, etc. Revokers types may be created and\n// reused for different election types.\n//\n// When revoking the \"cause\" bytes may be arbitrarily marshalled into evidence,\n// memos, etc.\ntype Revoker interface {\n    RevokeName() string      // identifier for this revoker type\n    RevokeMember(addr sdk.AccAddress, cause []byte) error\n}\n```\n\nCertain level of commonality likely exists between the existing code within\n`x/governance` and required functionality of elections. This common\nfunctionality should be abstracted during implementation. Similarly for each\nvote implementation client CLI/REST functionality should be abstracted\nto be reused for multiple elections.\n\nThe specialization group abstraction firstly extends the `Electionator`\nbut also further defines traits of the group.\n\n``` golang\ntype SpecializationGroup interface {\n    Electionator\n    GetName() string\n    GetDescription() string\n\n    // general soft contract the group is expected\n    // to fulfill with the greater community\n    GetContract() string\n\n    // messages which can be executed by the members of the group\n    Handler(ctx sdk.Context, msg sdk.Msg) sdk.Result\n\n    // logic to be executed at endblock, this may for instance\n    // include payment of a stipend to the group members\n    // for participation in the security group.\n    EndBlocker(ctx sdk.Context)\n}\n```\n\n## Status\n\n> Proposed\n\n## Consequences\n\n### Positive\n\n* increases specialization capabilities of a blockchain\n* improve abstractions in `x/gov/` such that they can be used with specialization groups\n\n### Negative\n\n* could be used to increase centralization within a community\n\n### Neutral\n\n## References\n\n* [dCERT ADR](./adr-008-dCERT-group.md)"
  },
  {
    "number": 8,
    "filename": "adr-008-dCERT-group.md",
    "title": "ADR 008: Decentralized Computer Emergency Response Team (dCERT) Group",
    "content": "# ADR 008: Decentralized Computer Emergency Response Team (dCERT) Group\n\n## Changelog\n\n* 2019 Jul 31: Initial Draft\n\n## Context\n\nIn order to reduce the number of parties involved with handling sensitive\ninformation in an emergency scenario, we propose the creation of a\nspecialization group named The Decentralized Computer Emergency Response Team\n(dCERT).  Initially this group's role is intended to serve as coordinators\nbetween various actors within a blockchain community such as validators,\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\naggregate and relay input from a variety of stakeholders to the developers who\nare actively devising a patch to the software, this way sensitive information\ndoes not need to be publicly disclosed while some input from the community can\nstill be gained.\n\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\nto \"circuit-break\" (aka. temporarily disable)  a particular message path. Note\nthat this privilege should be enabled/disabled globally with a governance\nparameter such that this privilege could start disabled and later be enabled\nthrough a parameter change proposal, once a dCERT group has been established.\n\nIn the future it is foreseeable that the community may wish to expand the roles\nof dCERT with further responsibilities such as the capacity to \"pre-approve\" a\nsecurity update on behalf of the community prior to a full community\nwide vote whereby the sensitive information would be revealed prior to a\nvulnerability being patched on the live network.  \n\n## Decision\n\nThe dCERT group is proposed to include an implementation of a `SpecializationGroup`\nas defined in [ADR 007](./adr-007-specialization-groups.md). This will include the\nimplementation of:\n\n* continuous voting\n* slashing due to breach of soft contract\n* revoking a member due to breach of soft contract\n* emergency disband of the entire dCERT group (ex. for colluding maliciously)\n* compensation stipend from the community pool or other means decided by\n   governance\n\nThis system necessitates the following new parameters:\n\n* blockly stipend allowance per dCERT member\n* maximum number of dCERT members\n* required staked slashable tokens for each dCERT member\n* quorum for suspending a particular member\n* proposal wager for disbanding the dCERT group\n* stabilization period for dCERT member transition\n* circuit break dCERT privileges enabled\n\nThese parameters are expected to be implemented through the param keeper such\nthat governance may change them at any given point.\n\n### Continuous Voting Electionator\n\nAn `Electionator` object is to be implemented as continuous voting and with the\nfollowing specifications:\n\n* All delegation addresses may submit votes at any point which updates their\n   preferred representation on the dCERT group.\n* Preferred representation may be arbitrarily split between addresses (ex. 50%\n   to John, 25% to Sally, 25% to Carol)\n* In order for a new member to be added to the dCERT group they must\n   send a transaction accepting their admission at which point the validity of\n   their admission is to be confirmed.\n    * A sequence number is assigned when a member is added to dCERT group.\n     If a member leaves the dCERT group and then enters back, a new sequence number\n     is assigned.  \n* Addresses which control the greatest amount of preferred-representation are\n   eligible to join the dCERT group (up the _maximum number of dCERT members_).\n   If the dCERT group is already full and new member is admitted, the existing\n   dCERT member with the lowest amount of votes is kicked from the dCERT group.\n    * In the split situation where the dCERT group is full but a vying candidate\n     has the same amount of vote as an existing dCERT member, the existing\n     member should maintain its position.\n    * In the split situation where somebody must be kicked out but the two\n     addresses with the smallest number of votes have the same number of votes,\n     the address with the smallest sequence number maintains its position.  \n* A stabilization period can be optionally included to reduce the\n   \"flip-flopping\" of the dCERT membership tail members. If a stabilization\n   period is provided which is greater than 0, when members are kicked due to\n   insufficient support, a queue entry is created which documents which member is\n   to replace which other member. While this entry is in the queue, no new entries\n   to kick that same dCERT member can be made. When the entry matures at the\n   duration of the  stabilization period, the new member is instantiated, and old\n   member kicked.\n\n### Staking/Slashing\n\nAll members of the dCERT group must stake tokens _specifically_ to maintain\neligibility as a dCERT member. These tokens can be staked directly by the vying\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\nbenefits for doing so). This staking mechanism should use the existing global\nunbonding time of tokens staked for network validator security. A dCERT member\ncan _only be_ a member if it has the required tokens staked under this\nmechanism. If those tokens are unbonded then the dCERT member must be\nautomatically kicked from the group.  \n\nSlashing of a particular dCERT member due to soft-contract breach should be\nperformed by governance on a per member basis based on the magnitude of the\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\nby the dCERT group prior to being slashed by governance.  \n\nMembership suspension by the dCERT group takes place through a voting procedure\nby the dCERT group members. After this suspension has taken place, a governance\nproposal to slash the dCERT member must be submitted, if the proposal is not\napproved by the time the rescinding member has completed unbonding their\ntokens, then the tokens are no longer staked and unable to be slashed.\n\nAdditionally in the case of an emergency situation of a colluding and malicious\ndCERT group, the community needs the capability to disband the entire dCERT\ngroup and likely fully slash them. This could be achieved though a special new\nproposal type (implemented as a general governance proposal) which would halt\nthe functionality of the dCERT group until the proposal was concluded. This\nspecial proposal type would likely need to also have a fairly large wager which\ncould be slashed if the proposal creator was malicious. The reason a large\nwager should be required is because as soon as the proposal is made, the\ncapability of the dCERT group to halt message routes is put on temporarily\nsuspended, meaning that a malicious actor who created such a proposal could\nthen potentially exploit a bug during this period of time, with no dCERT group\ncapable of shutting down the exploitable message routes.\n\n### dCERT membership transactions\n\nActive dCERT members\n\n* change of the description of the dCERT group\n* circuit break a message route\n* vote to suspend a dCERT member.\n\nHere circuit-breaking refers to the capability to disable a groups of messages,\nThis could for instance mean: \"disable all staking-delegation messages\", or\n\"disable all distribution messages\". This could be accomplished by verifying\nthat the message route has not been \"circuit-broken\" at CheckTx time (in\n`baseapp/baseapp.go`).\n\n\"unbreaking\" a circuit is anticipated only to occur during a hard fork upgrade\nmeaning that no capability to unbreak a message route on a live chain is\nrequired.\n\nNote also, that if there was a problem with governance voting (for instance a\ncapability to vote many times) then governance would be broken and should be\nhalted with this mechanism, it would be then up to the validator set to\ncoordinate and hard-fork upgrade to a patched version of the software where\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\nthey should all be severely slashed.\n\n## Status\n\nProposed\n\n## Consequences\n\n### Positive\n\n* Potential to reduces the number of parties to coordinate with during an emergency\n* Reduction in possibility of disclosing sensitive information to malicious parties\n\n### Negative\n\n* Centralization risks\n\n### Neutral\n\n## References\n\n  [Specialization Groups ADR](./adr-007-specialization-groups.md)"
  },
  {
    "number": 9,
    "filename": "adr-009-evidence-module.md",
    "title": "ADR 009: Evidence Module",
    "content": "# ADR 009: Evidence Module\n\n## Changelog\n\n* 2019 July 31: Initial draft\n* 2019 October 24: Initial implementation\n\n## Status\n\nAccepted\n\n## Context\n\nIn order to support building highly secure, robust and interoperable blockchain\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\nevidence can be submitted, evaluated and verified resulting in some agreed upon\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\nFurthermore, such a mechanism is paramount for any\n[IBC](https://github.com/cosmos/ics/blob/master/ibc/2_IBC_ARCHITECTURE.md) or\ncross-chain validation protocol implementation in order to support the ability\nfor any misbehavior to be relayed back from a collateralized chain to a primary\nchain so that the equivocating validator(s) can be slashed.\n\n## Decision\n\nWe will implement an evidence module in the Cosmos SDK supporting the following\nfunctionality:\n\n* Provide developers with the abstractions and interfaces necessary to define\n  custom evidence messages, message handlers, and methods to slash and penalize\n  accordingly for misbehavior.\n* Support the ability to route evidence messages to handlers in any module to\n  determine the validity of submitted misbehavior.\n* Support the ability, through governance, to modify slashing penalties of any\n  evidence type.\n* Querier implementation to support querying params, evidence types, params, and\n  all submitted valid misbehavior.\n\n### Types\n\nFirst, we define the `Evidence` interface type. The `x/evidence` module may implement\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\nIn addition, other modules may implement their own `Evidence` types in a similar\nmanner in which governance is extensible. It is important to note any concrete\ntype implementing the `Evidence` interface may include arbitrary fields such as\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\n\nWhen submitting evidence to the `x/evidence` module, the concrete type must provide\nthe validator's consensus address, which should be known by the `x/slashing`\nmodule (assuming the infraction is valid), the height at which the infraction\noccurred and the validator's power at same height in which the infraction occurred.\n\n```go\ntype Evidence interface {\n  Route() string\n  Type() string\n  String() string\n  Hash() HexBytes\n  ValidateBasic() error\n\n  // The consensus address of the malicious validator at time of infraction\n  GetConsensusAddress() ConsAddress\n\n  // Height at which the infraction occurred\n  GetHeight() int64\n\n  // The total power of the malicious validator at time of infraction\n  GetValidatorPower() int64\n\n  // The total validator set power at time of infraction\n  GetTotalPower() int64\n}\n```\n\n### Routing & Handling\n\nEach `Evidence` type must map to a specific unique route and be registered with\nthe `x/evidence` module. It accomplishes this through the `Router` implementation.\n\n```go\ntype Router interface {\n  AddRoute(r string, h Handler) Router\n  HasRoute(r string) bool\n  GetRoute(path string) Handler\n  Seal()\n}\n```\n\nUpon successful routing through the `x/evidence` module, the `Evidence` type\nis passed through a `Handler`. This `Handler` is responsible for executing all\ncorresponding business logic necessary for verifying the evidence as valid. In\naddition, the `Handler` may execute any necessary slashing and potential jailing.\nSince slashing fractions will typically result from some form of static functions,\nallow the `Handler` to do this provides the greatest flexibility. An example could\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\nby governance. The `Evidence` type should provide all the external information\nnecessary in order for the `Handler` to make the necessary state transitions.\nIf no error is returned, the `Evidence` is considered valid.\n\n```go\ntype Handler func(Context, Evidence) error\n```\n\n### Submission\n\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\nhandled by the `x/evidence` module's `SubmitEvidence`.\n\n```go\ntype MsgSubmitEvidence struct {\n  Evidence\n}\n\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\n  if err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\n    return err.Result()\n  }\n\n  // emit events...\n\n  return Result{\n    // ...\n  }\n}\n```\n\nThe `x/evidence` module's keeper is responsible for matching the `Evidence` against\nthe module's router and invoking the corresponding `Handler` which may include\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\n\n```go\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\n  handler := keeper.router.GetRoute(evidence.Route())\n  if err := handler(ctx, evidence); err != nil {\n    return ErrInvalidEvidence(keeper.codespace, err)\n  }\n\n  keeper.setEvidence(ctx, evidence)\n  return nil\n}\n```\n\n### Genesis\n\nFinally, we need to represent the genesis state of the `x/evidence` module. The\nmodule only needs a list of all submitted valid infractions and any necessary params\nfor which the module needs in order to handle submitted evidence. The `x/evidence`\nmodule will naturally define and route native evidence types for which it'll most\nlikely need slashing penalty constants for.\n\n```go\ntype GenesisState struct {\n  Params       Params\n  Infractions  []Evidence\n}\n```\n\n## Consequences\n\n### Positive\n\n* Allows the state machine to process misbehavior submitted on-chain and penalize\n  validators based on agreed upon slashing parameters.\n* Allows evidence types to be defined and handled by any module. This further allows\n  slashing and jailing to be defined by more complex mechanisms.\n* Does not solely rely on Tendermint to submit evidence.\n\n### Negative\n\n* No easy way to introduce new evidence types through governance on a live chain\n  due to the inability to introduce the new evidence type's corresponding handler\n\n### Neutral\n\n* Should we persist infractions indefinitely? Or should we rather rely on events?\n\n## References\n\n* [ICS](https://github.com/cosmos/ics)\n* [IBC Architecture](https://github.com/cosmos/ics/blob/master/ibc/1_IBC_ARCHITECTURE.md)\n* [Tendermint Fork Accountability](https://github.com/tendermint/spec/blob/7b3138e69490f410768d9b1ffc7a17abc23ea397/spec/consensus/fork-accountability.md)"
  },
  {
    "number": 10,
    "filename": "adr-010-modular-antehandler.md",
    "title": "ADR 010: Modular AnteHandler",
    "content": "# ADR 010: Modular AnteHandler\n\n## Changelog\n\n* 2019 Aug 31: Initial draft\n* 2021 Sep 14: Superseded by ADR-045\n\n## Status\n\nSUPERSEDED by ADR-045\n\n## Context\n\nThe current AnteHandler design allows users to either use the default AnteHandler provided in `x/auth` or to build their own AnteHandler from scratch. Ideally AnteHandler functionality is split into multiple, modular functions that can be chained together along with custom ante-functions so that users do not have to rewrite common antehandler logic when they want to implement custom behavior.\n\nFor example, let's say a user wants to implement some custom signature verification logic. In the current codebase, the user would have to write their own Antehandler from scratch largely reimplementing much of the same code and then set their own custom, monolithic antehandler in the baseapp. Instead, we would like to allow users to specify custom behavior when necessary and combine them with default ante-handler functionality in a way that is as modular and flexible as possible.\n\n## Proposals\n\n### Per-Module AnteHandler\n\nOne approach is to use the [ModuleManager](https://pkg.go.dev/github.com/cosmos/cosmos-sdk/types/module) and have each module implement its own antehandler if it requires custom antehandler logic. The ModuleManager can then be passed in an AnteHandler order in the same way it has an order for BeginBlockers and EndBlockers. The ModuleManager returns a single AnteHandler function that will take in a tx and run each module's `AnteHandle` in the specified order. The module manager's AnteHandler is set as the baseapp's AnteHandler.\n\nPros:\n\n1. Simple to implement\n2. Utilizes the existing ModuleManager architecture\n\nCons:\n\n1. Improves granularity but still cannot get more granular than a per-module basis. e.g. If auth's `AnteHandle` function is in charge of validating memo and signatures, users cannot swap the signature-checking functionality while keeping the rest of auth's `AnteHandle` functionality.\n2. Module AnteHandler are run one after the other. There is no way for one AnteHandler to wrap or \"decorate\" another.\n\n### Decorator Pattern\n\nThe [weave project](https://github.com/iov-one/weave) achieves AnteHandler modularity through the use of a decorator pattern. The interface is designed as follows:\n\n```go\n// Decorator wraps a Handler to provide common functionality\n// like authentication, or fee-handling, to many Handlers\ntype Decorator interface {\n\tCheck(ctx Context, store KVStore, tx Tx, next Checker) (*CheckResult, error)\n\tDeliver(ctx Context, store KVStore, tx Tx, next Deliverer) (*DeliverResult, error)\n}\n```\n\nEach decorator works like a modularized Cosmos SDK antehandler function, but it can take in a `next` argument that may be another decorator or a Handler (which does not take in a next argument). These decorators can be chained together, one decorator being passed in as the `next` argument of the previous decorator in the chain. The chain ends in a Router which can take a tx and route to the appropriate msg handler.\n\nA key benefit of this approach is that one Decorator can wrap its internal logic around the next Checker/Deliverer. A weave Decorator may do the following:\n\n```go\n// Example Decorator's Deliver function\nfunc (example Decorator) Deliver(ctx Context, store KVStore, tx Tx, next Deliverer) {\n    // Do some pre-processing logic\n\n    res, err := next.Deliver(ctx, store, tx)\n\n    // Do some post-processing logic given the result and error\n}\n```\n\nPros:\n\n1. Weave Decorators can wrap over the next decorator/handler in the chain. The ability to both pre-process and post-process may be useful in certain settings.\n2. Provides a nested modular structure that isn't possible in the solution above, while also allowing for a linear one-after-the-other structure like the solution above.\n\nCons:\n\n1. It is hard to understand at first glance the state updates that would occur after a Decorator runs given the `ctx`, `store`, and `tx`. A Decorator can have an arbitrary number of nested Decorators being called within its function body, each possibly doing some pre- and post-processing before calling the next decorator on the chain. Thus to understand what a Decorator is doing, one must also understand what every other decorator further along the chain is also doing. This can get quite complicated to understand. A linear, one-after-the-other approach while less powerful, may be much easier to reason about.\n\n### Chained Micro-Functions\n\nThe benefit of Weave's approach is that the Decorators can be very concise, which when chained together allows for maximum customizability. However, the nested structure can get quite complex and thus hard to reason about.\n\nAnother approach is to split the AnteHandler functionality into tightly scoped \"micro-functions\", while preserving the one-after-the-other ordering that would come from the ModuleManager approach.\n\nWe can then have a way to chain these micro-functions so that they run one after the other. Modules may define multiple ante micro-functions and then also provide a default per-module AnteHandler that implements a default, suggested order for these micro-functions.\n\nUsers can order the AnteHandlers easily by simply using the ModuleManager. The ModuleManager will take in a list of AnteHandlers and return a single AnteHandler that runs each AnteHandler in the order of the list provided. If the user is comfortable with the default ordering of each module, this is as simple as providing a list with each module's antehandler (exactly the same as BeginBlocker and EndBlocker).\n\nIf however, users wish to change the order or add, modify, or delete ante micro-functions in anyway; they can always define their own ante micro-functions and add them explicitly to the list that gets passed into module manager.\n\n#### Default Workflow\n\nThis is an example of a user's AnteHandler if they choose not to make any custom micro-functions.\n\n##### Cosmos SDK code\n\n```go\n// Chains together a list of AnteHandler micro-functions that get run one after the other.\n// Returned AnteHandler will abort on first error.\nfunc Chainer(order []AnteHandler) AnteHandler {\n    return func(ctx Context, tx Tx, simulate bool) (newCtx Context, err error) {\n        for _, ante := range order {\n            ctx, err := ante(ctx, tx, simulate)\n            if err != nil {\n                return ctx, err\n            }\n        }\n        return ctx, err\n    }\n}\n```\n\n```go\n// AnteHandler micro-function to verify signatures\nfunc VerifySignatures(ctx Context, tx Tx, simulate bool) (newCtx Context, err error) {\n    // verify signatures\n    // Returns InvalidSignature Result and abort=true if sigs invalid\n    // Return OK result and abort=false if sigs are valid\n}\n\n// AnteHandler micro-function to validate memo\nfunc ValidateMemo(ctx Context, tx Tx, simulate bool) (newCtx Context, err error) {\n    // validate memo\n}\n\n// Auth defines its own default ante-handler by chaining its micro-functions in a recommended order\nAuthModuleAnteHandler := Chainer([]AnteHandler{VerifySignatures, ValidateMemo})\n```\n\n```go\n// Distribution micro-function to deduct fees from tx\nfunc DeductFees(ctx Context, tx Tx, simulate bool) (newCtx Context, err error) {\n    // Deduct fees from tx\n    // Abort if insufficient funds in account to pay for fees\n}\n\n// Distribution micro-function to check if fees > mempool parameter\nfunc CheckMempoolFees(ctx Context, tx Tx, simulate bool) (newCtx Context, err error) {\n    // If CheckTx: Abort if the fees are less than the mempool's minFee parameter\n}\n\n// Distribution defines its own default ante-handler by chaining its micro-functions in a recommended order\nDistrModuleAnteHandler := Chainer([]AnteHandler{CheckMempoolFees, DeductFees})\n```\n\n```go\ntype ModuleManager struct {\n    // other fields\n    AnteHandlerOrder []AnteHandler\n}\n\nfunc (mm ModuleManager) GetAnteHandler() AnteHandler {\n    return Chainer(mm.AnteHandlerOrder)\n}\n```\n\n##### User Code\n\n```go\n// Note: Since user is not making any custom modifications, we can just SetAnteHandlerOrder with the default AnteHandlers provided by each module in our preferred order\nmoduleManager.SetAnteHandlerOrder([]AnteHandler(AuthModuleAnteHandler, DistrModuleAnteHandler))\n\napp.SetAnteHandler(mm.GetAnteHandler())\n```\n\n#### Custom Workflow\n\nThis is an example workflow for a user that wants to implement custom antehandler logic. In this example, the user wants to implement custom signature verification and change the order of antehandler so that validate memo runs before signature verification.\n\n##### User Code\n\n```go\n// User can implement their own custom signature verification antehandler micro-function\nfunc CustomSigVerify(ctx Context, tx Tx, simulate bool) (newCtx Context, err error) {\n    // do some custom signature verification logic\n}\n```\n\n```go\n// Micro-functions allow users to change order of when they get executed, and swap out default ante-functionality with their own custom logic.\n// Note that users can still chain the default distribution module handler, and auth micro-function along with their custom ante function\nmoduleManager.SetAnteHandlerOrder([]AnteHandler(ValidateMemo, CustomSigVerify, DistrModuleAnteHandler))\n```\n\nPros:\n\n1. Allows for ante functionality to be as modular as possible.\n2. For users that do not need custom ante-functionality, there is little difference between how antehandlers work and how BeginBlock and EndBlock work in ModuleManager.\n3. Still easy to understand\n\nCons:\n\n1. Cannot wrap antehandlers with decorators like you can with Weave.\n\n### Simple Decorators\n\nThis approach takes inspiration from Weave's decorator design while trying to minimize the number of breaking changes to the Cosmos SDK and maximizing simplicity. Like Weave decorators, this approach allows one `AnteDecorator` to wrap the next AnteHandler to do pre- and post-processing on the result. This is useful since decorators can do defer/cleanups after an AnteHandler returns as well as perform some setup beforehand. Unlike Weave decorators, these `AnteDecorator` functions can only wrap over the AnteHandler rather than the entire handler execution path. This is deliberate as we want decorators from different modules to perform authentication/validation on a `tx`. However, we do not want decorators being capable of wrapping and modifying the results of a `MsgHandler`.\n\nIn addition, this approach will not break any core Cosmos SDK API's. Since we preserve the notion of an AnteHandler and still set a single AnteHandler in baseapp, the decorator is simply an additional approach available for users that desire more customization. The API of modules (namely `x/auth`) may break with this approach, but the core API remains untouched.\n\nAllow Decorator interface that can be chained together to create a Cosmos SDK AnteHandler.\n\nThis allows users to choose between implementing an AnteHandler by themselves and setting it in the baseapp, or use the decorator pattern to chain their custom decorators with the Cosmos SDK provided decorators in the order they wish.\n\n```go\n// An AnteDecorator wraps an AnteHandler, and can do pre- and post-processing on the next AnteHandler\ntype AnteDecorator interface {\n    AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (newCtx Context, err error)\n}\n```\n\n```go\n// ChainAnteDecorators will recursively link all of the AnteDecorators in the chain and return a final AnteHandler function\n// This is done to preserve the ability to set a single AnteHandler function in the baseapp.\nfunc ChainAnteDecorators(chain ...AnteDecorator) AnteHandler {\n    if len(chain) == 1 {\n        return func(ctx Context, tx Tx, simulate bool) {\n            chain[0].AnteHandle(ctx, tx, simulate, nil)\n        }\n    }\n    return func(ctx Context, tx Tx, simulate bool) {\n        chain[0].AnteHandle(ctx, tx, simulate, ChainAnteDecorators(chain[1:]))\n    }\n}\n```\n\n#### Example Code\n\nDefine AnteDecorator functions\n\n```go\n// Setup GasMeter, catch OutOfGasPanic and handle appropriately\ntype SetUpContextDecorator struct{}\n\nfunc (sud SetUpContextDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (newCtx Context, err error) {\n    ctx.GasMeter = NewGasMeter(tx.Gas)\n\n    defer func() {\n        // recover from OutOfGas panic and handle appropriately\n    }\n\n    return next(ctx, tx, simulate)\n}\n\n// Signature Verification decorator. Verify Signatures and move on\ntype SigVerifyDecorator struct{}\n\nfunc (svd SigVerifyDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (newCtx Context, err error) {\n    // verify sigs. Return error if invalid\n\n    // call next antehandler if sigs ok\n    return next(ctx, tx, simulate)\n}\n\n// User-defined Decorator. Can choose to pre- and post-process on AnteHandler\ntype UserDefinedDecorator struct{\n    // custom fields\n}\n\nfunc (udd UserDefinedDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (newCtx Context, err error) {\n    // pre-processing logic\n\n    ctx, err = next(ctx, tx, simulate)\n\n    // post-processing logic\n}\n```\n\nLink AnteDecorators to create a final AnteHandler. Set this AnteHandler in baseapp.\n\n```go\n// Create final antehandler by chaining the decorators together\nantehandler := ChainAnteDecorators(NewSetUpContextDecorator(), NewSigVerifyDecorator(), NewUserDefinedDecorator())\n\n// Set chained Antehandler in the baseapp\nbapp.SetAnteHandler(antehandler)\n```\n\nPros:\n\n1. Allows one decorator to pre- and post-process the next AnteHandler, similar to the Weave design.\n2. Do not need to break baseapp API. Users can still set a single AnteHandler if they choose.\n\nCons:\n\n1. Decorator pattern may have a deeply nested structure that is hard to understand, this is mitigated by having the decorator order explicitly listed in the `ChainAnteDecorators` function.\n2. Does not make use of the ModuleManager design. Since this is already being used for BeginBlocker/EndBlocker, this proposal seems unaligned with that design pattern.\n\n## Consequences\n\nSince pros and cons are written for each approach, it is omitted from this section\n\n## References\n\n* [#4572](https://github.com/cosmos/cosmos-sdk/issues/4572):  Modular AnteHandler Issue\n* [#4582](https://github.com/cosmos/cosmos-sdk/pull/4583): Initial Implementation of Per-Module AnteHandler Approach\n* [Weave Decorator Code](https://github.com/iov-one/weave/blob/master/handler.go#L35)\n* [Weave Design Videos](https://vimeo.com/showcase/6189877)"
  },
  {
    "number": 11,
    "filename": "adr-011-generalize-genesis-accounts.md",
    "title": "ADR 011: Generalize Genesis Accounts",
    "content": "# ADR 011: Generalize Genesis Accounts\n\n## Changelog\n\n* 2019-08-30: initial draft\n\n## Context\n\nCurrently, the Cosmos SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\n\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\n\n## Decision\n\nIn summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`’s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\n\nDetailed changes:\n\n### 1) (Un)Marshal accounts directly using amino\n\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\n\n```go\n// GenesisState - all auth state that must be provided at genesis\ntype GenesisState struct {\n    Params   Params           `json:\"params\" yaml:\"params\"`\n    Accounts []GenesisAccount `json:\"accounts\" yaml:\"accounts\"`\n}\n```\n\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\n\n```go\n// InitGenesis - Init store state from genesis data\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\n    ak.SetParams(ctx, data.Params)\n    // load the accounts\n    for _, a := range data.Accounts {\n        acc := ak.NewAccount(ctx, a) // set account number\n        ak.SetAccount(ctx, acc)\n    }\n}\n\n// ExportGenesis returns a GenesisState for a given context and keeper\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\n    params := ak.GetParams(ctx)\n\n    var genAccounts []exported.GenesisAccount\n    ak.IterateAccounts(ctx, func(account exported.Account) bool {\n        genAccount := account.(exported.GenesisAccount)\n        genAccounts = append(genAccounts, genAccount)\n        return false\n    })\n\n    return NewGenesisState(params, genAccounts)\n}\n```\n\n### 2) Register custom account types on the `auth` codec\n\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\n\nAn example custom account definition:\n\n```go\nimport authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n\n// Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\nfunc init() {\n    authtypes.RegisterAccountTypeCodec(ModuleAccount{}, \"cosmos-sdk/ModuleAccount\")\n}\n\ntype ModuleAccount struct {\n    ...\n```\n\nThe `auth` codec definition:\n\n```go\nvar ModuleCdc *codec.LegacyAmino\n\nfunc init() {\n    ModuleCdc = codec.NewLegacyAmino()\n    // register module msg's and Account interface\n    ...\n    // leave the codec unsealed\n}\n\n// RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\n    ModuleCdc.RegisterConcrete(o, name, nil)\n}\n```\n\n### 3) Genesis validation for custom account types\n\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\n\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\n\n```go\ntype GenesisAccount interface {\n    exported.Account\n    Validate() error\n}\n```\n\nThen the `auth` `ValidateGenesis` function becomes:\n\n```go\n// ValidateGenesis performs basic validation of auth genesis data returning an\n// error for any failed validation criteria.\nfunc ValidateGenesis(data GenesisState) error {\n    // Validate params\n    ...\n\n    // Validate accounts\n    addrMap := make(map[string]bool, len(data.Accounts))\n    for _, acc := range data.Accounts {\n\n        // check for duplicated accounts\n        addrStr := acc.GetAddress().String()\n        if _, ok := addrMap[addrStr]; ok {\n            return fmt.Errorf(\"duplicate account found in genesis state; address: %s\", addrStr)\n        }\n        addrMap[addrStr] = true\n\n        // check account specific validation\n        if err := acc.Validate(); err != nil {\n            return fmt.Errorf(\"invalid account found in genesis state; address: %s, error: %s\", addrStr, err.Error())\n        }\n\n    }\n    return nil\n}\n```\n\n### 4) Move add-genesis-account cli to `auth`\n\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\n\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\n\n### 5) Update module and vesting accounts\n\nUnder the new scheme, module and vesting account types need some minor updates:\n\n* Type registration on `auth`'s codec (shown above)\n* A `Validate` method for each `Account` concrete type\n\n## Status\n\nProposed\n\n## Consequences\n\n### Positive\n\n* custom accounts can be used without needing to fork `genaccounts`\n* reduction in lines of code\n\n### Negative\n\n### Neutral\n\n* `genaccounts` module no longer exists\n* accounts in genesis files are stored under `accounts` in `auth` rather than in the `genaccounts` module.\n-`add-genesis-account` cli command now in `auth`\n\n## References"
  },
  {
    "number": 12,
    "filename": "adr-012-state-accessors.md",
    "title": "ADR 012: State Accessors",
    "content": "# ADR 012: State Accessors\n\n## Changelog\n\n* 2019 Sep 04: Initial draft\n\n## Context\n\nCosmos SDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\nmediocre.\n\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\nwhich are repetitive and hard to maintain.\n\nSecond, this makes it harder to align with the object capability theorem: the right to access the\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\n\nFinally, because the getter/setter functions are defined as methods of a module's `Keeper`, the reviewers\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\nThere is no static way to know which part of the state that the function is accessing (and which is not).\n\n## Decision\n\nWe will define a type named `Value`:\n\n```go\ntype Value struct {\n  m   Mapping\n  key []byte\n}\n```\n\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\nspace it will access and `Value.key` defines the exact key for the reference.\n\nWe will define a type named `Mapping`:\n\n```go\ntype Mapping struct {\n  storeKey sdk.StoreKey\n  cdc      *codec.LegacyAmino\n  prefix   []byte\n}\n```\n\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\n\nWe will define the following core methods for the `Value` type:\n\n```go\n// Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\nfunc (Value) Get(ctx Context, ptr interface{}) {}\n\n// Get and unmarshal stored data, return error if not exists or cannot unmarshal\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\n\n// Get stored data as raw byte slice\nfunc (Value) GetRaw(ctx Context) []byte {}\n\n// Marshal and set a raw value\nfunc (Value) Set(ctx Context, o interface{}) {}\n\n// Check if a raw value exists\nfunc (Value) Exists(ctx Context) bool {}\n\n// Delete a raw value \nfunc (Value) Delete(ctx Context) {}\n```\n\nWe will define the following core methods for the `Mapping` type:\n\n```go\n// Constructs key-value pair reference corresponding to the key argument in the Mapping space\nfunc (Mapping) Value(key []byte) Value {}\n\n// Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\n\n// Get and unmarshal stored data, return error if not exists or cannot unmarshal\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\n\n// Get stored data as raw byte slice\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\n\n// Marshal and set a raw value\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\n\n// Check if a raw value exists\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\n\n// Delete a raw value\nfunc (Mapping) Delete(ctx Context, key []byte) {}\n```\n\nEach method of the `Mapping` type that is passed the arguments `ctx`, `key`, and `args...` will proxy\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\n\nIn addition, we will define and provide a common set of types derived from the `Value` type:\n\n```go\ntype Boolean struct { Value }\ntype Enum struct { Value }\ntype Integer struct { Value; enc IntEncoding }\ntype String struct { Value }\n// ...\n```\n\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\nin core methods are replaced by explicit return types.\n\nFinally, we will define a family of types derived from the `Mapping` type:\n\n```go\ntype Indexer struct {\n  m   Mapping\n  enc IntEncoding\n}\n```\n\nWhere the `key` argument in core method is typed.\n\nSome of the properties of the accessor types are:\n\n* State access happens only when a function which takes a `Context` as an argument is invoked\n* Accessor type structs give rights to access the state only that the struct is referring, no other\n* Marshalling/Unmarshalling happens implicitly within the core methods\n\n## Status\n\nProposed\n\n## Consequences\n\n### Positive\n\n* Serialization will be done automatically\n* Shorter code size, less boilerplate, better UX\n* References to the state can be transferred safely\n* Explicit scope of accessing\n\n### Negative\n\n* Serialization format will be hidden\n* Different architecture from the current, but the use of accessor types can be opt-in\n* Type-specific types (e.g. `Boolean` and `Integer`) have to be defined manually\n\n### Neutral\n\n## References\n\n* [#4554](https://github.com/cosmos/cosmos-sdk/issues/4554)"
  },
  {
    "number": 13,
    "filename": "adr-013-metrics.md",
    "title": "ADR 013: Observability",
    "content": "# ADR 013: Observability\n\n## Changelog\n\n* 20-01-2020: Initial Draft\n\n## Status\n\nProposed\n\n## Context\n\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\n\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\nBy default, when telemetry is enabled, the application should track and expose metrics that are\nstored in-memory. The operator may choose to enable additional sinks, where we support only\n[Prometheus](https://prometheus.io/) for now, as it's battle-tested, simple to setup, open source,\nand is rich with ecosystem tooling.\n\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\nmetrics may be added or removed at will and without much friction. To do this, we will use the\n[go-metrics](https://github.com/hashicorp/go-metrics) library.\n\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\nwill be exposed via `/metrics?format={text|prometheus}` via the API server.\n\n## Decision\n\nWe will add an additional configuration block to `app.toml` that defines telemetry settings:\n\n```toml\n###############################################################################\n###                         Telemetry Configuration                         ###\n###############################################################################\n\n[telemetry]\n\n# Prefixed with keys to separate services\nservice-name = {{ .Telemetry.ServiceName }}\n\n# Enabled enables the application telemetry functionality. When enabled,\n# an in-memory sink is also enabled by default. Operators may also enabled\n# other sinks such as Prometheus.\nenabled = {{ .Telemetry.Enabled }}\n\n# Enable prefixing gauge values with hostname\nenable-hostname = {{ .Telemetry.EnableHostname }}\n\n# Enable adding hostname to labels\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\n\n# Enable adding service to labels\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\n\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\n```\n\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\n\n```go\n// Metrics defines a wrapper around application telemetry functionality. It allows\n// metrics to be gathered at any point in time. When creating a Metrics object,\n// internally, a global metrics is registered with a set of sinks as configured\n// by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\n// dump of formatted recent metrics will be sent to STDERR.\ntype Metrics struct {\n  memSink           *metrics.InmemSink\n  prometheusEnabled bool\n}\n\n// Gather collects all registered metrics and returns a GatherResponse where the\n// metrics are encoded depending on the type. Metrics are either encoded via\n// Prometheus or JSON if in-memory.\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\n  switch format {\n  case FormatPrometheus:\n    return m.gatherPrometheus()\n\n  case FormatText:\n    return m.gatherGeneric()\n\n  case FormatDefault:\n    return m.gatherGeneric()\n\n  default:\n    return GatherResponse{}, fmt.Errorf(\"unsupported metrics format: %s\", format)\n  }\n}\n```\n\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\n\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\nhandler accordingly.\n\n```go\nfunc (s *Server) Start(cfg config.Config) error {\n  // ...\n\n  if cfg.Telemetry.Enabled {\n    m, err := telemetry.New(cfg.Telemetry)\n    if err != nil {\n      return err\n    }\n\n    s.metrics = m\n    s.registerMetrics()\n  }\n\n  // ...\n}\n\nfunc (s *Server) registerMetrics() {\n  metricsHandler := func(w http.ResponseWriter, r *http.Request) {\n    format := strings.TrimSpace(r.FormValue(\"format\"))\n\n    gr, err := s.metrics.Gather(format)\n    if err != nil {\n      rest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(\"failed to gather metrics: %s\", err))\n      return\n    }\n\n    w.Header().Set(\"Content-Type\", gr.ContentType)\n    _, _ = w.Write(gr.Metrics)\n  }\n\n  s.Router.HandleFunc(\"/metrics\", metricsHandler).Methods(\"GET\")\n}\n```\n\nApplication developers may track counters, gauges, summaries, and key/value metrics. There is no\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\n\n```go\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\n  defer metrics.MeasureSince(time.Now(), \"MintCoins\")\n  // ...\n}\n```\n\n## Consequences\n\n### Positive\n\n* Exposure into the performance and behavior of an application\n\n### Negative\n\n### Neutral\n\n## References"
  },
  {
    "number": 14,
    "filename": "adr-014-proportional-slashing.md",
    "title": "ADR 14: Proportional Slashing",
    "content": "# ADR 14: Proportional Slashing\n\n## Changelog\n\n* 2019-10-15: Initial draft\n* 2020-05-25: Removed correlation root slashing\n* 2020-07-01: Updated to include S-curve function instead of linear\n\n## Context\n\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\n\n## Decision\n\n### Design\n\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\n\n```text\nslash_amount = k * power // power is the faulting validator's voting power and k is some on-chain constant\n```\n\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts (sybil attack), so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\n\n```text\nslash_amount = k * (power_1 + power_2 + ... + power_n) // where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\n```\n\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will get slashed at the sum 10% amount.\n\nHowever in practice, we likely don't want a linear relation between amount of stake at fault, and the percentage of stake to slash. In particular, solely 5% of stake double signing effectively did nothing to majorly threaten security, whereas 30% of stake being at fault clearly merits a large slashing factor, due to being very close to the point at which Tendermint security is threatened. A linear relation would require a factor of 6 gap between these two, whereas the difference in risk posed to the network is much larger. We propose using S-curves (formally [logistic functions](https://en.wikipedia.org/wiki/Logistic_function) to solve this). S-Curves capture the desired criterion quite well. They allow the slashing factor to be minimal for small values, and then grow very rapidly near some threshold point where the risk posed becomes notable.\n\n#### Parameterization\n\nThis requires parameterizing a logistic function. It is very well understood how to parameterize this. It has four parameters:\n\n1) A minimum slashing factor\n2) A maximum slashing factor\n3) The inflection point of the S-curve (essentially where do you want to center the S)\n4) The rate of growth of the S-curve (How elongated is the S)\n\n#### Correlation across non-sybil validators\n\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\n\n#### Griefing\n\nGriefing, the act of intentionally getting oneself slashed in order to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker also gets equally impacted by the grief as the victim, so it would not provide much benefit to the griefer.\n\n### Implementation\n\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occurred within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occurred within the last `jail period`.\n\n```go\ntype SlashEvent struct {\n    Address                     sdk.ValAddress\n    ValidatorVotingPercent      sdk.Dec\n    SlashedSoFar                sdk.Dec\n}\n```\n\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\n\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\n\nWe then will iterate over all the SlashEvents in the queue, adding their `ValidatorVotingPercent` to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\n\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occurred, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\n\n## Status\n\nProposed\n\n## Consequences\n\n### Positive\n\n* Increases decentralization by disincentivizing delegating to large validators\n* Incentivizes Decorrelation of Validators\n* More severely punishes attacks than accidental faults\n* More flexibility in slashing rates parameterization\n\n### Negative\n\n* More computationally expensive than current implementation.  Will require more data about \"recent slashing events\" to be stored on chain."
  },
  {
    "number": 16,
    "filename": "adr-016-validator-consensus-key-rotation.md",
    "title": "ADR 016: Validator Consensus Key Rotation",
    "content": "# ADR 016: Validator Consensus Key Rotation\n\n## Changelog\n\n* 2019 Oct 23: Initial draft\n* 2019 Nov 28: Add key rotation fee\n\n## Context\n\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https://github.com/tendermint/tendermint/issues/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\n\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint's point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\n\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering the multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\n\n## Decision\n\n### Pseudo procedure for consensus key rotation\n\n* create new random consensus key.\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with a signature from the validator's operator key.\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\n* start validating with new consensus key.\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` is committed to the blockchain.\n\n### Considerations\n\n* consensus key mapping information management strategy\n    * store history of each key mapping changes in the kvstore.\n    * the state machine can search corresponding consensus key paired with the given validator operator for any arbitrary height in a recent unbonding period.\n    * the state machine does not need any historical mapping information which is past more than unbonding period.\n* key rotation costs related to LCD and IBC\n    * LCD and IBC will have a traffic/computation burden when there exists frequent power changes\n    * In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\n    * Therefore, to minimize unnecessary frequent key rotation behavior, we limited the maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\n* limits\n    * a validator cannot rotate its consensus key more than `MaxConsPubKeyRotations` time for any unbonding period, to prevent spam.\n    * parameters can be decided by governance and stored in genesis file.\n* key rotation fee\n    * a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\n    * `KeyRotationFee` = (max(`VotingPowerPercentage` *100, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\n* evidence module\n    * evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for the given height.\n* abci.ValidatorUpdate\n    * tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\n    * validator consensus key update can be done via creating new + delete old by change the power to zero.\n    * therefore, we expect we do not even need to change Tendermint codebase at all to implement this feature.\n* new genesis parameters in `staking` module\n    * `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\n    * `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\n\n### Workflow\n\n1. The validator generates a new consensus keypair.\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\n\n    ```go\n    type MsgRotateConsPubKey struct {\n        ValidatorAddress  sdk.ValAddress\n        NewPubKey         crypto.PubKey\n    }\n    ```\n\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\n4. `RotateConsPubKey`\n    * checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\n    * checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\n    * checks if the signing account has enough balance to pay `KeyRotationFee`\n    * pays `KeyRotationFee` to community fund\n    * overwrites `NewPubKey` in `validator.ConsPubKey`\n    * deletes old `ValidatorByConsAddr`\n    * `SetValidatorByConsAddr` for `NewPubKey`\n    * Add `ConsPubKeyRotationHistory` for tracking rotation\n\n    ```go\n    type ConsPubKeyRotationHistory struct {\n        OperatorAddress         sdk.ValAddress\n        OldConsPubKey           crypto.PubKey\n        NewConsPubKey           crypto.PubKey\n        RotatedHeight           int64\n    }\n    ```\n\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\n\n    ```go\n    abci.ValidatorUpdate{\n        PubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\n        Power:  0,\n    }\n\n    abci.ValidatorUpdate{\n        PubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\n        Power:  v.ConsensusPower(),\n    }\n    ```\n\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\n\n* Note : All above features shall be implemented in `staking` module.\n\n## Status\n\nProposed\n\n## Consequences\n\n### Positive\n\n* Validators can immediately or periodically rotate their consensus key to have a better security policy\n* improved security against Long-Range attacks (https://nearprotocol.com/blog/long-range-attacks-and-a-new-fork-choice-rule) given a validator throws away the old consensus key(s)\n\n### Negative\n\n* Slash module needs more computation because it needs to look up the corresponding consensus key of validators for each height\n* frequent key rotations will make light client bisection less efficient\n\n### Neutral\n\n## References\n\n* on tendermint repo : https://github.com/tendermint/tendermint/issues/1136\n* on cosmos-sdk repo : https://github.com/cosmos/cosmos-sdk/issues/5231\n* about multiple consensus keys : https://github.com/tendermint/tendermint/issues/1758#issuecomment-545291698"
  },
  {
    "number": 17,
    "filename": "adr-017-historical-header-module.md",
    "title": "ADR 17: Historical Header Module",
    "content": "# ADR 17: Historical Header Module\n\n## Changelog\n\n* 26 November 2019: Start of first version\n* 2 December 2019: Final draft of first version\n\n## Context\n\nIn order for the Cosmos SDK to implement the [IBC specification](https://github.com/cosmos/ics), modules within the Cosmos SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\n\n## Decision\n\nThe application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\n\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\n\n```go\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper, req abci.RequestBeginBlock) abci.ResponseBeginBlock {\n  info := HistoricalInfo{\n    Header: ctx.BlockHeader(),\n    ValSet: keeper.StakingKeeper.GetAllValidators(ctx), // note that this must be stored in a canonical order\n  }\n  keeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\n  n := keeper.GetParamRecentHeadersToStore()\n  keeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\n  // continue handling request\n}\n```\n\nAlternatively, the application MAY store only the hash of the validator set.\n\nThe application MUST make these past `n` committed headers available for querying by Cosmos SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x/staking` or `x/ibc`).\n\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\n\n## Status\n\nProposed.\n\n## Consequences\n\nImplementation of this ADR will require changes to the Cosmos SDK. It will not require changes to Tendermint.\n\n### Positive\n\n* Easy retrieval of headers & state roots for recent past heights by modules anywhere in the Cosmos SDK.\n* No RPC calls to Tendermint required.\n* No ABCI alterations required.\n\n### Negative\n\n* Duplicates `n` headers data in Tendermint & the application (additional disk usage) - in the long term, an approach such as [this](https://github.com/tendermint/tendermint/issues/4210) might be preferable.\n\n### Neutral\n\n(none known)\n\n## References\n\n* [ICS 2: \"Consensus state introspection\"](https://github.com/cosmos/ibc/tree/master/spec/core/ics-002-client-semantics#consensus-state-introspection)"
  },
  {
    "number": 18,
    "filename": "adr-018-extendable-voting-period.md",
    "title": "ADR 18: Extendable Voting Periods",
    "content": "# ADR 18: Extendable Voting Periods\n\n## Changelog\n\n* 1 January 2020: Start of first version\n\n## Context\n\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficiently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion/consideration.\n\n## Decision\n\nWe would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\n\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\n\nThus, we propose the following mechanism:\n\n### Params\n\n* The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the default voting period that all governance proposal voting periods start with.\n* There is a new gov param called `MaxVotingPeriodExtension`.\n\n### Mechanism\n\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\n\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\n\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the gauge the contentiousness/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\n\n### Dealing with Unbonding/Redelegation\n\nThere is one thing that needs to be addressed.  How to deal with redelegation/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\n\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\n\n### Delegators\n\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and \"unextend\" as that would contradict the \"voting power length can only be ratcheted up\" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\n\n## Status\n\nProposed\n\n## Consequences\n\n### Positive\n\n* More complex/contentious governance proposals will have more time to properly digest and deliberate\n\n### Negative\n\n* Governance process becomes more complex and requires more understanding to interact with effectively\n* Can no longer predict when a governance proposal will end. Can't assume order in which governance proposals will end.\n\n### Neutral\n\n* The minimum voting period can be made shorter\n\n## References\n\n* [Cosmos Forum post where idea first originated](https://forum.cosmos.network/t/proposal-draft-reduce-governance-voting-period-to-7-days/3032/9)"
  },
  {
    "number": 19,
    "filename": "adr-019-protobuf-state-encoding.md",
    "title": "ADR 019: Protocol Buffer State Encoding",
    "content": "# ADR 019: Protocol Buffer State Encoding\n\n## Changelog\n\n* 2020 Feb 15: Initial Draft\n* 2020 Feb 24: Updates to handle messages with interface fields\n* 2020 Apr 27: Convert usages of `oneof` for interfaces to `Any`\n* 2020 May 15: Describe `cosmos_proto` extensions and amino compatibility\n* 2020 Dec 4: Move and rename `MarshalAny` and `UnmarshalAny` into the `codec.Codec` interface.\n* 2021 Feb 24: Remove mentions of `HybridCodec`, which has been abandoned in [#6843](https://github.com/cosmos/cosmos-sdk/pull/6843).\n\n## Status\n\nAccepted\n\n## Context\n\nCurrently, the Cosmos SDK utilizes [go-amino](https://github.com/tendermint/go-amino/) for binary\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\n\nFrom the Amino docs:\n\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\n> support. See the [Proto3 spec](https://developers.google.com/protocol-buffers/docs/proto3) for more\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\n>\n> The goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.\n\nAmino also aims to have the following goals (not a complete list):\n\n* Binary bytes must be decodable with a schema.\n* Schema must be upgradeable.\n* The encoder and decoder logic must be reasonably simple.\n\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\nclients written in various languages while providing virtually little in the way of true backwards\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1</sup>. This is\nlargely reflected in the performance of simulations and application transaction throughput.\n\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\n\n* Language agnostic\n* Platform agnostic\n* Rich client support and thriving ecosystem\n* High performance\n* Minimal encoded message size\n* Codegen-based over reflection-based\n* Supports backward and forward compatibility\n\nNote, migrating away from Amino should be viewed as a two-pronged approach, state and client encoding.\nThis ADR focuses on state serialization in the Cosmos SDK state machine. A corresponding ADR will be\nmade to address client-side encoding.\n\n## Decision\n\nWe will adopt [Protocol Buffers](https://developers.google.com/protocol-buffers) for serializing\npersisted structured data in the Cosmos SDK while providing a clean mechanism and developer UX for\napplications wishing to continue to use Amino. We will provide this mechanism by updating modules to\naccept a codec interface, `Marshaler`, instead of a concrete Amino codec. Furthermore, the Cosmos SDK\nwill provide two concrete implementations of the `Marshaler` interface: `AminoCodec` and `ProtoCodec`.\n\n* `AminoCodec`: Uses Amino for both binary and JSON encoding.\n* `ProtoCodec`: Uses Protobuf for both binary and JSON encoding.\n\nModules will use whichever codec is instantiated in the app. By default, the Cosmos SDK's `simapp`\ninstantiates a `ProtoCodec` as the concrete implementation of `Marshaler`, inside the `MakeTestEncodingConfig`\nfunction. This can be easily overwritten by app developers if they so desire.\n\nThe ultimate goal will be to replace Amino JSON encoding with Protobuf encoding and thus have\nmodules accept and/or extend `ProtoCodec`. Until then, Amino JSON is still provided for legacy use-cases.\nA handful of places in the Cosmos SDK still have Amino JSON hardcoded, such as the Legacy API REST endpoints\nand the `x/params` store. They are planned to be converted to Protobuf in a gradual manner.\n\n### Module Codecs\n\nModules that do not require the ability to work with and serialize interfaces, the path to Protobuf\nmigration is pretty straightforward. These modules are to simply migrate any existing types that\nare encoded and persisted via their concrete Amino codec to Protobuf and have their keeper accept a\n`Marshaler` that will be a `ProtoCodec`. This migration is simple as things will just work as-is.\n\nNote, any business logic that needs to encode primitive types like `bool` or `int64` should use\n[gogoprotobuf](https://github.com/cosmos/gogoproto) Value types.\n\nExample:\n\n```go\n  ts, err := gogotypes.TimestampProto(completionTime)\n  if err != nil {\n    // ...\n  }\n\n  bz := cdc.MustMarshal(ts)\n```\n\nHowever, modules can vary greatly in purpose and design and so we must support the ability for modules\nto be able to encode and work with interfaces (e.g. `Account` or `Content`). For these modules, they\nmust define their own codec interface that extends `Marshaler`. These specific interfaces are unique\nto the module and will contain method contracts that know how to serialize the needed interfaces.\n\nExample:\n\n```go\n// x/auth/types/codec.go\n\ntype Codec interface {\n  codec.Codec\n\n  MarshalAccount(acc exported.Account) ([]byte, error)\n  UnmarshalAccount(bz []byte) (exported.Account, error)\n\n  MarshalAccountJSON(acc exported.Account) ([]byte, error)\n  UnmarshalAccountJSON(bz []byte) (exported.Account, error)\n}\n```\n\n### Usage of `Any` to encode interfaces\n\nIn general, module-level .proto files should define messages which encode interfaces\nusing [`google.protobuf.Any`](https://github.com/protocolbuffers/protobuf/blob/master/src/google/protobuf/any.proto).\nAfter [extension discussion](https://github.com/cosmos/cosmos-sdk/issues/6030),\nthis was chosen as the preferred alternative to application-level `oneof`s\nas in our original protobuf design. The arguments in favor of `Any` can be\nsummarized as follows:\n\n* `Any` provides a simpler, more consistent client UX for dealing with\ninterfaces than app-level `oneof`s that will need to be coordinated more\ncarefully across applications. Creating a generic transaction\nsigning library using `oneof`s may be cumbersome and critical logic may need\nto be reimplemented for each chain\n* `Any` provides more resistance against human error than `oneof`\n* `Any` is generally simpler to implement for both modules and apps\n\nThe main counter-argument to using `Any` centers around its additional space\nand possibly performance overhead. The space overhead could be dealt with using\ncompression at the persistence layer in the future and the performance impact\nis likely to be small. Thus, not using `Any` is seen as a pre-mature optimization,\nwith user experience as the higher order concern.\n\nNote, that given the Cosmos SDK's decision to adopt the `Codec` interfaces described\nabove, apps can still choose to use `oneof` to encode state and transactions\nbut it is not the recommended approach. If apps do choose to use `oneof`s\ninstead of `Any` they will likely lose compatibility with client apps that\nsupport multiple chains. Thus developers should think carefully about whether\nthey care more about what is possibly a premature optimization or end-user\nand client developer UX.\n\n### Safe usage of `Any`\n\nBy default, the [gogo protobuf implementation of `Any`](https://pkg.go.dev/github.com/cosmos/gogoproto/types)\nuses [global type registration]( https://github.com/cosmos/gogoproto/blob/master/proto/properties.go#L540)\nto decode values packed in `Any` into concrete\ngo types. This introduces a vulnerability where any malicious module\nin the dependency tree could register a type with the global protobuf registry\nand cause it to be loaded and unmarshaled by a transaction that referenced\nit in the `type_url` field.\n\nTo prevent this, we introduce a type registration mechanism for decoding `Any`\nvalues into concrete types through the `InterfaceRegistry` interface which\nbears some similarity to type registration with Amino:\n\n```go\ntype InterfaceRegistry interface {\n    // RegisterInterface associates protoName as the public name for the\n    // interface passed in as iface\n    // Ex:\n    //   registry.RegisterInterface(\"cosmos_sdk.Msg\", (*sdk.Msg)(nil))\n    RegisterInterface(protoName string, iface interface{})\n\n    // RegisterImplementations registers impls as concrete implementations of\n    // the interface iface\n    // Ex:\n    //  registry.RegisterImplementations((*sdk.Msg)(nil), &MsgSend{}, &MsgMultiSend{})\n    RegisterImplementations(iface interface{}, impls ...proto.Message)\n\n}\n```\n\nIn addition to serving as a whitelist, `InterfaceRegistry` can also serve\nto communicate the list of concrete types that satisfy an interface to clients.\n\nIn .proto files:\n\n* fields which accept interfaces should be annotated with `cosmos_proto.accepts_interface`\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\n* interface implementations should be annotated with `cosmos_proto.implements_interface`\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\n\nIn the future, `protoName`, `cosmos_proto.accepts_interface`, `cosmos_proto.implements_interface`\nmay be used via code generation, reflection &/or static linting.\n\nThe same struct that implements `InterfaceRegistry` will also implement an\ninterface `InterfaceUnpacker` to be used for unpacking `Any`s:\n\n```go\ntype InterfaceUnpacker interface {\n    // UnpackAny unpacks the value in any to the interface pointer passed in as\n    // iface. Note that the type in any must have been registered with\n    // RegisterImplementations as a concrete type for that interface\n    // Ex:\n    //    var msg sdk.Msg\n    //    err := ctx.UnpackAny(any, &msg)\n    //    ...\n    UnpackAny(any *Any, iface interface{}) error\n}\n```\n\nNote that `InterfaceRegistry` usage does not deviate from standard protobuf\nusage of `Any`, it just introduces a security and introspection layer for\ngolang usage.\n\n`InterfaceRegistry` will be a member of `ProtoCodec`\ndescribed above. In order for modules to register interface types, app modules\ncan optionally implement the following interface:\n\n```go\ntype InterfaceModule interface {\n    RegisterInterfaceTypes(InterfaceRegistry)\n}\n```\n\nThe module manager will include a method to call `RegisterInterfaceTypes` on\nevery module that implements it in order to populate the `InterfaceRegistry`.\n\n### Using `Any` to encode state\n\nThe Cosmos SDK will provide support methods `MarshalInterface` and `UnmarshalInterface` to hide the complexity of wrapping interface types into `Any` and allow easy serialization.\n\n```go\nimport \"github.com/cosmos/cosmos-sdk/codec\"\n\n// note: eviexported.Evidence is an interface type\nfunc MarshalEvidence(cdc codec.BinaryCodec, e eviexported.Evidence) ([]byte, error) {\n\treturn cdc.MarshalInterface(e)\n}\n\nfunc UnmarshalEvidence(cdc codec.BinaryCodec, bz []byte) (eviexported.Evidence, error) {\n\tvar evi eviexported.Evidence\n\terr := cdc.UnmarshalInterface(&evi, bz)\n    return err, nil\n}\n```\n\n### Using `Any` in `sdk.Msg`s\n\nA similar concept is to be applied for messages that contain interface fields.\nFor example, we can define `MsgSubmitEvidence` as follows where `Evidence` is\nan interface:\n\n```protobuf\n// x/evidence/types/types.proto\n\nmessage MsgSubmitEvidence {\n  bytes submitter = 1\n    [\n      (gogoproto.casttype) = \"github.com/cosmos/cosmos-sdk/types.AccAddress\"\n    ];\n  google.protobuf.Any evidence = 2;\n}\n```\n\nNote that in order to unpack the evidence from `Any` we do need a reference to\n`InterfaceRegistry`. In order to reference evidence in methods like\n`ValidateBasic` which shouldn't have to know about the `InterfaceRegistry`, we\nintroduce an `UnpackInterfaces` phase to deserialization which unpacks\ninterfaces before they're needed.\n\n### Unpacking Interfaces\n\nTo implement the `UnpackInterfaces` phase of deserialization which unpacks\ninterfaces wrapped in `Any` before they're needed, we create an interface\nthat `sdk.Msg`s and other types can implement:\n\n```go\ntype UnpackInterfacesMessage interface {\n  UnpackInterfaces(InterfaceUnpacker) error\n}\n```\n\nWe also introduce a private `cachedValue interface{}` field onto the `Any`\nstruct itself with a public getter `GetCachedValue() interface{}`.\n\nThe `UnpackInterfaces` method is to be invoked during message deserialization right\nafter `Unmarshal` and any interface values packed in `Any`s will be decoded\nand stored in `cachedValue` for reference later.\n\nThen unpacked interface values can safely be used in any code afterwards\nwithout knowledge of the `InterfaceRegistry`\nand messages can introduce a simple getter to cast the cached value to the\ncorrect interface type.\n\nThis has the added benefit that unmarshaling of `Any` values only happens once\nduring initial deserialization rather than every time the value is read. Also,\nwhen `Any` values are first packed (for instance in a call to\n`NewMsgSubmitEvidence`), the original interface value is cached so that\nunmarshaling isn't needed to read it again.\n\n`MsgSubmitEvidence` could implement `UnpackInterfaces`, plus a convenience getter\n`GetEvidence` as follows:\n\n```go\nfunc (msg MsgSubmitEvidence) UnpackInterfaces(ctx sdk.InterfaceRegistry) error {\n  var evi eviexported.Evidence\n  return ctx.UnpackAny(msg.Evidence, *evi)\n}\n\nfunc (msg MsgSubmitEvidence) GetEvidence() eviexported.Evidence {\n  return msg.Evidence.GetCachedValue().(eviexported.Evidence)\n}\n```\n\n### Amino Compatibility\n\nOur custom implementation of `Any` can be used transparently with Amino if used\nwith the proper codec instance. What this means is that interfaces packed within\n`Any`s will be amino marshaled like regular Amino interfaces (assuming they\nhave been registered properly with Amino).\n\nIn order for this functionality to work:\n\n* **all legacy code must use `*codec.LegacyAmino` instead of `*amino.Codec` which is\n  now a wrapper which properly handles `Any`**\n* **all new code should use `Marshaler` which is compatible with both amino and\n  protobuf**\n* Also, before v0.39, `codec.LegacyAmino` will be renamed to `codec.LegacyAmino`.\n\n### Why Wasn't X Chosen Instead\n\nFor a more complete comparison to alternative protocols, see [here](https://codeburst.io/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f).\n\n### Cap'n Proto\n\nWhile [Cap’n Proto](https://capnproto.org/) does seem like an advantageous alternative to Protobuf\ndue to its native support for interfaces/generics and built-in canonicalization, it does lack the\nrich client ecosystem compared to Protobuf and is a bit less mature.\n\n### FlatBuffers\n\n[FlatBuffers](https://google.github.io/flatbuffers/) is also a potentially viable alternative, with the\nprimary difference being that FlatBuffers does not need a parsing/unpacking step to a secondary\nrepresentation before you can access data, often coupled with per-object memory allocation.\n\nHowever, it would require great efforts into research and a full understanding the scope of the migration\nand path forward -- which isn't immediately clear. In addition, FlatBuffers aren't designed for\nuntrusted inputs.\n\n## Future Improvements & Roadmap\n\nIn the future we may consider a compression layer right above the persistence\nlayer which doesn't change tx or merkle tree hashes, but reduces the storage\noverhead of `Any`. In addition, we may adopt protobuf naming conventions which\nmake type URLs a bit more concise while remaining descriptive.\n\nAdditional code generation support around the usage of `Any` is something that\ncould also be explored in the future to make the UX for go developers more\nseamless.\n\n## Consequences\n\n### Positive\n\n* Significant performance gains.\n* Supports backward and forward type compatibility.\n* Better support for cross-language clients.\n\n### Negative\n\n* Learning curve required to understand and implement Protobuf messages.\n* Slightly larger message size due to use of `Any`, although this could be offset\n  by a compression layer in the future\n\n### Neutral\n\n## References\n\n1. https://github.com/cosmos/cosmos-sdk/issues/4977\n2. https://github.com/cosmos/cosmos-sdk/issues/5444"
  },
  {
    "number": 20,
    "filename": "adr-020-protobuf-transaction-encoding.md",
    "title": "ADR 020: Protocol Buffer Transaction Encoding",
    "content": "# ADR 020: Protocol Buffer Transaction Encoding\n\n## Changelog\n\n* 2020 March 06: Initial Draft\n* 2020 March 12: API Updates\n* 2020 April 13: Added details on interface `oneof` handling\n* 2020 April 30: Switch to `Any`\n* 2020 May 14: Describe public key encoding\n* 2020 June 08: Store `TxBody` and `AuthInfo` as bytes in `SignDoc`; Document `TxRaw` as broadcast and storage type.\n* 2020 August 07: Use ADR 027 for serializing `SignDoc`.\n* 2020 August 19: Move sequence field from `SignDoc` to `SignerInfo`, as discussed in [#6966](https://github.com/cosmos/cosmos-sdk/issues/6966).\n* 2020 September 25: Remove `PublicKey` type in favor of `secp256k1.PubKey`, `ed25519.PubKey` and `multisig.LegacyAminoPubKey`.\n* 2020 October 15: Add `GetAccount` and `GetAccountWithHeight` methods to the `AccountRetriever` interface.\n* 2021 Feb 24: The Cosmos SDK does not use Tendermint's `PubKey` interface anymore, but its own `cryptotypes.PubKey`. Updates to reflect this.\n* 2021 May 3: Rename `clientCtx.JSONMarshaler` to `clientCtx.JSONCodec`.\n* 2021 June 10: Add `clientCtx.Codec: codec.Codec`.\n\n## Status\n\nAccepted\n\n## Context\n\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](./adr-019-protobuf-state-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\n\nSpecifically, the client-side migration path primarily includes tx generation and\nsigning, message construction and routing, in addition to CLI & REST handlers and\nbusiness logic (i.e. queriers).\n\nWith this in mind, we will tackle the migration path via two main areas, txs and\nquerying. However, this ADR solely focuses on transactions. Querying should be\naddressed in a future ADR, but it should build off of these proposals.\n\nBased on detailed discussions ([\\#6030](https://github.com/cosmos/cosmos-sdk/issues/6030)\nand [\\#6078](https://github.com/cosmos/cosmos-sdk/issues/6078)), the original\ndesign for transactions was changed substantially from an `oneof` /JSON-signing\napproach to the approach described below.\n\n## Decision\n\n### Transactions\n\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\n`sdk.Msg`s are encoded with `Any` in transactions.\n\nOne of the main goals of using `Any` to encode interface values is to have a\ncore set of types which is reused by apps so that\nclients can safely be compatible with as many chains as possible.\n\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\nformat that can serve a wide variety of use cases without breaking the client\ncompatibility.\n\nIn order to facilitate signing, transactions are separated into `TxBody`,\nwhich will be reused by `SignDoc` below, and `signatures`:\n\n```protobuf\n// types/types.proto\npackage cosmos_sdk.v1;\n\nmessage Tx {\n    TxBody body = 1;\n    AuthInfo auth_info = 2;\n    // A list of signatures that matches the length and order of AuthInfo's signer_infos to\n    // allow connecting signature meta information like public key and signing mode by position.\n    repeated bytes signatures = 3;\n}\n\n// A variant of Tx that pins the signer's exact binary representation of body and\n// auth_info. This is used for signing, broadcasting and verification. The binary\n// `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\n// becomes the \"txhash\", commonly used as the transaction ID.\nmessage TxRaw {\n    // A protobuf serialization of a TxBody that matches the representation in SignDoc.\n    bytes body = 1;\n    // A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\n    bytes auth_info = 2;\n    // A list of signatures that matches the length and order of AuthInfo's signer_infos to\n    // allow connecting signature meta information like public key and signing mode by position.\n    repeated bytes signatures = 3;\n}\n\nmessage TxBody {\n    // A list of messages to be executed. The required signers of those messages define\n    // the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\n    // Each required signer address is added to the list only the first time it occurs.\n    //\n    // By convention, the first required signer (usually from the first message) is referred\n    // to as the primary signer and pays the fee for the whole transaction.\n    repeated google.protobuf.Any messages = 1;\n    string memo = 2;\n    int64 timeout_height = 3;\n    repeated google.protobuf.Any extension_options = 1023;\n}\n\nmessage AuthInfo {\n    // This list defines the signing modes for the required signers. The number\n    // and order of elements must match the required signers from TxBody's messages.\n    // The first element is the primary signer and the one which pays the fee.\n    repeated SignerInfo signer_infos = 1;\n    // The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\n    Fee fee = 2;\n}\n\nmessage SignerInfo {\n    // The public key is optional for accounts that already exist in state. If unset, the\n    // verifier can use the required signer address for this position and lookup the public key.\n    google.protobuf.Any public_key = 1;\n    // ModeInfo describes the signing mode of the signer and is a nested\n    // structure to support nested multisig pubkey's\n    ModeInfo mode_info = 2;\n    // sequence is the sequence of the account, which describes the\n    // number of committed transactions signed by a given address. It is used to prevent\n    // replay attacks.\n    uint64 sequence = 3;\n}\n\nmessage ModeInfo {\n    oneof sum {\n        Single single = 1;\n        Multi multi = 2;\n    }\n\n    // Single is the mode info for a single signer. It is structured as a message\n    // to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\n    message Single {\n        SignMode mode = 1;\n    }\n\n    // Multi is the mode info for a multisig public key\n    message Multi {\n        // bitarray specifies which keys within the multisig are signing\n        CompactBitArray bitarray = 1;\n        // mode_infos is the corresponding modes of the signers of the multisig\n        // which could include nested multisig public keys\n        repeated ModeInfo mode_infos = 2;\n    }\n}\n\nenum SignMode {\n    SIGN_MODE_UNSPECIFIED = 0;\n\n    SIGN_MODE_DIRECT = 1;\n\n    SIGN_MODE_TEXTUAL = 2;\n\n    SIGN_MODE_LEGACY_AMINO_JSON = 127;\n}\n```\n\nAs will be discussed below, in order to include as much of the `Tx` as possible\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\nraw signatures themselves live outside of what is signed over.\n\nBecause we are aiming for a flexible, extensible cross-chain transaction\nformat, new transaction processing options should be added to `TxBody` as soon\nthose use cases are discovered, even if they can't be implemented yet.\n\nBecause there is coordination overhead in this, `TxBody` includes an\n`extension_options` field which can be used for any transaction processing\noptions that are not already covered. App developers should, nevertheless,\nattempt to upstream important improvements to `Tx`.\n\n### Signing\n\nAll of the signing modes below aim to provide the following guarantees:\n\n* **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\n  is signed\n* **Predictable Gas**: if I am signing a transaction where I am paying a fee,\n  the final gas is fully dependent on what I am signing\n\nThese guarantees give the maximum amount of confidence to message signers that\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\n\n#### `SIGN_MODE_DIRECT`\n\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\nthe wire. This has the advantages of:\n\n* requiring the minimum additional client capabilities beyond a standard protocol\n  buffers implementation\n* leaving effectively zero holes for transaction malleability (i.e. there are no\n  subtle differences between the signing and encoding formats which could\n  potentially be exploited by an attacker)\n\nSignatures are structured using the `SignDoc` below which reuses the serialization of\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\n\n```protobuf\n// types/types.proto\nmessage SignDoc {\n    // A protobuf serialization of a TxBody that matches the representation in TxRaw.\n    bytes body = 1;\n    // A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\n    bytes auth_info = 2;\n    string chain_id = 3;\n    uint64 account_number = 4;\n}\n```\n\nIn order to sign in the default mode, clients take the following steps:\n\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\n2. Create a `SignDoc` and serialize it using [ADR 027](./adr-027-deterministic-protobuf-serialization.md).\n3. Sign the encoded `SignDoc` bytes.\n4. Build a `TxRaw` and serialize it for broadcasting.\n\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https://github.com/regen-network/canonical-proto3)\nalgorithm which creates added complexity for clients in addition to preventing\nsome forms of upgradeability (to be addressed later in this document).\n\nSignature verifiers do:\n\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\n2. Create a list of required signer addresses from the messages.\n3. For each required signer:\n   * Pull account number and sequence from the state.\n   * Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\n   * Create a `SignDoc` and serialize it using [ADR 027](./adr-027-deterministic-protobuf-serialization.md).\n   * Verify the signature at the same list position against the serialized `SignDoc`.\n\n#### `SIGN_MODE_LEGACY_AMINO`\n\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\nsupported transaction signing. Once wallets and exchanges have had a\nchance to upgrade to protobuf-based signing, this option will be disabled. In\nthe meantime, it is foreseen that disabling the current Amino signing would cause\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\nCosmos Hub and other chains may choose to disable Amino signing immediately.\n\nLegacy clients will be able to sign a transaction using the current Amino\nJSON format and have it encoded to protobuf using the REST `/tx/encode`\nendpoint before broadcasting.\n\n#### `SIGN_MODE_TEXTUAL`\n\nAs was discussed extensively in [\\#6078](https://github.com/cosmos/cosmos-sdk/issues/6078),\nthere is a desire for a human-readable signing encoding, especially for hardware\nwallets like the [Ledger](https://www.ledger.com) which display\ntransaction contents to users before signing. JSON was an attempt at this but\nfalls short of the ideal.\n\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\nencoding which will replace Amino JSON. This new encoding should be even more\nfocused on readability than JSON, possibly based on formatting strings like\n[MessageFormat](http://userguide.icu-project.org/formatparse/messages).\n\nIn order to ensure that the new human-readable format does not suffer from\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\nto generate sign bytes.\n\nMultiple human-readable formats (maybe even localized messages) may be supported\nby `SIGN_MODE_TEXTUAL` when it is implemented.\n\n### Unknown Field Filtering\n\nUnknown fields in protobuf messages should generally be rejected by the transaction\nprocessors because:\n\n* important data may be present in the unknown fields, that if ignored, will\n  cause unexpected behavior for clients\n* they present a malleability vulnerability where attackers can bloat tx size\n  by adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\n  not `TxBody`)\n\nThere are also scenarios where we may choose to safely ignore unknown fields\n(https://github.com/cosmos/cosmos-sdk/issues/6078#issuecomment-624400188) to\nprovide graceful forwards compatibility with newer clients.\n\nWe propose that field numbers with bit 11 set (for most use cases this is\nthe range of 1024-2047) be considered non-critical fields that can safely be\nignored if unknown.\n\nTo handle this we will need an unknown field filter that:\n\n* always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\n  unsigned parts of `AuthInfo` if present based on the signing mode)\n* rejects unknown fields in all messages (including nested `Any`s) other than\n  fields with bit 11 set\n\nThis will likely need to be a custom protobuf parser pass that takes message bytes\nand `FileDescriptor`s and returns a boolean result.\n\n### Public Key Encoding\n\nPublic keys in the Cosmos SDK implement the `cryptotypes.PubKey` interface.\nWe propose to use `Any` for protobuf encoding as we are doing with other interfaces (for example, in `BaseAccount.PubKey` and `SignerInfo.PublicKey`).\nThe following public keys are implemented: secp256k1, secp256r1, ed25519 and legacy-multisignature.\n\nEx:\n\n```protobuf\nmessage PubKey {\n    bytes key = 1;\n}\n```\n\n`multisig.LegacyAminoPubKey` has an array of `Any`'s member to support any\nprotobuf public key type.\n\nApps should only attempt to handle a registered set of public keys that they\nhave tested. The provided signature verification ante handler decorators will\nenforce this.\n\n### CLI & REST\n\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\nin the client can be interfaces, similar to how we described in [ADR 019](./adr-019-protobuf-state-encoding.md),\nthe client logic will now need to take a codec interface that knows not only how\nto handle all the types, but also knows how to generate transactions, signatures,\nand messages.\n\n```go\ntype AccountRetriever interface {\n  GetAccount(clientCtx Context, addr sdk.AccAddress) (client.Account, error)\n  GetAccountWithHeight(clientCtx Context, addr sdk.AccAddress) (client.Account, int64, error)\n  EnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\n  GetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\n}\n\ntype Generator interface {\n  NewTx() TxBuilder\n  NewFee() ClientFee\n  NewSignature() ClientSignature\n  MarshalTx(tx types.Tx) ([]byte, error)\n}\n\ntype TxBuilder interface {\n  GetTx() sdk.Tx\n\n  SetMsgs(...sdk.Msg) error\n  GetSignatures() []sdk.Signature\n  SetSignatures(...sdk.Signature)\n  GetFee() sdk.Fee\n  SetFee(sdk.Fee)\n  GetMemo() string\n  SetMemo(string)\n}\n```\n\nWe then update `Context` to have new fields: `Codec`, `TxGenerator`,\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\na `Context` which should have all of these fields pre-populated.\n\nEach client method should then use one of the `Init` methods to re-initialize\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\ngenerate or broadcast a transaction. For example:\n\n```go\nimport \"github.com/spf13/cobra\"\nimport \"github.com/cosmos/cosmos-sdk/client\"\nimport \"github.com/cosmos/cosmos-sdk/client/tx\"\n\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\n\treturn &cobra.Command{\n\t\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\t\tclientCtx := ctx.InitWithInput(cmd.InOrStdin())\n\t\t\tmsg := NewSomeMsg{...}\n\t\t\ttx.GenerateOrBroadcastTx(clientCtx, msg)\n\t\t},\n\t}\n}\n```\n\n## Future Improvements\n\n### `SIGN_MODE_TEXTUAL` specification\n\nA concrete specification and implementation of `SIGN_MODE_TEXTUAL` is intended\nas a near-term future improvement so that the ledger app and other wallets\ncan gracefully transition away from Amino JSON.\n\n### `SIGN_MODE_DIRECT_AUX`\n\n(\\*Documented as option (3) in https://github.com/cosmos/cosmos-sdk/issues/6078#issuecomment-628026933)\n\nWe could add a mode `SIGN_MODE_DIRECT_AUX`\nto support scenarios where multiple signatures\nare being gathered into a single transaction but the message composer does not\nyet know which signatures will be included in the final transaction. For instance,\nI may have a 3/5 multisig wallet and want to send a `TxBody` to all 5\nsigners to see who signs first. As soon as I have 3 signatures then I will go\nahead and build the full transaction.\n\nWith `SIGN_MODE_DIRECT`, each signer needs\nto sign the full `AuthInfo` which includes the full list of all signers and\ntheir signing modes, making the above scenario very hard.\n\n`SIGN_MODE_DIRECT_AUX` would allow \"auxiliary\" signers to create their signature\nusing only `TxBody` and their own `PublicKey`. This allows the full list of\nsigners in `AuthInfo` to be delayed until signatures have been collected.\n\nAn \"auxiliary\" signer is any signer besides the primary signer who is paying\nthe fee. For the primary signer, the full `AuthInfo` is actually needed to calculate gas and fees\nbecause that is dependent on how many signers and which key types and signing\nmodes they are using. Auxiliary signers, however, do not need to worry about\nfees or gas and thus can just sign `TxBody`.\n\nTo generate a signature in `SIGN_MODE_DIRECT_AUX` these steps would be followed:\n\n1. Encode `SignDocAux` (with the same requirement that fields must be serialized\n   in order):\n\n    ```protobuf\n    // types/types.proto\n    message SignDocAux {\n        bytes body_bytes = 1;\n        // PublicKey is included in SignDocAux :\n        // 1. as a special case for multisig public keys. For multisig public keys,\n        // the signer should use the top-level multisig public key they are signing\n        // against, not their own public key. This is to prevent a form\n        // of malleability where a signature could be taken out of context of the\n        // multisig key that was intended to be signed for\n        // 2. to guard against scenario where configuration information is encoded\n        // in public keys (it has been proposed) such that two keys can generate\n        // the same signature but have different security properties\n        //\n        // By including it here, the composer of AuthInfo cannot reference the\n        // a public key variant the signer did not intend to use\n        PublicKey public_key = 2;\n        string chain_id = 3;\n        uint64 account_number = 4;\n    }\n    ```\n\n2. Sign the encoded `SignDocAux` bytes\n3. Send their signature and `SignerInfo` to the primary signer who will then\n   sign and broadcast the final transaction (with `SIGN_MODE_DIRECT` and `AuthInfo`\n   added) once enough signatures have been collected\n\n### `SIGN_MODE_DIRECT_RELAXED`\n\n(_Documented as option (1)(a) in https://github.com/cosmos/cosmos-sdk/issues/6078#issuecomment-628026933_)\n\nThis is a variation of `SIGN_MODE_DIRECT` where multiple signers wouldn't need to\ncoordinate public keys and signing modes in advance. It would involve an alternate\n`SignDoc` similar to `SignDocAux` above with fee. This could be added in the future\nif client developers found the burden of collecting public keys and modes in advance\ntoo burdensome.\n\n## Consequences\n\n### Positive\n\n* Significant performance gains.\n* Supports backward and forward type compatibility.\n* Better support for cross-language clients.\n* Multiple signing modes allow for greater protocol evolution\n\n### Negative\n\n* `google.protobuf.Any` type URLs increase transaction size although the effect\n  may be negligible or compression may be able to mitigate it.\n\n### Neutral\n\n## References"
  },
  {
    "number": 21,
    "filename": "adr-021-protobuf-query-encoding.md",
    "title": "ADR 021: Protocol Buffer Query Encoding",
    "content": "# ADR 021: Protocol Buffer Query Encoding\n\n## Changelog\n\n* 2020 March 27: Initial Draft\n\n## Status\n\nAccepted\n\n## Context\n\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](./adr-019-protobuf-state-encoding.md) and\n[ADR 020](./adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\n\nThis ADR continues from [ADR 020](./adr-020-protobuf-transaction-encoding.md)\nto specify the encoding of queries.\n\n## Decision\n\n### Custom Query Definition\n\nModules define custom queries through a protocol buffers `service` definition.\nThese `service` definitions are generally associated with and used by the\nGRPC protocol. However, the protocol buffers specification indicates that\nthey can be used more generically by any request/response protocol that uses\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\n\nEach module with custom queries should define a service canonically named `Query`:\n\n```protobuf\n// x/bank/types/types.proto\n\nservice Query {\n  rpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\n  rpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\n}\n```\n\n#### Handling of Interface Types\n\nModules that use interface types and need true polymorphism generally force a\n`oneof` up to the app-level that provides the set of concrete implementations of\nthat interface that the app supports. While app's are welcome to do the same for\nqueries and implement an app-level query service, it is recommended that modules\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\nThere is a concern on the transaction level that the overhead of `Any` is too\nhigh to justify its usage. However for queries this is not a concern, and\nproviding generic module-level queries that use `Any` does not preclude apps\nfrom also providing app-level queries that return using the app-level `oneof`s.\n\nA hypothetical example for the `gov` module would look something like:\n\n```protobuf\n// x/gov/types/types.proto\n\nimport \"google/protobuf/any.proto\";\n\nservice Query {\n  rpc GetProposal(GetProposalParams) returns (AnyProposal) { }\n}\n\nmessage AnyProposal {\n  ProposalBase base = 1;\n  google.protobuf.Any content = 2;\n}\n```\n\n### Custom Query Implementation\n\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https://github.com/cosmos/gogoproto)\ngrpc plugin, which for a service named `Query` generates an interface named\n`QueryServer` as below:\n\n```go\ntype QueryServer interface {\n\tQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\n\tQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\n}\n```\n\nThe custom queries for our module are implemented by implementing this interface.\n\nThe first parameter in this generated interface is a generic `context.Context`,\nwhereas querier methods generally need an instance of `sdk.Context` to read\nfrom the store. Since arbitrary values can be attached to `context.Context`\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\n`context.Context`.\n\nAn example implementation of `QueryBalance` for the bank module as above would\nlook something like:\n\n```go\ntype Querier struct {\n\tKeeper\n}\n\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\n\tbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\n\treturn &balance, nil\n}\n```\n\n### Custom Query Registration and Routing\n\nQuery server implementations as above would be registered with `AppModule`s using\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\nas below:\n\n```go\n// x/bank/module.go\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\n\ttypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\n}\n```\n\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\nquery routing table (with the routing method being described below).\nThe signature for this method matches the existing\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\nquery server implementation described above.\n\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\nand method name (ex. `QueryBalance`) combined with `/`s to form a full\nmethod name (ex. `/cosmos_sdk.x.bank.v1.Query/QueryBalance`). This gets translated\ninto an ABCI query as `custom/cosmos_sdk.x.bank.v1.Query/QueryBalance`. Service handlers\nregistered with `QueryRouter.RegisterService` will be routed this way.\n\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\n`sdk.Query` and `QueryRouter` infrastructure.\n\nThis basic specification allows us to reuse protocol buffer `service` definitions\nfor ABCI custom queries substantially reducing the need for manual decoding and\nencoding in query methods.\n\n### GRPC Protocol Support\n\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\nproxy server that routes requests in the GRPC protocol to ABCI query requests\nunder the hood. In this way, clients could use their host languages' existing\nGRPC implementations to make direct queries against Cosmos SDK app's using\nthese `service` definitions. In order for this server to work, the `QueryRouter`\non `BaseApp` will need to expose the service handlers registered with\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\nlaunch the proxy server on a separate port in the same process as the ABCI app\nwith a command-line flag.\n\n### REST Queries and Swagger Generation\n\n[grpc-gateway](https://github.com/grpc-ecosystem/grpc-gateway) is a project that\ntranslates REST calls into GRPC calls using special annotations on service\nmethods. Modules that want to expose REST queries should add `google.api.http`\nannotations to their `rpc` methods as in this example below.\n\n```protobuf\n// x/bank/types/types.proto\n\nservice Query {\n  rpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\n    option (google.api.http) = {\n      get: \"/x/bank/v1/balance/{address}/{denom}\"\n    };\n  }\n  rpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\n    option (google.api.http) = {\n      get: \"/x/bank/v1/balances/{address}\"\n    };\n  }\n}\n```\n\ngrpc-gateway will work directly against the GRPC proxy described above which will\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\ngenerate Swagger definitions automatically.\n\nIn the current implementation of REST queries, each module needs to implement\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\napproach, there will be no need to generate separate REST query handlers, just\nquery servers as described above as grpc-gateway handles the translation of protobuf\nto REST as well as Swagger definitions.\n\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\na separate process or the same process as the ABCI app, as well as provide a\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\nfile.\n\n### Client Usage\n\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\ninterface like:\n\n```go\ntype QueryClient interface {\n\tQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\n\tQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\n}\n```\n\nVia a small patch to gogo protobuf ([gogo/protobuf#675](https://github.com/gogo/protobuf/pull/675))\nwe have tweaked the grpc codegen to use an interface rather than a concrete type\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\nfor ABCI client queries.\n\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\nthat routes calls to ABCI queries\n\nClients (such as CLI methods) will then be able to call query methods like this:\n\n```go\nclientCtx := client.NewContext()\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\nparams := &types.QueryBalanceParams{addr, denom}\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\n```\n\n### Testing\n\nTests would be able to create a query client directly from keeper and `sdk.Context`\nreferences using a `QueryServerTestHelper` as below:\n\n```go\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\nqueryClient := types.NewQueryClient(queryHelper)\n```\n\n## Future Improvements\n\n## Consequences\n\n### Positive\n\n* greatly simplified querier implementation (no manual encoding/decoding)\n* easy query client generation (can use existing grpc and swagger tools)\n* no need for REST query implementations\n* type safe query methods (generated via grpc plugin)\n* going forward, there will be less breakage of query methods because of the\nbackwards compatibility guarantees provided by buf\n\n### Negative\n\n* all clients using the existing ABCI/REST queries will need to be refactored\nfor both the new GRPC/REST query paths as well as protobuf/proto-json encoded\ndata, but this is more or less unavoidable in the protobuf refactoring\n\n### Neutral\n\n## References"
  },
  {
    "number": 22,
    "filename": "adr-022-custom-panic-handling.md",
    "title": "ADR 022: Custom BaseApp panic handling",
    "content": "# ADR 022: Custom BaseApp panic handling\n\n## Changelog\n\n* 2020 Apr 24: Initial Draft\n* 2021 Sep 14: Superseded by ADR-045\n\n## Status\n\nSUPERSEDED by ADR-045\n\n## Context\n\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\n[runTx()](https://github.com/cosmos/cosmos-sdk/blob/bad4ca75f58b182f600396ca350ad844c18fc80b/baseapp/baseapp.go#L539)\nmethod. We think that this method can be more flexible and can give Cosmos SDK users more options for customizations without\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\nmight be handled in a \"standard\" way (middleware) alongside the others.\n\nWe propose middleware-solution, which could help developers implement the following cases:\n\n* add external logging (let's say sending reports to external services like [Sentry](https://sentry.io));\n* call panic for specific error cases;\n\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#recovery-middleware)).\n\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\nto crash the node and reboot it once the problem is solved. That behaviour makes the node's state machine execution\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\nin order to customize it.\n\n## Decision\n\n### Design\n\n#### Overview\n\nInstead of hardcoding custom error handling into BaseApp we suggest using a set of middlewares which can be customized\nexternally and will allow developers to use as many custom error handlers as they want. Implementation with tests\ncan be found [here](https://github.com/cosmos/cosmos-sdk/pull/6053).\n\n#### Implementation details\n\n##### Recovery handler\n\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\n`recover()` from the `builtin` package.\n\n```go\ntype RecoveryHandler func(recoveryObj interface{}) error\n```\n\nHandler should type assert (or other methods) an object to define if the object should be handled.\n`nil` should be returned if the input object can't be handled by that `RecoveryHandler` (not a handler's target type).\nNot `nil` error should be returned if the input object was handled and the middleware chain execution should be stopped.\n\nAn example:\n\n```go\nfunc exampleErrHandler(recoveryObj interface{}) error {\n    err, ok := recoveryObj.(error)\n    if !ok { return nil }\n\n    if someSpecificError.Is(err) {\n        panic(customPanicMsg)\n    } else {\n        return nil\n    }\n}\n```\n\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\n\n##### Recovery middleware\n\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\nchain processing.\n\n```go\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\n\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\n    return func(recoveryObj interface{}) (recoveryMiddleware, error) {\n        if err := handler(recoveryObj); err != nil {\n            return nil, err\n        }\n        return next, nil\n    }\n}\n```\n\nFunction receives a `recoveryObj` object and returns:\n\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\n\n`OutOfGas` middleware example:\n\n```go\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\n    handler := func(recoveryObj interface{}) error {\n        err, ok := recoveryObj.(sdk.ErrorOutOfGas)\n        if !ok { return nil }\n\n        return errorsmod.Wrap(\n            sdkerrors.ErrOutOfGas, fmt.Sprintf(\n                \"out of gas in location: %v; gasWanted: %d, gasUsed: %d\", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\n            ),\n        )\n    }\n\n    return newRecoveryMiddleware(handler, next)\n}\n```\n\n`Default` middleware example:\n\n```go\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\n    handler := func(recoveryObj interface{}) error {\n        return errorsmod.Wrap(\n            sdkerrors.ErrPanic, fmt.Sprintf(\"recovered: %v\\nstack:\\n%v\", recoveryObj, string(debug.Stack())),\n        )\n    }\n\n    return newRecoveryMiddleware(handler, nil)\n}\n```\n\n##### Recovery processing\n\nBasic chain of middlewares processing would look like:\n\n```go\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\n\tif middleware == nil { return nil }\n\n\tnext, err := middleware(recoveryObj)\n\tif err != nil { return err }\n\tif next == nil { return nil }\n\n\treturn processRecovery(recoveryObj, next)\n}\n```\n\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\n`default` handler which must return an `error`.\n\n##### BaseApp changes\n\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\n\n```go\ntype BaseApp struct {\n    // ...\n    runTxRecoveryMiddleware recoveryMiddleware\n}\n\nfunc NewBaseApp(...) {\n    // ...\n    app.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\n}\n\nfunc (app *BaseApp) runTx(...) {\n    // ...\n    defer func() {\n        if r := recover(); r != nil {\n            recoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\n            err, result = processRecovery(r, recoveryMW), nil\n        }\n\n        gInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\n    }()\n    // ...\n}\n```\n\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\n\n```go\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\n    for _, h := range handlers {\n        app.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\n    }\n}\n```\n\nThis method would prepend handlers to an existing chain.\n\n## Consequences\n\n### Positive\n\n* Developers of Cosmos SDK-based projects can add custom panic handlers to:\n    * add error context for custom panic sources (panic inside of custom keepers);\n    * emit `panic()`: passthrough recovery object to the Tendermint core;\n    * other necessary handling;\n* Developers can use standard Cosmos SDK `BaseApp` implementation, rather than rewriting it in their projects;\n* Proposed solution doesn't break the current \"standard\" `runTx()` flow;\n\n### Negative\n\n* Introduces changes to the execution model design.\n\n### Neutral\n\n* `OutOfGas` error handler becomes one of the middlewares;\n* Default panic handler becomes one of the middlewares;\n\n## References\n\n* [PR-6053 with proposed solution](https://github.com/cosmos/cosmos-sdk/pull/6053)\n* [Similar solution. ADR-010 Modular AnteHandler](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-010-modular-antehandler.md)"
  },
  {
    "number": 23,
    "filename": "adr-023-protobuf-naming.md",
    "title": "ADR 023: Protocol Buffer Naming and Versioning Conventions",
    "content": "# ADR 023: Protocol Buffer Naming and Versioning Conventions\n\n## Changelog\n\n* 2020 April 27: Initial Draft\n* 2020 August 5: Update guidelines\n\n## Status\n\nAccepted\n\n## Context\n\nProtocol Buffers provide a basic [style guide](https://developers.google.com/protocol-buffers/docs/style)\nand [Buf](https://buf.build/docs/style-guide) builds upon that. To the\nextent possible, we want to follow industry accepted guidelines and wisdom for\nthe effective usage of protobuf, deviating from those only when there is clear\nrationale for our use case.\n\n### Adoption of `Any`\n\nThe adoption of `google.protobuf.Any` as the recommended approach for encoding\ninterface types (as opposed to `oneof`) makes package naming a central part\nof the encoding as fully-qualified message names now appear in encoded\nmessages.\n\n### Current Directory Organization\n\nThus far we have mostly followed [Buf's](https://buf.build) [DEFAULT](https://buf.build/docs/lint-checkers#default)\nrecommendations, with the minor deviation of disabling [`PACKAGE_DIRECTORY_MATCH`](https://buf.build/docs/lint-checkers#file_layout)\nwhich although being convenient for developing code comes with the warning\nfrom Buf that:\n\n> you will have a very bad time with many Protobuf plugins across various languages if you do not do this\n\n### Adoption of gRPC Queries\n\nIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobuf\nnative queries. The full gRPC service path thus becomes a key part of ABCI query\npath. In the future, gRPC queries may be allowed from within persistent scripts\nby technologies such as CosmWasm and these query routes would be stored within\nscript binaries.\n\n## Decision\n\nThe goal of this ADR is to provide thoughtful naming conventions that:\n\n* encourage a good user experience for when users interact directly with\n.proto files and fully-qualified protobuf names\n* balance conciseness against the possibility of either over-optimizing (making\nnames too short and cryptic) or under-optimizing (just accepting bloated names\nwith lots of redundant information)\n\nThese guidelines are meant to act as a style guide for both the Cosmos SDK and\nthird-party modules.\n\nAs a starting point, we should adopt all of the [DEFAULT](https://buf.build/docs/lint-checkers#default)\ncheckers in [Buf's](https://buf.build) including [`PACKAGE_DIRECTORY_MATCH`](https://buf.build/docs/lint-checkers#file_layout),\nexcept:\n\n* [PACKAGE_VERSION_SUFFIX](https://buf.build/docs/lint-checkers#package_version_suffix)\n* [SERVICE_SUFFIX](https://buf.build/docs/lint-checkers#service_suffix)\n\nFurther guidelines to be described below.\n\n### Principles\n\n#### Concise and Descriptive Names\n\nNames should be descriptive enough to convey their meaning and distinguish\nthem from other names.\n\nGiven that we are using fully-qualified names within\n`google.protobuf.Any` as well as within gRPC query routes, we should aim to\nkeep names concise, without going overboard. The general rule of thumb should\nbe if a shorter name would convey more or else the same thing, pick the shorter\nname.\n\nFor instance, `cosmos.bank.MsgSend` (19 bytes) conveys roughly the same information\nas `cosmos_sdk.x.bank.v1.MsgSend` (28 bytes) but is more concise.\n\nSuch conciseness makes names both more pleasant to work with and take up less\nspace within transactions and on the wire.\n\nWe should also resist the temptation to over-optimize, by making names\ncryptically short with abbreviations. For instance, we shouldn't try to\nreduce `cosmos.bank.MsgSend` to `csm.bk.MSnd` just to save a few bytes.\n\nThe goal is to make names **_concise but not cryptic_**.\n\n#### Names are for Clients First\n\nPackage and type names should be chosen for the benefit of users, not\nnecessarily because of legacy concerns related to the go code-base.\n\n#### Plan for Longevity\n\nIn the interests of long-term support, we should plan on the names we do\nchoose to be in usage for a long time, so now is the opportunity to make\nthe best choices for the future.\n\n### Versioning\n\n#### Guidelines on Stable Package Versions\n\nIn general, schema evolution is the way to update protobuf schemas. That means that new fields,\nmessages, and RPC methods are _added_ to existing schemas and old fields, messages and RPC methods\nare maintained as long as possible.\n\nBreaking things is often unacceptable in a blockchain scenario. For instance, immutable smart contracts\nmay depend on certain data schemas on the host chain. If the host chain breaks those schemas, the smart\ncontract may be irreparably broken. Even when things can be fixed (for instance in client software),\nthis often comes at a high cost.\n\nInstead of breaking things, we should make every effort to evolve schemas rather than just breaking them.\n[Buf](https://buf.build) breaking change detection should be used on all stable (non-alpha or beta) packages\nto prevent such breakage.\n\nWith that in mind, different stable versions (i.e. `v1` or `v2`) of a package should more or less be considered\ndifferent packages and this should be a last resort approach for upgrading protobuf schemas. Scenarios where creating\na `v2` may make sense are:\n\n* we want to create a new module with similar functionality to an existing module and adding `v2` is the most natural\nway to do this. In that case, there are really just two different, but similar modules with different APIs.\n* we want to add a new revamped API for an existing module and it's just too cumbersome to add it to the existing package,\nso putting it in `v2` is cleaner for users. In this case, care should be made to not deprecate support for\n`v1` if it is actively used in immutable smart contracts.\n\n#### Guidelines on unstable (alpha and beta) package versions\n\nThe following guidelines are recommended for marking packages as alpha or beta:\n\n* marking something as `alpha` or `beta` should be a last resort and just putting something in the\nstable package (i.e. `v1` or `v2`) should be preferred\n* a package _should_ be marked as `alpha` _if and only if_ there are active discussions to remove\nor significantly alter the package in the near future\n* a package _should_ be marked as `beta` _if and only if_ there is an active discussion to\nsignificantly refactor/rework the functionality in the near future but do not remove it\n* modules _can and should_ have types in both stable (i.e. `v1` or `v2`) and unstable (`alpha` or `beta`) packages.\n\n_`alpha` and `beta` should not be used to avoid responsibility for maintaining compatibility._\nWhenever code is released into the wild, especially on a blockchain, there is a high cost to changing things. In some\ncases, for instance with immutable smart contracts, a breaking change may be impossible to fix.\n\nWhen marking something as `alpha` or `beta`, maintainers should ask the following questions:\n\n* what is the cost of asking others to change their code vs the benefit of us maintaining the optionality to change it?\n* what is the plan for moving this to `v1` and how will that affect users?\n\n`alpha` or `beta` should really be used to communicate \"changes are planned\".\n\nAs a case study, gRPC reflection is in the package `grpc.reflection.v1alpha`. It hasn't been changed since\n2017 and it is now used in other widely used software like gRPCurl. Some folks probably use it in production services\nand so if they actually went and changed the package to `grpc.reflection.v1`, some software would break and\nthey probably don't want to do that... So now the `v1alpha` package is more or less the de-facto `v1`. Let's not do that.\n\nThe following are guidelines for working with non-stable packages:\n\n* [Buf's recommended version suffix](https://buf.build/docs/lint-checkers#package_version_suffix)\n(ex. `v1alpha1`) _should_ be used for non-stable packages\n* non-stable packages should generally be excluded from breaking change detection\n* immutable smart contract modules (i.e. CosmWasm) _should_ block smart contracts/persistent\nscripts from interacting with `alpha`/`beta` packages\n\n#### Omit v1 suffix\n\nInstead of using [Buf's recommended version suffix](https://buf.build/docs/lint-checkers#package_version_suffix),\nwe can omit `v1` for packages that don't actually have a second version. This\nallows for more concise names for common use cases like `cosmos.bank.Send`.\nPackages that do have a second or third version can indicate that with `.v2`\nor `.v3`.\n\n### Package Naming\n\n#### Adopt a short, unique top-level package name\n\nTop-level packages should adopt a short name that is known not to collide with\nother names in common usage within the Cosmos ecosystem. In the near future, a\nregistry should be created to reserve and index top-level package names used\nwithin the Cosmos ecosystem. Because the Cosmos SDK is intended to provide\nthe top-level types for the Cosmos project, the top-level package name `cosmos`\nis recommended for usage within the Cosmos SDK instead of the longer `cosmos_sdk`.\n[ICS](https://github.com/cosmos/ics) specifications could consider a\nshort top-level package like `ics23` based upon the standard number.\n\n#### Limit sub-package depth\n\nSub-package depth should be increased with caution. Generally a single\nsub-package is needed for a module or a library. Even though `x` or `modules`\nis used in source code to denote modules, this is often unnecessary for .proto\nfiles as modules are the primary thing sub-packages are used for. Only items which\nare known to be used infrequently should have deep sub-package depths.\n\nFor the Cosmos SDK, it is recommended that we simply write `cosmos.bank`,\n`cosmos.gov`, etc. rather than `cosmos.x.bank`. In practice, most non-module\ntypes can go straight in the `cosmos` package or we can introduce a\n`cosmos.base` package if needed. Note that this naming _will not_ change\ngo package names, i.e. the `cosmos.bank` protobuf package will still live in\n`x/bank`.\n\n### Message Naming\n\nMessage type names should be as concise as possible without losing clarity. `sdk.Msg`\ntypes which are used in transactions will retain the `Msg` prefix as that provides\nhelpful context.\n\n### Service and RPC Naming\n\n[ADR 021](adr-021-protobuf-query-encoding.md) specifies that modules should\nimplement a gRPC query service. We should consider the principle of conciseness\nfor query service and RPC names as these may be called from persistent script\nmodules such as CosmWasm. Also, users may use these query paths from tools like\n[gRPCurl](https://github.com/fullstorydev/grpcurl). As an example, we can shorten\n`/cosmos_sdk.x.bank.v1.QueryService/QueryBalance` to\n`/cosmos.bank.Query/Balance` without losing much useful information.\n\nRPC request and response types _should_ follow the `ServiceNameMethodNameRequest`/\n`ServiceNameMethodNameResponse` naming convention. i.e. for an RPC method named `Balance`\non the `Query` service, the request and response types would be `QueryBalanceRequest`\nand `QueryBalanceResponse`. This will be more self-explanatory than `BalanceRequest`\nand `BalanceResponse`.\n\n#### Use just `Query` for the query service\n\nInstead of [Buf's default service suffix recommendation](https://github.com/cosmos/cosmos-sdk/pull/6033),\nwe should simply use the shorter `Query` for query services.\n\nFor other types of gRPC services, we should consider sticking with Buf's\ndefault recommendation.\n\n#### Omit `Get` and `Query` from query service RPC names\n\n`Get` and `Query` should be omitted from `Query` service names because they are\nredundant in the fully-qualified name. For instance, `/cosmos.bank.Query/QueryBalance`\njust says `Query` twice without any new information.\n\n## Future Improvements\n\nA registry of top-level package names should be created to coordinate naming\nacross the ecosystem, prevent collisions, and also help developers discover\nuseful schemas. A simple starting point would be a git repository with\ncommunity-based governance.\n\n## Consequences\n\n### Positive\n\n* names will be more concise and easier to read and type\n* all transactions using `Any` will be at shorter (`_sdk.x` and `.v1` will be removed)\n* `.proto` file imports will be more standard (without `\"third_party/proto\"` in\nthe path)\n* code generation will be easier for clients because .proto files will be\nin a single `proto/` directory which can be copied rather than scattered\nthroughout the Cosmos SDK\n\n### Negative\n\n### Neutral\n\n* `.proto`  files will need to be reorganized and refactored\n* some modules may need to be marked as alpha or beta\n\n## References"
  },
  {
    "number": 24,
    "filename": "adr-024-coin-metadata.md",
    "title": "ADR 024: Coin Metadata",
    "content": "# ADR 024: Coin Metadata\n\n## Changelog\n\n* 05/19/2020: Initial draft\n\n## Status\n\nProposed\n\n## Context\n\nAssets in the Cosmos SDK are represented via a `Coins` type that consists of an `amount` and a `denom`,\nwhere the `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an\naccount-based model where there are two types of primary accounts -- basic accounts and module accounts.\nAll account types have a set of balances that are composed of `Coins`. The `x/bank` module keeps\ntrack of all balances for all accounts and also keeps track of the total supply of balances in an\napplication.\n\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based\nchain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by\nthe time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single\nunit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and\noperations in the Cosmos SDK work off of units of 10^6.\n\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and\nas a result the total amount of asset types increases. We propose to have `x/bank` additionally keep\ntrack of metadata per `denom` in order to help clients, wallet providers, and explorers improve their\nUX and remove the requirement for making any assumptions on the unit of denomination.\n\n## Decision\n\nThe `x/bank` module will be updated to store and index metadata by `denom`, specifically the \"base\" or\nsmallest unit -- the unit the Cosmos SDK state-machine works with.\n\nMetadata may also include a non-zero length list of denominations. Each entry contains the name of\nthe denomination `denom`, the exponent to the base and a list of aliases. An entry is to be\ninterpreted as `1 denom = 10^exponent base_denom` (e.g. `1 ETH = 10^18 wei` and `1 uatom = 10^0 uatom`).\n\nThere are two denominations that are of high importance for clients: the `base`, which is the smallest\npossible unit and the `display`, which is the unit that is commonly referred to in human communication\nand on exchanges. The values in those fields link to an entry in the list of denominations.\n\nThe list in `denom_units` and the `display` entry may be changed via governance.\n\nAs a result, we can define the type as follows:\n\n```protobuf\nmessage DenomUnit {\n  string denom    = 1;\n  uint32 exponent = 2;  \n  repeated string aliases = 3;\n}\n\nmessage Metadata {\n  string description = 1;\n  repeated DenomUnit denom_units = 2;\n  string base = 3;\n  string display = 4;\n}\n```\n\nAs an example, the ATOM's metadata can be defined as follows:\n\n```json\n{\n  \"name\": \"atom\",\n  \"description\": \"The native staking token of the Cosmos Hub.\",\n  \"denom_units\": [\n    {\n      \"denom\": \"uatom\",\n      \"exponent\": 0,\n      \"aliases\": [\n        \"microatom\"\n      ],\n    },\n    {\n      \"denom\": \"matom\",\n      \"exponent\": 3,\n      \"aliases\": [\n        \"milliatom\"\n      ]\n    },\n    {\n      \"denom\": \"atom\",\n      \"exponent\": 6,\n    }\n  ],\n  \"base\": \"uatom\",\n  \"display\": \"atom\",\n}\n```\n\nGiven the above metadata, a client may infer the following things:\n\n* 4.3atom = 4.3 * (10^6) = 4,300,000uatom\n* The string \"atom\" can be used as a display name in a list of tokens.\n* The balance 4300000 can be displayed as 4,300,000uatom or 4,300matom or 4.3atom.\n  The `display` denomination 4.3atom is a good default if the authors of the client don't make\n  an explicit decision to choose a different representation.\n\nA client should be able to query for metadata by denom both via the CLI and REST interfaces. In\naddition, we will add handlers to these interfaces to convert from any unit to another given unit,\nas the base framework for this already exists in the Cosmos SDK.\n\nFinally, we need to ensure metadata exists in the `GenesisState` of the `x/bank` module which is also\nindexed by the base `denom`.\n\n```go\ntype GenesisState struct {\n  SendEnabled   bool        `json:\"send_enabled\" yaml:\"send_enabled\"`\n  Balances      []Balance   `json:\"balances\" yaml:\"balances\"`\n  Supply        sdk.Coins   `json:\"supply\" yaml:\"supply\"`\n  DenomMetadata []Metadata  `json:\"denom_metadata\" yaml:\"denom_metadata\"`\n}\n```\n\n## Future Work\n\nIn order for clients to avoid having to convert assets to the base denomination -- either manually or\nvia an endpoint, we may consider supporting automatic conversion of a given unit input.\n\n## Consequences\n\n### Positive\n\n* Provides clients, wallet providers and block explorers with additional data on\n  asset denomination to improve UX and remove any need to make assumptions on\n  denomination units.\n\n### Negative\n\n* A small amount of required additional storage in the `x/bank` module. The amount\n  of additional storage should be minimal as the amount of total assets should not\n  be large.\n\n### Neutral\n\n## References"
  },
  {
    "number": 27,
    "filename": "adr-027-deterministic-protobuf-serialization.md",
    "title": "ADR 027: Deterministic Protobuf Serialization",
    "content": "# ADR 027: Deterministic Protobuf Serialization\n\n## Changelog\n\n* 2020-08-07: Initial Draft\n* 2020-09-01: Further clarify rules\n\n## Status\n\nProposed\n\n## Abstract\n\nFully deterministic structure serialization, which works across many languages and clients,\nis needed when signing messages. We need to be sure that whenever we serialize\na data structure, no matter in which supported language, the raw bytes\nwill stay the same.\n[Protobuf](https://developers.google.com/protocol-buffers/docs/proto3)\nserialization is not bijective (i.e. there exists a practically unlimited number of\nvalid binary representations for a given protobuf document)<sup>1</sup>.\n\nThis document describes a deterministic serialization scheme for\na subset of protobuf documents, that covers this use case but can be reused in\nother cases as well.\n\n### Context\n\nFor signature verification in Cosmos SDK, the signer and verifier need to agree on\nthe same serialization of a `SignDoc` as defined in\n[ADR-020](./adr-020-protobuf-transaction-encoding.md) without transmitting the\nserialization.\n\nCurrently, for block signatures we are using a workaround: we create a new [TxRaw](https://github.com/cosmos/cosmos-sdk/blob/9e85e81e0e8140067dd893421290c191529c148c/proto/cosmos/tx/v1beta1/tx.proto#L30)\ninstance (as defined in [adr-020-protobuf-transaction-encoding](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-020-protobuf-transaction-encoding.md#transactions))\nby converting all [Tx](https://github.com/cosmos/cosmos-sdk/blob/9e85e81e0e8140067dd893421290c191529c148c/proto/cosmos/tx/v1beta1/tx.proto#L13)\nfields to bytes on the client side. This adds an additional manual\nstep when sending and signing transactions.\n\n### Decision\n\nThe following encoding scheme is to be used by other ADRs,\nand in particular for `SignDoc` serialization.\n\n## Specification\n\n### Scope\n\nThis ADR defines a protobuf3 serializer. The output is a valid protobuf\nserialization, such that every protobuf parser can parse it.\n\nNo maps are supported in version 1 due to the complexity of defining a\ndeterministic serialization. This might change in future. Implementations must\nreject documents containing maps as invalid input.\n\n### Background - Protobuf3 Encoding\n\nMost numeric types in protobuf3 are encoded as\n[varints](https://developers.google.com/protocol-buffers/docs/encoding#varints).\nVarints are at most 10 bytes, and since each varint byte has 7 bits of data,\nvarints are a representation of `uint70` (70-bit unsigned integer). When\nencoding, numeric values are casted from their base type to `uint70`, and when\ndecoding, the parsed `uint70` is casted to the appropriate numeric type.\n\nThe maximum valid value for a varint that complies with protobuf3 is\n`FF FF FF FF FF FF FF FF FF 7F` (i.e. `2**70 -1`). If the field type is\n`{,u,s}int64`, the highest 6 bits of the 70 are dropped during decoding,\nintroducing 6 bits of malleability. If the field type is `{,u,s}int32`, the\nhighest 38 bits of the 70 are dropped during decoding, introducing 38 bits of\nmalleability.\n\nAmong other sources of non-determinism, this ADR eliminates the possibility of\nencoding malleability.\n\n### Serialization rules\n\nThe serialization is based on the\n[protobuf3 encoding](https://developers.google.com/protocol-buffers/docs/encoding)\nwith the following additions:\n\n1. Fields must be serialized only once in ascending order\n2. Extra fields or any extra data must not be added\n3. [Default values](https://developers.google.com/protocol-buffers/docs/proto3#default)\n   must be omitted\n4. `repeated` fields of scalar numeric types must use\n   [packed encoding](https://developers.google.com/protocol-buffers/docs/encoding#packed)\n5. Varint encoding must not be longer than needed:\n    * No trailing zero bytes (in little endian, i.e. no leading zeroes in big\n      endian). Per rule 3 above, the default value of `0` must be omitted, so\n      this rule does not apply in such cases.\n    * The maximum value for a varint must be `FF FF FF FF FF FF FF FF FF 01`.\n      In other words, when decoded, the highest 6 bits of the 70-bit unsigned\n      integer must be `0`. (10-byte varints are 10 groups of 7 bits, i.e.\n      70 bits, of which only the lowest 70-6=64 are useful.)\n    * The maximum value for 32-bit values in varint encoding must be `FF FF FF FF 0F`\n      with one exception (below). In other words, when decoded, the highest 38\n      bits of the 70-bit unsigned integer must be `0`.\n        * The one exception to the above is _negative_ `int32`, which must be\n          encoded using the full 10 bytes for sign extension<sup>2</sup>.\n    * The maximum value for Boolean values in varint encoding must be `01` (i.e.\n      it must be `0` or `1`). Per rule 3 above, the default value of `0` must\n      be omitted, so if a Boolean is included it must have a value of `1`.\n\nWhile rules number 1. and 2. should be pretty straightforward and describe the\ndefault behavior of all protobuf encoders the author is aware of, the 3rd rule\nis more interesting. After a protobuf3 deserialization you cannot differentiate\nbetween unset fields and fields set to the default value<sup>3</sup>. At\nserialization level however, it is possible to set the fields with an empty\nvalue or omit them entirely. This is a significant difference to e.g. JSON\nwhere a property can be empty (`\"\"`, `0`), `null` or undefined, leading to 3\ndifferent documents.\n\nOmitting fields set to default values is valid because the parser must assign\nthe default value to fields missing in the serialization<sup>4</sup>. For scalar\ntypes, omitting defaults is required by the spec<sup>5</sup>. For `repeated`\nfields, not serializing them is the only way to express empty lists. Enums must\nhave a first element of numeric value 0, which is the default<sup>6</sup>. And\nmessage fields default to unset<sup>7</sup>.\n\nOmitting defaults allows for some amount of forward compatibility: users of\nnewer versions of a protobuf schema produce the same serialization as users of\nolder versions as long as newly added fields are not used (i.e. set to their\ndefault value).\n\n### Implementation\n\nThere are three main implementation strategies, ordered from the least to the\nmost custom development:\n\n* **Use a protobuf serializer that follows the above rules by default.** E.g.\n  [gogoproto](https://pkg.go.dev/github.com/cosmos/gogoproto/gogoproto) is known to\n  be compliant in most cases, but not when certain annotations such as\n  `nullable = false` are used. It might also be an option to configure an\n  existing serializer accordingly.\n* **Normalize default values before encoding them.** If your serializer follows\n  rules 1. and 2. and allows you to explicitly unset fields for serialization,\n  you can normalize default values to unset. This can be done when working with\n  [protobuf.js](https://www.npmjs.com/package/protobufjs):\n\n  ```js\n  const bytes = SignDoc.encode({\n    bodyBytes: body.length > 0 ? body : null, // normalize empty bytes to unset\n    authInfoBytes: authInfo.length > 0 ? authInfo : null, // normalize empty bytes to unset\n    chainId: chainId || null, // normalize \"\" to unset\n    accountNumber: accountNumber || null, // normalize 0 to unset\n    accountSequence: accountSequence || null, // normalize 0 to unset\n  }).finish();\n  ```\n\n* **Use a hand-written serializer for the types you need.** If none of the above\n  ways works for you, you can write a serializer yourself. For SignDoc this\n  would look something like this in Go, building on existing protobuf utilities:\n\n  ```go\n  if !signDoc.body_bytes.empty() {\n      buf.WriteUVarInt64(0xA) // wire type and field number for body_bytes\n      buf.WriteUVarInt64(signDoc.body_bytes.length())\n      buf.WriteBytes(signDoc.body_bytes)\n  }\n\n  if !signDoc.auth_info.empty() {\n      buf.WriteUVarInt64(0x12) // wire type and field number for auth_info\n      buf.WriteUVarInt64(signDoc.auth_info.length())\n      buf.WriteBytes(signDoc.auth_info)\n  }\n\n  if !signDoc.chain_id.empty() {\n      buf.WriteUVarInt64(0x1a) // wire type and field number for chain_id\n      buf.WriteUVarInt64(signDoc.chain_id.length())\n      buf.WriteBytes(signDoc.chain_id)\n  }\n\n  if signDoc.account_number != 0 {\n      buf.WriteUVarInt64(0x20) // wire type and field number for account_number\n      buf.WriteUVarInt(signDoc.account_number)\n  }\n\n  if signDoc.account_sequence != 0 {\n      buf.WriteUVarInt64(0x28) // wire type and field number for account_sequence\n      buf.WriteUVarInt(signDoc.account_sequence)\n  }\n  ```\n\n### Test vectors\n\nGiven the protobuf definition `Article.proto`\n\n```protobuf\npackage blog;\nsyntax = \"proto3\";\n\nenum Type {\n  UNSPECIFIED = 0;\n  IMAGES = 1;\n  NEWS = 2;\n};\n\nenum Review {\n  UNSPECIFIED = 0;\n  ACCEPTED = 1;\n  REJECTED = 2;\n};\n\nmessage Article {\n  string title = 1;\n  string description = 2;\n  uint64 created = 3;\n  uint64 updated = 4;\n  bool public = 5;\n  bool promoted = 6;\n  Type type = 7;\n  Review review = 8;\n  repeated string comments = 9;\n  repeated string backlinks = 10;\n};\n```\n\nserializing the values\n\n```yaml\ntitle: \"The world needs change 🌳\"\ndescription: \"\"\ncreated: 1596806111080\nupdated: 0\npublic: true\npromoted: false\ntype: Type.NEWS\nreview: Review.UNSPECIFIED\ncomments: [\"Nice one\", \"Thank you\"]\nbacklinks: []\n```\n\nmust result in the serialization\n\n```text\n0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75\n```\n\nWhen inspecting the serialized document, you see that every second field is\nomitted:\n\n```shell\n$ echo 0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75 | xxd -r -p | protoc --decode_raw\n1: \"The world needs change \\360\\237\\214\\263\"\n3: 1596806111080\n5: 1\n7: 2\n9: \"Nice one\"\n9: \"Thank you\"\n```\n\n## Consequences\n\nHaving such an encoding available allows us to get deterministic serialization\nfor all protobuf documents we need in the context of Cosmos SDK signing.\n\n### Positive\n\n* Well defined rules that can be verified independently of a reference\n  implementation\n* Simple enough to keep the barrier to implementing transaction signing low\n* It allows us to continue to use 0 and other empty values in SignDoc, avoiding\n  the need to work around 0 sequences. This does not imply the change from\n  https://github.com/cosmos/cosmos-sdk/pull/6949 should not be merged, but not\n  too important anymore.\n\n### Negative\n\n* When implementing transaction signing, the encoding rules above must be\n  understood and implemented.\n* The need for rule number 3. adds some complexity to implementations.\n* Some data structures may require custom code for serialization. Thus\n  the code is not very portable - it will require additional work for each\n  client implementing serialization to properly handle custom data structures.\n\n### Neutral\n\n### Usage in Cosmos SDK\n\nFor the reasons mentioned above (\"Negative\" section) we prefer to keep workarounds\nfor shared data structure. Example: the aforementioned `TxRaw` is using raw bytes\nas a workaround. This allows them to use any valid Protobuf library without\nthe need to implement a custom serializer that adheres to this standard (and related risks of bugs).\n\n## References\n\n* <sup>1</sup> _When a message is serialized, there is no guaranteed order for\n  how its known or unknown fields should be written. Serialization order is an\n  implementation detail and the details of any particular implementation may\n  change in the future. Therefore, protocol buffer parsers must be able to parse\n  fields in any order._ from\n  https://developers.google.com/protocol-buffers/docs/encoding#order\n* <sup>2</sup> https://developers.google.com/protocol-buffers/docs/encoding#signed_integers\n* <sup>3</sup> _Note that for scalar message fields, once a message is parsed\n  there's no way of telling whether a field was explicitly set to the default\n  value (for example whether a boolean was set to false) or just not set at all:\n  you should bear this in mind when defining your message types. For example,\n  don't have a boolean that switches on some behavior when set to false if you\n  don't want that behavior to also happen by default._ from\n  https://developers.google.com/protocol-buffers/docs/proto3#default\n* <sup>4</sup> _When a message is parsed, if the encoded message does not\n  contain a particular singular element, the corresponding field in the parsed\n  object is set to the default value for that field._ from\n  https://developers.google.com/protocol-buffers/docs/proto3#default\n* <sup>5</sup> _Also note that if a scalar message field is set to its default,\n  the value will not be serialized on the wire._ from\n  https://developers.google.com/protocol-buffers/docs/proto3#default\n* <sup>6</sup> _For enums, the default value is the first defined enum value,\n  which must be 0._ from\n  https://developers.google.com/protocol-buffers/docs/proto3#default\n* <sup>7</sup> _For message fields, the field is not set. Its exact value is\n  language-dependent._ from\n  https://developers.google.com/protocol-buffers/docs/proto3#default\n* Encoding rules and parts of the reasoning taken from\n  [canonical-proto3 Aaron Craelius](https://github.com/regen-network/canonical-proto3)"
  },
  {
    "number": 28,
    "filename": "adr-028-public-key-addresses.md",
    "title": "ADR 028: Public Key Addresses",
    "content": "# ADR 028: Public Key Addresses\n\n## Changelog\n\n* 2020/08/18: Initial version\n* 2021/01/15: Analysis and algorithm update\n\n## Status\n\nProposed\n\n## Abstract\n\nThis ADR defines an address format for all addressable Cosmos SDK accounts. That includes: new public key algorithms, multisig public keys, and module accounts.\n\n## Context\n\nIssue [\\#3685](https://github.com/cosmos/cosmos-sdk/issues/3685) identified that public key\naddress spaces are currently overlapping. We confirmed that it significantly decreases security of Cosmos SDK.\n\n### Problem\n\nAn attacker can control an input for an address generation function. This leads to a birthday attack, which significantly decreases the security space.\nTo overcome this, we need to separate the inputs for different kinds of account types:\na security break of one account type shouldn't impact the security of other account types.\n\n### Initial proposals\n\nOne initial proposal was to extend the address length and\nadding prefixes for different types of addresses.\n\n@ethanfrey explained an alternate approach originally used in https://github.com/iov-one/weave:\n\n> I spent quite a bit of time thinking about this issue while building weave... The other cosmos Sdk.\n> Basically I define a condition to be a type and format as human readable string with some binary data appended. This condition is hashed into an Address (again at 20 bytes). The use of this prefix makes it impossible to find a preimage for a given address with a different condition (eg ed25519 vs secp256k1).\n> This is explained in depth here https://weave.readthedocs.io/en/latest/design/permissions.html\n> And the code is here, look mainly at the top where we process conditions. https://github.com/iov-one/weave/blob/master/conditions.go\n\nAnd explained how this approach should be sufficiently collision resistant:\n\n> Yeah, AFAIK, 20 bytes should be collision resistance when the preimages are unique and not malleable. A space of 2^160 would expect some collision to be likely around 2^80 elements (birthday paradox). And if you want to find a collision for some existing element in the database, it is still 2^160. 2^80 only if all these elements are written to state.\n> The good example you brought up was eg. a public key bytes being a valid public key on two algorithms supported by the codec. Meaning if either was broken, you would break accounts even if they were secured with the safer variant. This is only as the issue when no differentiating type info is present in the preimage (before hashing into an address).\n> I would like to hear an argument if the 20 bytes space is an actual issue for security, as I would be happy to increase my address sizes in weave. I just figured cosmos and ethereum and bitcoin all use 20 bytes, it should be good enough. And the arguments above which made me feel it was secure. But I have not done a deeper analysis.\n\nThis led to the first proposal (which we proved to be not good enough):\nwe concatenate a key type with a public key, hash it and take the first 20 bytes of that hash, summarized as `sha256(keyTypePrefix || keybytes)[:20]`.\n\n### Review and Discussions\n\nIn [\\#5694](https://github.com/cosmos/cosmos-sdk/issues/5694) we discussed various solutions.\nWe agreed that 20 bytes it's not future proof, and extending the address length is the only way to allow addresses of different types, various signature types, etc.\nThis disqualifies the initial proposal.\n\nIn the issue we discussed various modifications:\n\n* Choice of the hash function.\n* Move the prefix out of the hash function: `keyTypePrefix + sha256(keybytes)[:20]` [post-hash-prefix-proposal].\n* Use double hashing: `sha256(keyTypePrefix + sha256(keybytes)[:20])`.\n* Increase to keybytes hash slice from 20 bytes to 32 or 40 bytes. We concluded that 32 bytes, produced by a good hash functions is future secure.\n\n### Requirements\n\n* Support currently used tools - we don't want to break an ecosystem, or add a long adaptation period. Ref: https://github.com/cosmos/cosmos-sdk/issues/8041\n* Try to keep the address length small - addresses are widely used in state, both as part of a key and object value.\n\n### Scope\n\nThis ADR only defines a process for the generation of address bytes. For end-user interactions with addresses (through the API, or CLI, etc.), we still use bech32 to format these addresses as strings. This ADR doesn't change that.\nUsing Bech32 for string encoding gives us support for checksum error codes and handling of user typos.\n\n## Decision\n\nWe define the following account types, for which we define the address function:\n\n1. simple accounts: represented by a regular public key (ie: secp256k1, sr25519)\n2. naive multisig: accounts composed by other addressable objects (ie: naive multisig)\n3. composed accounts with a native address key (ie: bls, group module accounts)\n4. module accounts: basically any accounts which cannot sign transactions and which are managed internally by modules\n\n### Legacy Public Key Addresses Don't Change\n\nCurrently (Jan 2021), the only officially supported Cosmos SDK user accounts are `secp256k1` basic accounts and legacy amino multisig.\nThey are used in existing Cosmos SDK zones. They use the following address formats:\n\n* secp256k1: `ripemd160(sha256(pk_bytes))[:20]`\n* legacy amino multisig: `sha256(aminoCdc.Marshal(pk))[:20]`\n\nWe don't want to change existing addresses. So the addresses for these two key types will remain the same.\n\nThe current multisig public keys use amino serialization to generate the address. We will retain\nthose public keys and their address formatting, and call them \"legacy amino\" multisig public keys\nin protobuf. We will also create multisig public keys without amino addresses to be described below.\n\n### Hash Function Choice\n\nAs in other parts of the Cosmos SDK, we will use `sha256`.\n\n### Basic Address\n\nWe start by defining a base algorithm for generating addresses which we will call `Hash`. Notably, it's used for accounts represented by a single key pair. For each public key schema we have to have an associated `typ` string, explained in the next section. `hash` is the cryptographic hash function defined in the previous section.\n\n```go\nconst A_LEN = 32\n\nfunc Hash(typ string, key []byte) []byte {\n    return hash(hash(typ) + key)[:A_LEN]\n}\n```\n\nThe `+` is bytes concatenation, which doesn't use any separator.\n\nThis algorithm is the outcome of a consultation session with a professional cryptographer.\nMotivation: this algorithm keeps the address relatively small (length of the `typ` doesn't impact the length of the final address)\nand it's more secure than [post-hash-prefix-proposal] (which uses the first 20 bytes of a pubkey hash, significantly reducing the address space).\nMoreover the cryptographer motivated the choice of adding `typ` in the hash to protect against a switch table attack.\n\n`address.Hash` is a low level function to generate _base_ addresses for new key types. Example:\n\n* BLS: `address.Hash(\"bls\", pubkey)`\n\n### Composed Addresses\n\nFor simple composed accounts (like a new naive multisig) we generalize the `address.Hash`. The address is constructed by recursively creating addresses for the sub accounts, sorting the addresses and composing them into a single address. It ensures that the ordering of keys doesn't impact the resulting address.\n\n```go\n// We don't need a PubKey interface - we need anything which is addressable.\ntype Addressable interface {\n    Address() []byte\n}\n\nfunc Composed(typ string, subaccounts []Addressable) []byte {\n    addresses = map(subaccounts, \\a -> LengthPrefix(a.Address()))\n    addresses = sort(addresses)\n    return address.Hash(typ, addresses[0] + ... + addresses[n])\n}\n```\n\nThe `typ` parameter should be a schema descriptor, containing all significant attributes with deterministic serialization (eg: utf8 string).\n`LengthPrefix` is a function which prepends 1 byte to the address. The value of that byte is the length of the address bits before prepending. The address must be at most 255 bits long.\nWe are using `LengthPrefix` to eliminate conflicts - it assures, that for 2 lists of addresses: `as = {a1, a2, ..., an}` and `bs = {b1, b2, ..., bm}` such that every `bi` and `ai` is at most 255 long, `concatenate(map(as, (a) => LengthPrefix(a))) = map(bs, (b) => LengthPrefix(b))` if `as = bs`.\n\nImplementation Tip: account implementations should cache addresses.\n\n#### Multisig Addresses\n\nFor a new multisig public keys, we define the `typ` parameter not based on any encoding scheme (amino or protobuf). This avoids issues with non-determinism in the encoding scheme.\n\nExample:\n\n```protobuf\npackage cosmos.crypto.multisig;\n\nmessage PubKey {\n  uint32 threshold = 1;\n  repeated google.protobuf.Any pubkeys = 2;\n}\n```\n\n```go\nfunc (multisig PubKey) Address() {\n\t// first gather all nested pub keys\n\tvar keys []address.Addressable  // cryptotypes.PubKey implements Addressable\n\tfor _, _key := range multisig.Pubkeys {\n\t\tkeys = append(keys, key.GetCachedValue().(cryptotypes.PubKey))\n\t}\n\n\t// form the type from the message name (cosmos.crypto.multisig.PubKey) and the threshold joined together\n\tprefix := fmt.Sprintf(\"%s/%d\", proto.MessageName(multisig), multisig.Threshold)\n\n\t// use the Composed function defined above\n\treturn address.Composed(prefix, keys)\n}\n```\n\n\n### Derived Addresses\n\nWe must be able to cryptographically derive one address from another one. The derivation process must guarantee hash properties, hence we use the already defined `Hash` function:\n\n```go\nfunc Derive(address, derivationKey []byte) []byte {\n\treturn Hash(address, derivationKey)\n}\n```\n\n### Module Account Addresses\n\nA module account will have `\"module\"` type. Module accounts can have sub accounts. The submodule account will be created based on module name, and sequence of derivation keys. Typically, the first derivation key should be a class of the derived accounts. The derivation process has a defined order: module name, submodule key, subsubmodule key... An example module account is created using:\n\n```go\naddress.Module(moduleName, key)\n```\n\nAn example sub-module account is created using:\n\n```go\ngroupPolicyAddresses := []byte{1}\naddress.Module(moduleName, groupPolicyAddresses, policyID)\n```\n\nThe `address.Module` function is using `address.Hash` with `\"module\"` as the type argument, and byte representation of the module name concatenated with submodule key. The last two components must be uniquely separated to avoid potential clashes (example: modulename=\"ab\" & submodulekey=\"bc\" will have the same derivation key as modulename=\"a\" & submodulekey=\"bbc\").\nWe use a null byte (`'\\x00'`) to separate module name from the submodule key. This works, because null byte is not a part of a valid module name. Finally, the sub-submodule accounts are created by applying the `Derive` function recursively.\nWe could use `Derive` function also in the first step (rather than concatenating the module name with a zero byte and the submodule key). We decided to do concatenation to avoid one level of derivation and speed up computation.\n\nFor backward compatibility with the existing `authtypes.NewModuleAddress`, we add a special case in `Module` function: when no derivation key is provided, we fallback to the \"legacy\" implementation. \n\n```go\nfunc Module(moduleName string, derivationKeys ...[]byte) []byte{\n\tif len(derivationKeys) == 0 {\n\t\treturn authtypes.NewModuleAddress(moduleName)  // legacy case\n\t}\n\tsubmoduleAddress := Hash(\"module\", []byte(moduleName) + 0 + key)\n\treturn fold((a, k) => Derive(a, k), subsubKeys, submoduleAddress)\n}\n```\n\n**Example 1**  A lending BTC pool address would be:\n\n```go\nbtcPool := address.Module(\"lending\", btc.Address()})\n```\n\nIf we want to create an address for a module account depending on more than one key, we can concatenate them:\n\n```go\nbtcAtomAMM := address.Module(\"amm\", btc.Address() + atom.Address()})\n```\n\n**Example 2**  a smart-contract address could be constructed by:\n\n```go\nsmartContractAddr = Module(\"mySmartContractVM\", smartContractsNamespace, smartContractKey})\n\n// which equals to:\nsmartContractAddr = Derived(\n    Module(\"mySmartContractVM\", smartContractsNamespace), \n    []{smartContractKey})\n```\n\n### Schema Types\n\nA `typ` parameter used in `Hash` function SHOULD be unique for each account type.\nSince all Cosmos SDK account types are serialized in the state, we propose to use the protobuf message name string.\n\nExample: all public key types have a unique protobuf message type similar to:\n\n```protobuf\npackage cosmos.crypto.sr25519;\n\nmessage PubKey {\n\tbytes key = 1;\n}\n```\n\nAll protobuf messages have unique fully qualified names, in this example `cosmos.crypto.sr25519.PubKey`.\nThese names are derived directly from .proto files in a standardized way and used\nin other places such as the type URL in `Any`s. We can easily obtain the name using\n`proto.MessageName(msg)`.\n\n## Consequences\n\n### Backwards Compatibility\n\nThis ADR is compatible with what was committed and directly supported in the Cosmos SDK repository.\n\n### Positive\n\n* a simple algorithm for generating addresses for new public keys, complex accounts and modules\n* the algorithm generalizes _native composed keys_\n* increased security and collision resistance of addresses\n* the approach is extensible for future use-cases - one can use other address types, as long as they don't conflict with the address length specified here (20 or 32 bytes).\n* support new account types.\n\n### Negative\n\n* addresses do not communicate key type, a prefixed approach would have done this\n* addresses are 60% longer and will consume more storage space\n* requires a refactor of KVStore store keys to handle variable length addresses\n\n### Neutral\n\n* protobuf message names are used as key type prefixes\n\n## Further Discussions\n\nSome accounts can have a fixed name or may be constructed in another way (eg: modules). We were discussing an idea of an account with a predefined name (eg: `me.regen`), which could be used by institutions.\nWithout going into details, these kinds of addresses are compatible with the hash based addresses described here as long as they don't have the same length.\nMore specifically, any special account address must not have a length equal to 20 or 32 bytes.\n\n## Appendix: Consulting session\n\nEnd of Dec 2020 we had a session with [Alan Szepieniec](https://scholar.google.be/citations?user=4LyZn8oAAAAJ&hl=en) to consult the approach presented above.\n\nAlan general observations:\n\n* we don’t need 2-preimage resistance\n* we need 32bytes address space for collision resistance\n* when an attacker can control an input for an object with an address then we have a problem with a birthday attack\n* there is an issue with smart-contracts for hashing\n* sha2 mining can be used to break the address pre-image\n\nHashing algorithm\n\n* any attack breaking blake3 will break blake2\n* Alan is pretty confident about the current security analysis of the blake hash algorithm. It was a finalist, and the author is well known in security analysis.\n\nAlgorithm:\n\n* Alan recommends to hash the prefix: `address(pub_key) = hash(hash(key_type) + pub_key)[:32]`, main benefits:\n    * we are free to user arbitrary long prefix names\n    * we still don’t risk collisions\n    * switch tables\n* discussion about penalization -> about adding prefix post hash\n* Aaron asked about post hash prefixes (`address(pub_key) = key_type + hash(pub_key)`) and differences. Alan noted that this approach has longer address space and it’s stronger.\n\nAlgorithm for complex / composed keys:\n\n* merging tree-like addresses with same algorithm are fine\n\nModule addresses: Should module addresses have a different size to differentiate it?\n\n* we will need to set a pre-image prefix for module addresses to keep them in 32-byte space: `hash(hash('module') + module_key)`\n* Aaron observation: we already need to deal with variable length (to not break secp256k1 keys).\n\nDiscussion about an arithmetic hash function for ZKP\n\n* Poseidon / Rescue\n* Problem: much bigger risk because we don’t know much techniques and the history of crypto-analysis of arithmetic constructions. It’s still a new ground and area of active research.\n\nPost quantum signature size\n\n* Alan suggestion: Falcon: speed / size ratio - very good.\n* Aaron - should we think about it?\n  Alan: based on early extrapolation this thing will get able to break EC cryptography in 2050. But that’s a lot of uncertainty. But there is magic happening with recursions / linking / simulation and that can speedup the progress.\n\nOther ideas\n\n* Let’s say we use the same key and two different address algorithms for 2 different use cases. Is it still safe to use it? Alan: if we want to hide the public key (which is not our use case), then it’s less secure but there are fixes.\n\n### References\n\n* [Notes](https://hackmd.io/_NGWI4xZSbKzj1BkCqyZMw)"
  },
  {
    "number": 29,
    "filename": "adr-029-fee-grant-module.md",
    "title": "ADR 029: Fee Grant Module",
    "content": "# ADR 029: Fee Grant Module\n\n## Changelog\n\n* 2020/08/18: Initial Draft\n* 2021/05/05: Removed height based expiration support and simplified naming.\n\n## Status\n\nAccepted\n\n## Context\n\nIn order to make blockchain transactions, the signing account must possess a sufficient balance of the right denomination\nin order to pay fees. There are classes of transactions where needing to maintain a wallet with sufficient fees is a\nbarrier to adoption.\n\nFor instance, when proper permissions are set up, someone may temporarily delegate the ability to vote on proposals to\na \"burner\" account that is stored on a mobile phone with only minimal security.\n\nOther use cases include workers tracking items in a supply chain or farmers submitting field data for analytics\nor compliance purposes.\n\nFor all of these use cases, UX would be significantly enhanced by obviating the need for these accounts to always\nmaintain the appropriate fee balance. This is especially true if we want to achieve enterprise adoption for something\nlike supply chain tracking.\n\nWhile one solution would be to have a service that fills up these accounts automatically with the appropriate fees, a better UX\nwould be provided by allowing these accounts to pull from a common fee pool account with proper spending limits.\nA single pool would reduce the churn of making lots of small \"fill up\" transactions and also more effectively leverage\nthe resources of the organization setting up the pool.\n\n## Decision\n\nAs a solution we propose a module, `x/feegrant` which allows one account, the \"granter\" to grant another account, the \"grantee\"\nan allowance to spend the granter's account balance for fees within certain well-defined limits.\n\nFee allowances are defined by the extensible `FeeAllowanceI` interface:\n\n```go\ntype FeeAllowanceI {\n  // Accept can use fee payment requested as well as timestamp of the current block\n  // to determine whether or not to process this. This is checked in\n  // Keeper.UseGrantedFees and the return values should match how it is handled there.\n  //\n  // If it returns an error, the fee payment is rejected, otherwise it is accepted.\n  // The FeeAllowance implementation is expected to update it's internal state\n  // and will be saved again after an acceptance.\n  //\n  // If remove is true (regardless of the error), the FeeAllowance will be deleted from storage\n  // (eg. when it is used up). (See call to RevokeFeeAllowance in Keeper.UseGrantedFees)\n  Accept(ctx sdk.Context, fee sdk.Coins, msgs []sdk.Msg) (remove bool, err error)\n\n  // ValidateBasic should evaluate this FeeAllowance for internal consistency.\n  // Don't allow negative amounts, or negative periods for example.\n  ValidateBasic() error\n}\n```\n\nTwo basic fee allowance types, `BasicAllowance` and `PeriodicAllowance` are defined to support known use cases:\n\n```protobuf\n// BasicAllowance implements FeeAllowanceI with a one-time grant of tokens\n// that optionally expires. The delegatee can use up to SpendLimit to cover fees.\nmessage BasicAllowance {\n  // spend_limit specifies the maximum amount of tokens that can be spent\n  // by this allowance and will be updated as tokens are spent. If it is\n  // empty, there is no spend limit and any amount of coins can be spent.\n  repeated cosmos_sdk.v1.Coin spend_limit = 1;\n\n  // expiration specifies an optional time when this allowance expires\n  google.protobuf.Timestamp expiration = 2;\n}\n\n// PeriodicAllowance extends FeeAllowanceI to allow for both a maximum cap,\n// as well as a limit per time period.\nmessage PeriodicAllowance {\n  BasicAllowance basic = 1;\n\n  // period specifies the time duration in which period_spend_limit coins can\n  // be spent before that allowance is reset\n  google.protobuf.Duration period = 2;\n\n  // period_spend_limit specifies the maximum number of coins that can be spent\n  // in the period\n  repeated cosmos_sdk.v1.Coin period_spend_limit = 3;\n\n  // period_can_spend is the number of coins left to be spent before the period_reset time\n  repeated cosmos_sdk.v1.Coin period_can_spend = 4;\n\n  // period_reset is the time at which this period resets and a new one begins,\n  // it is calculated from the start time of the first transaction after the\n  // last period ended\n  google.protobuf.Timestamp period_reset = 5;\n}\n\n```\n\nAllowances can be granted and revoked using `MsgGrantAllowance` and `MsgRevokeAllowance`:\n\n```protobuf\n// MsgGrantAllowance adds permission for Grantee to spend up to Allowance\n// of fees from the account of Granter.\nmessage MsgGrantAllowance {\n     string granter = 1;\n     string grantee = 2;\n     google.protobuf.Any allowance = 3;\n }\n\n // MsgRevokeAllowance removes any existing FeeAllowance from Granter to Grantee.\n message MsgRevokeAllowance {\n     string granter = 1;\n     string grantee = 2;\n }\n```\n\nIn order to use allowances in transactions, we add a new field `granter` to the transaction `Fee` type:\n\n```protobuf\npackage cosmos.tx.v1beta1;\n\nmessage Fee {\n  repeated cosmos.base.v1beta1.Coin amount = 1;\n  uint64 gas_limit = 2;\n  string payer = 3;\n  string granter = 4;\n}\n```\n\n`granter` must either be left empty or must correspond to an account which has granted\na fee allowance to the fee payer (either the first signer or the value of the `payer` field).\n\nA new `AnteDecorator` named `DeductGrantedFeeDecorator` will be created in order to process transactions with `fee_payer`\nset and correctly deduct fees based on fee allowances.\n\n## Consequences\n\n### Positive\n\n* improved UX for use cases where it is cumbersome to maintain an account balance just for fees\n\n### Negative\n\n### Neutral\n\n* a new field must be added to the transaction `Fee` message and a new `AnteDecorator` must be\ncreated to use it\n\n## References\n\n* Blog article describing initial work: https://medium.com/regen-network/hacking-the-cosmos-cosmwasm-and-key-management-a08b9f561d1b\n* Initial public specification: https://gist.github.com/aaronc/b60628017352df5983791cad30babe56\n* Original subkeys proposal from B-harvest which influenced this design: https://github.com/cosmos/cosmos-sdk/issues/4480"
  },
  {
    "number": 30,
    "filename": "adr-030-authz-module.md",
    "title": "ADR 030: Authorization Module",
    "content": "# ADR 030: Authorization Module\n\n## Changelog\n\n* 2019-11-06: Initial Draft\n* 2020-10-12: Updated Draft\n* 2020-11-13: Accepted\n* 2020-05-06: proto API updates, use `sdk.Msg` instead of `sdk.ServiceMsg` (the latter concept was removed from Cosmos SDK)\n* 2022-04-20: Updated the `SendAuthorization` proto docs to clarify the `SpendLimit` is a required field. (Generic authorization can be used with bank msg type url to create limit less bank authorization)\n\n## Status\n\nAccepted\n\n## Abstract\n\nThis ADR defines the `x/authz` module which allows accounts to grant authorizations to perform actions\non behalf of that account to other accounts.\n\n## Context\n\nThe concrete use cases which motivated this module include:\n\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\ndelegated stake\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https://github.com/cosmos/cosmos-sdk/issues/4480) which\nis a term used to describe the functionality provided by this module together with\nthe `fee_grant` module from [ADR 029](./adr-029-fee-grant-module.md) and the [group module](https://github.com/cosmos/cosmos-sdk/tree/main/x/group).\n\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\nkeys.\n\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https://github.com/cosmos-gaians/cosmos-sdk/tree/hackatom/x/delegation).\n\n## Decision\n\nWe will create a module named `authz` which provides functionality for\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\nmust be granted for a particular `Msg` service methods one by one using an implementation\nof `Authorization` interface.\n\n### Types\n\nAuthorizations determine exactly what privileges are granted. They are extensible\nand can be defined for any `Msg` service method even outside of the module where\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\n\n#### Authorization\n\n```go\ntype Authorization interface {\n\tproto.Message\n\n\t// MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\n\t// which will process and accept or reject a request.\n\tMsgTypeURL() string\n\n\t// Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\n\t// so provides an upgraded authorization instance.\n\tAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\n\n\t// ValidateBasic does a simple validation check that\n\t// doesn't require access to any other information.\n\tValidateBasic() error\n}\n\n// AcceptResponse instruments the controller of an authz message if the request is accepted\n// and if it should be updated or deleted.\ntype AcceptResponse struct {\n\t// If Accept=true, the controller can accept and authorization and handle the update.\n\tAccept bool\n\t// If Delete=true, the controller must delete the authorization object and release\n\t// storage resources.\n\tDelete bool\n\t// Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\n\t// it must use the updated version and handle the update on the storage level.\n\tUpdated Authorization\n}\n```\n\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\na `SpendLimit` and updates it down to zero:\n\n```go\ntype SendAuthorization struct {\n\t// SpendLimit specifies the maximum amount of tokens that can be spent\n\t// by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization \n\t// can be used with bank msg type url to create limit less bank authorization).\n\tSpendLimit sdk.Coins\n}\n\nfunc (a SendAuthorization) MsgTypeURL() string {\n\treturn sdk.MsgTypeURL(&MsgSend{})\n}\n\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\n\tmSend, ok := msg.(*MsgSend)\n\tif !ok {\n\t\treturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\n\t}\n\tlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\n\tif isNegative {\n\t\treturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\n\t}\n\tif limitLeft.IsZero() {\n\t\treturn authz.AcceptResponse{Accept: true, Delete: true}, nil\n\t}\n\n\treturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\n}\n```\n\nA different type of capability for `MsgSend` could be implemented\nusing the `Authorization` interface with no need to change the underlying\n`bank` module.\n\n##### Small notes on `AcceptResponse`\n\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\n\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\nthe field will be `nil`.\n\n### `Msg` Service\n\n```protobuf\nservice Msg {\n  // Grant grants the provided authorization to the grantee on the granter's\n  // account with the provided expiration time.\n  rpc Grant(MsgGrant) returns (MsgGrantResponse);\n\n  // Exec attempts to execute the provided messages using\n  // authorizations granted to the grantee. Each message should have only\n  // one signer corresponding to the granter of the authorization.\n  rpc Exec(MsgExec) returns (MsgExecResponse);\n\n  // Revoke revokes any authorization corresponding to the provided method name on the\n  // granter's account that has been granted to the grantee.\n  rpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\n}\n\n// Grant gives permissions to execute\n// the provided method with expiration time.\nmessage Grant {\n  google.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\n  google.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\n}\n\nmessage MsgGrant {\n  string granter = 1;\n  string grantee = 2;\n\n  Grant grant = 3 [(gogoproto.nullable) = false];\n}\n\nmessage MsgExecResponse {\n  cosmos.base.abci.v1beta1.Result result = 1;\n}\n\nmessage MsgExec {\n  string   grantee                  = 1;\n  // Authorization Msg requests to execute. Each msg must implement Authorization interface\n  repeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];\n}\n```\n\n### Router Middleware\n\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\nto the router based on `Authorization` grants:\n\n```go\ntype Keeper interface {\n\t// DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\n\t// to send those messages by the first (and only) signer of each msg.\n    DispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\n}\n```\n\n### CLI\n\n#### `tx exec` Method\n\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\nwould send a transaction like this:\n\n```go\nMsgExec {\n  Grantee: mykey,\n  Msgs: []sdk.Msg{\n    MsgVote {\n      ProposalID: 1,\n      Voter: cosmos3thsdgh983egh823\n      Option: Yes\n    }\n  }\n}\n```\n\n#### `tx grant <grantee> <authorization> --from <granter>`\n\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\nJSON on the CLI.\n\n#### `tx revoke <grantee> <method-name> --from <granter>`\n\nThis CLI command will send a `MsgRevoke` transaction.\n\n### Built-in Authorizations\n\n#### `SendAuthorization`\n\n```protobuf\n// SendAuthorization allows the grantee to spend up to spend_limit coins from\n// the granter's account.\nmessage SendAuthorization {\n  repeated cosmos.base.v1beta1.Coin spend_limit = 1;\n}\n```\n\n#### `GenericAuthorization`\n\n```protobuf\n// GenericAuthorization gives the grantee unrestricted permissions to execute\n// the provided method on behalf of the granter's account.\nmessage GenericAuthorization {\n  option (cosmos_proto.implements_interface) = \"Authorization\";\n\n  // Msg, identified by it's type URL, to grant unrestricted permissions to execute\n  string msg = 1;\n}\n```\n\n## Consequences\n\n### Positive\n\n* Users will be able to authorize arbitrary actions on behalf of their accounts to other\nusers, improving key management for many use cases\n* The solution is more generic than previously considered approaches and the\n`Authorization` interface approach can be extended to cover other use cases by\nSDK users\n\n### Negative\n\n### Neutral\n\n## References\n\n* Initial Hackatom implementation: https://github.com/cosmos-gaians/cosmos-sdk/tree/hackatom/x/delegation\n* Post-Hackatom spec: https://gist.github.com/aaronc/b60628017352df5983791cad30babe56#delegation-module\n* B-Harvest subkeys spec: https://github.com/cosmos/cosmos-sdk/issues/4480"
  },
  {
    "number": 31,
    "filename": "adr-031-msg-service.md",
    "title": "ADR 031: Protobuf Msg Services",
    "content": "# ADR 031: Protobuf Msg Services\n\n## Changelog\n\n* 2020-10-05: Initial Draft\n* 2021-04-21: Remove `ServiceMsg`s to follow Protobuf `Any`'s spec, see [#9063](https://github.com/cosmos/cosmos-sdk/issues/9063).\n\n## Status\n\nAccepted\n\n## Abstract\n\nWe want to leverage protobuf `service` definitions for defining `Msg`s, which will give us significant developer UX\nimprovements in terms of the code that is generated and the fact that return types will now be well defined.\n\n## Context\n\nCurrently `Msg` handlers in the Cosmos SDK have return values that are placed in the `data` field of the response.\nThese return values, however, are not specified anywhere except in the golang handler code.\n\nIn early conversations [it was proposed](https://docs.google.com/document/d/1eEgYgvgZqLE45vETjhwIw4VOqK-5hwQtZtjVbiXnIGc/edit)\nthat `Msg` return types be captured using a protobuf extension field, ex:\n\n```protobuf\npackage cosmos.gov;\n\nmessage MsgSubmitProposal\n\toption (cosmos_proto.msg_return) = “uint64”;\n\tstring delegator_address = 1;\n\tstring validator_address = 2;\n\trepeated sdk.Coin amount = 3;\n}\n```\n\nThis was never adopted, however.\n\nHaving a well-specified return value for `Msg`s would improve client UX. For instance,\nin `x/gov`,  `MsgSubmitProposal` returns the proposal ID as a big-endian `uint64`.\nThis isn’t really documented anywhere and clients would need to know the internals\nof the Cosmos SDK to parse that value and return it to users.\n\nAlso, there may be cases where we want to use these return values programmatically.\nFor instance, https://github.com/cosmos/cosmos-sdk/issues/7093 proposes a method for\ndoing inter-module Ocaps using the `Msg` router. A well-defined return type would\nimprove the developer UX for this approach.\n\nIn addition, handler registration of `Msg` types tends to add a bit of\nboilerplate on top of keepers and is usually done through manual type switches.\nThis isn't necessarily bad, but it does add overhead to creating modules.\n\n## Decision\n\nWe decide to use protobuf `service` definitions for defining `Msg`s as well as\nthe code generated by them as a replacement for `Msg` handlers.\n\nBelow we define how this will look for the `SubmitProposal` message from `x/gov` module.\nWe start with a `Msg` `service` definition:\n\n```protobuf\npackage cosmos.gov;\n\nservice Msg {\n  rpc SubmitProposal(MsgSubmitProposal) returns (MsgSubmitProposalResponse);\n}\n\n// Note that for backwards compatibility this uses MsgSubmitProposal as the request\n// type instead of the more canonical MsgSubmitProposalRequest\nmessage MsgSubmitProposal {\n  google.protobuf.Any content = 1;\n  string proposer = 2;\n}\n\nmessage MsgSubmitProposalResponse {\n  uint64 proposal_id;\n}\n```\n\nWhile this is most commonly used for gRPC, overloading protobuf `service` definitions like this does not violate\nthe intent of the [protobuf spec](https://developers.google.com/protocol-buffers/docs/proto3#services) which says:\n> If you don’t want to use gRPC, it’s also possible to use protocol buffers with your own RPC implementation.\nWith this approach, we would get an auto-generated `MsgServer` interface:\n\nIn addition to clearly specifying return types, this has the benefit of generating client and server code. On the server\nside, this is almost like an automatically generated keeper method and could maybe be used instead of keepers eventually\n(see [\\#7093](https://github.com/cosmos/cosmos-sdk/issues/7093)):\n\n```go\npackage gov\n\ntype MsgServer interface {\n  SubmitProposal(context.Context, *MsgSubmitProposal) (*MsgSubmitProposalResponse, error)\n}\n```\n\nOn the client side, developers could take advantage of this by creating RPC implementations that encapsulate transaction\nlogic. Protobuf libraries that use asynchronous callbacks, like [protobuf.js](https://github.com/protobufjs/protobuf.js#using-services)\ncould use this to register callbacks for specific messages even for transactions that include multiple `Msg`s.\n\nEach `Msg` service method should have exactly one request parameter: its corresponding `Msg` type. For example, the `Msg` service method `/cosmos.gov.v1beta1.Msg/SubmitProposal` above has exactly one request parameter, namely the `Msg` type `/cosmos.gov.v1beta1.MsgSubmitProposal`. It is important the reader understands clearly the nomenclature difference between a `Msg` service (a Protobuf service) and a `Msg` type (a Protobuf message), and the differences in their fully-qualified name.\n\nThis convention has been decided over the more canonical `Msg...Request` names mainly for backwards compatibility, but also for better readability in `TxBody.messages` (see [Encoding section](#encoding) below): transactions containing `/cosmos.gov.MsgSubmitProposal` read better than those containing `/cosmos.gov.v1beta1.MsgSubmitProposalRequest`.\n\nOne consequence of this convention is that each `Msg` type can be the request parameter of only one `Msg` service method. However, we consider this limitation a good practice in explicitness.\n\n### Encoding\n\nEncoding of transactions generated with `Msg` services does not differ from current Protobuf transaction encoding as defined in [ADR-020](./adr-020-protobuf-transaction-encoding.md). We are encoding `Msg` types (which are exactly `Msg` service methods' request parameters) as `Any` in `Tx`s which involves packing the\nbinary-encoded `Msg` with its type URL.\n\n### Decoding\n\nSince `Msg` types are packed into `Any`, decoding transaction messages is done by unpacking `Any`s into `Msg` types. For more information, please refer to [ADR-020](./adr-020-protobuf-transaction-encoding.md#transactions).\n\n### Routing\n\nWe propose to add a `msg_service_router` in BaseApp. This router is a key/value map which maps `Msg` types' `type_url`s to their corresponding `Msg` service method handler. Since there is a 1-to-1 mapping between `Msg` types and `Msg` service method, the `msg_service_router` has exactly one entry per `Msg` service method.\n\nWhen a transaction is processed by BaseApp (in CheckTx or in DeliverTx), its `TxBody.messages` are decoded as `Msg`s. Each `Msg`'s `type_url` is matched against an entry in the `msg_service_router`, and the respective `Msg` service method handler is called.\n\nFor backward compatibility, the old handlers are not removed yet. If BaseApp receives a legacy `Msg` with no corresponding entry in the `msg_service_router`, it will be routed via its legacy `Route()` method into the legacy handler.\n\n### Module Configuration\n\nIn [ADR 021](./adr-021-protobuf-query-encoding.md), we introduced a method `RegisterQueryService`\nto `AppModule` which allows for modules to register gRPC queriers.\n\nTo register `Msg` services, we attempt a more extensible approach by converting `RegisterQueryService`\nto a more generic `RegisterServices` method:\n\n```go\ntype AppModule interface {\n  RegisterServices(Configurator)\n  ...\n}\n\ntype Configurator interface {\n  QueryServer() grpc.Server\n  MsgServer() grpc.Server\n}\n\n// example module:\nfunc (am AppModule) RegisterServices(cfg Configurator) {\n\ttypes.RegisterQueryServer(cfg.QueryServer(), keeper)\n\ttypes.RegisterMsgServer(cfg.MsgServer(), keeper)\n}\n```\n\nThe `RegisterServices` method and the `Configurator` interface are intended to\nevolve to satisfy the use cases discussed in [\\#7093](https://github.com/cosmos/cosmos-sdk/issues/7093)\nand [\\#7122](https://github.com/cosmos/cosmos-sdk/issues/7421).\n\nWhen `Msg` services are registered, the framework _should_ verify that all `Msg` types\nimplement the `sdk.Msg` interface and throw an error during initialization rather\nthan later when transactions are processed.\n\n### `Msg` Service Implementation\n\nJust like query services, `Msg` service methods can retrieve the `sdk.Context`\nfrom the `context.Context` parameter using the `sdk.UnwrapSDKContext`\nmethod:\n\n```go\npackage gov\n\nfunc (k Keeper) SubmitProposal(goCtx context.Context, params *types.MsgSubmitProposal) (*MsgSubmitProposalResponse, error) {\n\tctx := sdk.UnwrapSDKContext(goCtx)\n    ...\n}\n```\n\nThe `sdk.Context` should have an `EventManager` already attached by BaseApp's `msg_service_router`.\n\nSeparate handler definition is no longer needed with this approach.\n\n## Consequences\n\nThis design changes how a module functionality is exposed and accessed. It deprecates the existing `Handler` interface and `AppModule.Route` in favor of [Protocol Buffer Services](https://developers.google.com/protocol-buffers/docs/proto3#services) and Service Routing described above. This dramatically simplifies the code. We don't need to create handlers and keepers any more. Use of Protocol Buffer auto-generated clients clearly separates the communication interfaces between the module and a modules user. The control logic (aka handlers and keepers) is not exposed any more. A module interface can be seen as a black box accessible through a client API. It's worth to note that the client interfaces are also generated by Protocol Buffers.\n\nThis also allows us to change how we perform functional tests. Instead of mocking AppModules and Router, we will mock a client (server will stay hidden). More specifically: we will never mock `moduleA.MsgServer` in `moduleB`, but rather `moduleA.MsgClient`. One can think about it as working with external services (eg DBs, or online servers...). We assume that the transmission between clients and servers is correctly handled by generated Protocol Buffers.\n\nFinally, closing a module to client API opens desirable OCAP patterns discussed in ADR-033. Since server implementation and interface is hidden, nobody can hold \"keepers\"/servers and will be forced to relay on the client interface, which will drive developers for correct encapsulation and software engineering patterns.\n\n### Pros\n\n* communicates return type clearly\n* manual handler registration and return type marshaling is no longer needed, just implement the interface and register it\n* communication interface is automatically generated, the developer can now focus only on the state transition methods - this would improve the UX of [\\#7093](https://github.com/cosmos/cosmos-sdk/issues/7093) approach (1) if we chose to adopt that\n* generated client code could be useful for clients and tests\n* dramatically reduces and simplifies the code\n\n### Cons\n\n* using `service` definitions outside the context of gRPC could be confusing (but doesn’t violate the proto3 spec)\n\n## References\n\n* [Initial Github Issue \\#7122](https://github.com/cosmos/cosmos-sdk/issues/7122)\n* [proto 3 Language Guide: Defining Services](https://developers.google.com/protocol-buffers/docs/proto3#services)\n* [Initial pre-`Any` `Msg` designs](https://docs.google.com/document/d/1eEgYgvgZqLE45vETjhwIw4VOqK-5hwQtZtjVbiXnIGc)\n* [ADR 020](./adr-020-protobuf-transaction-encoding.md)\n* [ADR 021](./adr-021-protobuf-query-encoding.md)"
  },
  {
    "number": 32,
    "filename": "adr-032-typed-events.md",
    "title": "ADR 032: Typed Events",
    "content": "# ADR 032: Typed Events\n\n## Changelog\n\n* 28-Sept-2020: Initial Draft\n\n## Authors\n\n* Anil Kumar (@anilcse)\n* Jack Zampolin (@jackzampolin)\n* Adam Bozanich (@boz)\n\n## Status\n\nProposed\n\n## Abstract\n\nCurrently in the Cosmos SDK, events are defined in the handlers for each message as well as `BeginBlock` and `EndBlock`. Each module doesn't have types defined for each event, they are implemented as `map[string]string`. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\n\n## Context\n\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\n\n[Our platform](http://github.com/ovrclk/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https://github.com/ovrclk/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\n\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\n\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\n\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\n\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\n\n## Decision\n\n**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\n\n```go\n// types/events.go\n\n// EmitTypedEvent takes typed event and emits converting it into sdk.Event\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\n\tevtType := proto.MessageName(event)\n\tevtJSON, err := codec.ProtoMarshalJSON(event)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar attrMap map[string]json.RawMessage\n\terr = json.Unmarshal(evtJSON, &attrMap)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar attrs []abci.EventAttribute\n\tfor k, v := range attrMap {\n\t\tattrs = append(attrs, abci.EventAttribute{\n\t\t\tKey:   []byte(k),\n\t\t\tValue: v,\n\t\t})\n\t}\n\n\tem.EmitEvent(Event{\n\t\tType:       evtType,\n\t\tAttributes: attrs,\n\t})\n\n\treturn nil\n}\n\n// ParseTypedEvent converts abci.Event back to typed event\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\n\tconcreteGoType := proto.MessageType(event.Type)\n\tif concreteGoType == nil {\n\t\treturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\n\t}\n\n\tvar value reflect.Value\n\tif concreteGoType.Kind() == reflect.Ptr {\n\t\tvalue = reflect.New(concreteGoType.Elem())\n\t} else {\n\t\tvalue = reflect.Zero(concreteGoType)\n    }\n\n\tprotoMsg, ok := value.Interface().(proto.Message)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\n\t}\n\n\tattrMap := make(map[string]json.RawMessage)\n\tfor _, attr := range event.Attributes {\n\t\tattrMap[string(attr.Key)] = attr.Value\n\t}\n\n\tattrBytes, err := json.Marshal(attrMap)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn protoMsg, nil\n}\n```\n\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\n\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\n\n**Step-2**: Add proto definitions for typed events for msgs in each module:\n\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\n\n```protobuf\n// proto/cosmos/gov/v1beta1/gov.proto\n// Add typed event definition\n\npackage cosmos.gov.v1beta1;\n\nmessage EventSubmitProposal {\n    string from_address   = 1;\n    uint64 proposal_id    = 2;\n    TextProposal proposal = 3;\n}\n```\n\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\n\n```go\n// x/gov/handler.go\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\n    ...\n    types.Context.EventManager().EmitTypedEvent(\n        &EventSubmitProposal{\n            FromAddress: fromAddress,\n            ProposalId: id,\n            Proposal: proposal,\n        },\n    )\n    ...\n}\n```\n\n### How to subscribe to these typed events in `Client`\n\n> NOTE: Full code example below\n\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\n\nAkash Network has built a simple [`pubsub`](https://github.com/ovrclk/akash/blob/90d258caeb933b611d575355b8df281208a214f8/pubsub/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https://github.com/ovrclk/akash/blob/90d258caeb933b611d575355b8df281208a214f8/events/publish.go#L21) them as typed events.\n\nPlease see the below code sample for more detail on how this flow looks for clients.\n\n## Consequences\n\n### Positive\n\n* Improves consistency of implementation for the events currently in the Cosmos SDK\n* Provides a much more ergonomic way to handle events and facilitates writing event driven applications\n* This implementation will support a middleware ecosystem of `EventHandler`s\n\n### Negative\n\n## Detailed code example of publishing events\n\nThis ADR also proposes adding affordances to emit and consume these events. This way developers will only need to write\n`EventHandler`s which define the actions they desire to take.\n\n```go\n// EventEmitter is a type that describes event emitter functions\n// This should be defined in `types/events.go`\ntype EventEmitter func(context.Context, client.Context, ...EventHandler) error\n\n// EventHandler is a type of function that handles events coming out of the event bus\n// This should be defined in `types/events.go`\ntype EventHandler func(proto.Message) error\n\n// Sample use of the functions below\nfunc main() {\n    ctx, cancel := context.WithCancel(context.Background())\n\n    if err := TxEmitter(ctx, client.Context{}.WithNodeURI(\"tcp://localhost:26657\"), SubmitProposalEventHandler); err != nil {\n        cancel()\n        panic(err)\n    }\n\n    return\n}\n\n// SubmitProposalEventHandler is an example of an event handler that prints proposal details\n// when any EventSubmitProposal is emitted.\nfunc SubmitProposalEventHandler(ev proto.Message) (err error) {\n    switch event := ev.(type) {\n    // Handle governance proposal events creation events\n    case govtypes.EventSubmitProposal:\n        // Users define business logic here e.g.\n        fmt.Println(ev.FromAddress, ev.ProposalId, ev.Proposal)\n        return nil\n    default:\n        return nil\n    }\n}\n\n// TxEmitter is an example of an event emitter that emits just transaction events. This can and\n// should be implemented somewhere in the Cosmos SDK. The Cosmos SDK can include an EventEmitters for tm.event='Tx'\n// and/or tm.event='NewBlock' (the new block events may contain typed events)\nfunc TxEmitter(ctx context.Context, cliCtx client.Context, ehs ...EventHandler) (err error) {\n    // Instantiate and start CometBFT RPC client\n    client, err := cliCtx.GetNode()\n    if err != nil {\n        return err\n    }\n\n    if err = client.Start(); err != nil {\n        return err\n    }\n\n    // Start the pubsub bus\n    bus := pubsub.NewBus()\n    defer bus.Close()\n\n    // Initialize a new error group\n    eg, ctx := errgroup.WithContext(ctx)\n\n    // Publish chain events to the pubsub bus\n    eg.Go(func() error {\n        return PublishChainTxEvents(ctx, client, bus, simapp.ModuleBasics)\n    })\n\n    // Subscribe to the bus events\n    subscriber, err := bus.Subscribe()\n    if err != nil {\n        return err\n    }\n\n\t// Handle all the events coming out of the bus\n\teg.Go(func() error {\n        var err error\n        for {\n            select {\n            case <-ctx.Done():\n                return nil\n            case <-subscriber.Done():\n                return nil\n            case ev := <-subscriber.Events():\n                for _, eh := range ehs {\n                    if err = eh(ev); err != nil {\n                        break\n                    }\n                }\n            }\n        }\n        return nil\n\t})\n\n\treturn group.Wait()\n}\n\n// PublishChainTxEvents events using cmtclient. Waits on context shutdown signals to exit.\nfunc PublishChainTxEvents(ctx context.Context, client cmtclient.EventsClient, bus pubsub.Bus, mb module.BasicManager) (err error) {\n    // Subscribe to transaction events\n    txch, err := client.Subscribe(ctx, \"txevents\", \"tm.event='Tx'\", 100)\n    if err != nil {\n        return err\n    }\n\n    // Unsubscribe from transaction events on function exit\n    defer func() {\n        err = client.UnsubscribeAll(ctx, \"txevents\")\n    }()\n\n    // Use errgroup to manage concurrency\n    g, ctx := errgroup.WithContext(ctx)\n\n    // Publish transaction events in a goroutine\n    g.Go(func() error {\n        var err error\n        for {\n            select {\n            case <-ctx.Done():\n                break\n            case ed := <-ch:\n                switch evt := ed.Data.(type) {\n                case cmttypes.EventDataTx:\n                    if !evt.Result.IsOK() {\n                        continue\n                    }\n                    // range over events, parse them using the basic manager and\n                    // send them to the pubsub bus\n                    for _, abciEv := range events {\n                        typedEvent, err := sdk.ParseTypedEvent(abciEv)\n                        if err != nil {\n                            return err\n                        }\n                        if err := bus.Publish(typedEvent); err != nil {\n                            bus.Close()\n                            return\n                        }\n                        continue\n                    }\n                }\n            }\n        }\n        return err\n\t})\n\n    // Exit on error or context cancellation\n    return g.Wait()\n}\n```\n\n## References\n\n* [Publish Custom Events via a bus](https://github.com/ovrclk/akash/blob/90d258caeb933b611d575355b8df281208a214f8/events/publish.go#L19-L58)\n* [Consuming the events in `Client`](https://github.com/ovrclk/deploy/blob/bf6c633ab6c68f3026df59efd9982d6ca1bf0561/cmd/event-handlers.go#L57)"
  },
  {
    "number": 33,
    "filename": "adr-033-protobuf-inter-module-comm.md",
    "title": "ADR 033: Protobuf-based Inter-Module Communication",
    "content": "# ADR 033: Protobuf-based Inter-Module Communication\n\n## Changelog\n\n* 2020-10-05: Initial Draft\n\n## Status\n\nProposed\n\n## Abstract\n\nThis ADR introduces a system for permissioned inter-module communication leveraging the protobuf `Query` and `Msg`\nservice definitions defined in [ADR 021](./adr-021-protobuf-query-encoding.md) and\n[ADR 031](./adr-031-msg-service.md) which provides:\n\n* stable protobuf based module interfaces to potentially later replace the keeper paradigm\n* stronger inter-module object capabilities (OCAPs) guarantees\n* module accounts and sub-account authorization\n\n## Context\n\nIn the current Cosmos SDK documentation on the [Object-Capability Model](../docs/learn/advanced/10-ocap.md), it is stated that:\n\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\n\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\n\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\n\n### `x/bank` Case Study\n\nCurrently the `x/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\n\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module’s\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\n\nHowever, these permissions don’t really do much. They control what modules can be referenced in the `MintCoins`,\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access —\njust a simple string. So the `x/upgrade` module could mint tokens for the `x/staking` module simply by calling\n`MintCoins(“staking”)`. Furthermore, all modules which have access to these keeper methods, also have access to\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\n\n## Decision\n\nBased on [ADR-021](./adr-021-protobuf-query-encoding.md) and [ADR-031](./adr-031-msg-service.md), we introduce the\nInter-Module Communication framework for secure module authorization and OCAPs.\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\n\nOf particular note — the decision is to _enable_ this functionality for modules to adopt at their own discretion.\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\naddressed as amendments to this ADR.\n\n### New \"Keeper\" Paradigm\n\nIn [ADR 021](./adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\nwas introduced and in [ADR 31](./adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg/Send` message type:\n\n```go\npackage bank\n\ntype MsgClient interface {\n\tSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\n}\n\ntype MsgServer interface {\n\tSend(context.Context, *MsgSend) (*MsgSendResponse, error)\n}\n```\n\n[ADR 021](./adr-021-protobuf-query-encoding.md) and [ADR 31](./adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\n\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\nbased service interfaces already used by clients for inter-module communication.\n\nUsing this `QueryClient`/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\n\n1. Protobuf types are checked for breaking changes using [buf](https://buf.build/docs/breaking-overview) and because of\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\nevolution.\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\nobject capability system (see below).\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\nenabling atomicity of operations ([currently a problem](https://github.com/cosmos/cosmos-sdk/issues/8030)). Any failure within a module-to-module call would result in a failure of the entire\ntransaction\n\nThis mechanism has the added benefits of:\n\n* reducing boilerplate through code generation, and\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\n\n### Inter-module Communication\n\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https://github.com/grpc/grpc-go/blob/v1.49.x/clientconn.go#L441-L450)\nimplementation. For this we introduce\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\ndescribed in more detail below.\n\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\nmessage specifies required signers with `Msg.GetSigner`). The authentication check is performed by `AnteHandler`.\n\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\nthat we don't use any cryptographic signature in this case.\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `/cosmos.bank.Msg/Send` transaction. `MsgSend` validation\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\n\nHere's an example of a hypothetical module `foo` interacting with `x/bank`:\n\n```go\npackage foo\n\n\ntype FooMsgServer {\n  // ...\n\n  bankQuery bank.QueryClient\n  bankMsg   bank.MsgClient\n}\n\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\n  // ...\n\n  return FooMsgServer {\n    // ...\n    modouleKey: moduleKey,\n    bankQuery: bank.NewQueryClient(moduleKey),\n    bankMsg: bank.NewMsgClient(moduleKey),\n  }\n}\n\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\n  balance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: foo.moduleKey.Address(), Denom: \"foo\"})\n\n  ...\n\n  res, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\n\n  ...\n}\n```\n\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\ndenom prefix being restricted to certain modules (as discussed in\n[#7459](https://github.com/cosmos/cosmos-sdk/pull/7459#discussion_r529545528)).\n\n### `ModuleKey`s and `ModuleID`s\n\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\ncorresponding \"public key\". From the [ADR 028](./adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\nand forms its address based on the `AddressHash` method from [the ADR-028](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-028-public-key-addresses.md):\n\n```go\ntype ModuleID struct {\n  ModuleName string\n  Path []byte\n}\n\nfunc (key ModuleID) Address() []byte {\n  return AddressHash(key.ModuleName, key.Path)\n}\n```\n\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\nthe `ModuleKey` security.\n\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\n\n```go\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\n\ntype CallInfo {\n  Method string\n  Caller ModuleID\n}\n\ntype RootModuleKey struct {\n  moduleName string\n  invoker Invoker\n}\n\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { /* ... */}\n\ntype DerivedModuleKey struct {\n  moduleName string\n  path []byte\n  invoker Invoker\n}\n```\n\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\n\n```go\npackage foo\n\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\n  derivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\n  bankMsgClient := bank.NewMsgClient(derivedKey)\n  res, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\n  ...\n}\n```\n\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\nfrom either the root or a derived module account to pass through.\n\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\nchecking permissions.\n\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\nname impersonation.\n\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\n\n```go\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\n  f := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\n  return f(ctx, args, reply)\n}\n```\n\n### `AppModule` Wiring and Requirements\n\nIn [ADR 031](./adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\nspecify their dependencies on other modules using `RequireServer()`:\n\n```go\ntype Configurator interface {\n   MsgServer() grpc.Server\n   QueryServer() grpc.Server\n\n   ModuleKey() ModuleKey\n   RequireServer(msgServer interface{})\n}\n```\n\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\n`app.go`. For now, `ModuleKey`s will be created based on `AppModuleBasic.Name()`, but a more flexible system may be\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\n\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\nmodule `foo` could declare its dependency on `x/bank` like this:\n\n```go\npackage foo\n\nfunc (am AppModule) RegisterServices(cfg Configurator) {\n  cfg.RequireServer((*bank.QueryServer)(nil))\n  cfg.RequireServer((*bank.MsgServer)(nil))\n}\n```\n\n### Security Considerations\n\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\nthe underlying router infrastructure.\n\n#### Recursion and Re-entry\n\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\ncalls Module B and Module B calls module A again in the same call.\n\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\nin the router could be used to perform this security check.\n\n#### Queries\n\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\n\n### Internal Methods\n\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\npurpose, we add the `InternalServer` method to `Configurator`:\n\n```go\ntype Configurator interface {\n   MsgServer() grpc.Server\n   QueryServer() grpc.Server\n   InternalServer() grpc.Server\n}\n```\n\nAs an example, x/slashing's Slash must call x/staking's Slash, but we don't want to expose x/staking's Slash to end users\nand clients.\n\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\nproto package.\n\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\n\nAn alternative solution to internal-only methods could involve hooks / plugins as discussed [here](https://github.com/cosmos/cosmos-sdk/pull/7459#issuecomment-733807753).\nA more detailed evaluation of a hooks / plugin system will be addressed later in follow-ups to this ADR or as a separate\nADR.\n\n### Authorization\n\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-030-authz-module.md).\nThis middleware will allow accounts to authorize specific module accounts to perform actions on their behalf.\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\nother modules. This will be addressed in separate ADRs or updates to this ADR.\n\n### Future Work\n\nOther future improvements may include:\n\n* custom code generation that:\n    * simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\n    * optimizes inter-module calls - for instance caching resolved methods after first invocation\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\n* code generation which makes inter-module communication more performant\n* decoupling `ModuleKey` creation from `AppModuleBasic.Name()` so that app's can override root module account names\n* inter-module hooks and plugins\n\n## Alternatives\n\n### MsgServices vs `x/capability`\n\nThe `x/capability` module does provide a proper object-capability implementation that can be used by any module in the\nCosmos SDK and could even be used for inter-module OCAPs as described in [\\#5931](https://github.com/cosmos/cosmos-sdk/issues/5931).\n\nThe advantages of the approach described in this ADR are mostly around how it integrates with other parts of the Cosmos SDK,\nspecifically:\n\n* protobuf so that:\n    * code generation of interfaces can be leveraged for a better dev UX\n    * module interfaces are versioned and checked for breakage using [buf](https://docs.buf.build/breaking-overview)\n* sub-module accounts as per ADR 028\n* the general `Msg` passing paradigm and the way signers are specified by `GetSigners`\n\nAlso, this is a complete replacement for keepers and could be applied to _all_ inter-module communication whereas the\n`x/capability` approach in #5931 would need to be applied method by method.\n\n## Consequences\n\n### Backwards Compatibility\n\nThis ADR is intended to provide a pathway to a scenario where there is greater long term compatibility between modules.\nIn the short-term, this will likely result in breaking certain `Keeper` interfaces which are too permissive and/or\nreplacing `Keeper` interfaces altogether.\n\n### Positive\n\n* an alternative to keepers which can more easily lead to stable inter-module interfaces\n* proper inter-module OCAPs\n* improved module developer DevX, as commented on by several participants on\n    [Architecture Review Call, Dec 3](https://hackmd.io/E0wxxOvRQ5qVmTf6N_k84Q)\n* lays the groundwork for what can be a greatly simplified `app.go`\n* router can be setup to enforce atomic transactions for module-to-module calls\n\n### Negative\n\n* modules which adopt this will need significant refactoring\n\n### Neutral\n\n## Test Cases [optional]\n\n## References\n\n* [ADR 021](./adr-021-protobuf-query-encoding.md)\n* [ADR 031](./adr-031-msg-service.md)\n* [ADR 028](./adr-028-public-key-addresses.md)\n* [ADR 030 draft](https://github.com/cosmos/cosmos-sdk/pull/7105)\n* [Object-Capability Model](https://docs.network.com/main/core/ocap)"
  },
  {
    "number": 34,
    "filename": "adr-034-account-rekeying.md",
    "title": "ADR 034: Account Rekeying",
    "content": "# ADR 034: Account Rekeying\n\n## Changelog\n\n* 30-09-2020: Initial Draft\n\n## Status\n\nPROPOSED\n\n## Abstract\n\nAccount rekeying is a process that allows an account to replace its authentication pubkey with a new one.\n\n## Context\n\nCurrently, in the Cosmos SDK, the address of an auth `BaseAccount` is based on the hash of the public key.  Once an account is created, the public key for the account is set in stone, and cannot be changed.  This can be a problem for users, as key rotation is a useful security practice, but is not possible currently.  Furthermore, as multisigs are a type of pubkey, once a multisig for an account is set, it cannot be updated.  This is problematic, as multisigs are often used by organizations or companies, who may need to change their set of multisig signers for internal reasons.\n\nTransferring all the assets of an account to a new account with the updated pubkey is not sufficient, because some \"engagements\" of an account are not easily transferable.  For example, in staking, to transfer bonded Atoms, an account would have to unbond all delegations and wait the three-week unbonding period.  Even more significantly, for validator operators, ownership over a validator is not transferable at all, meaning that the operator key for a validator can never be updated, leading to poor operational security for validators.\n\n## Decision\n\nWe propose the addition of a new feature to `x/auth` that allows accounts to update the public key associated with their account, while keeping the address the same.\n\nThis is possible because the Cosmos SDK `BaseAccount` stores the public key for an account in state, instead of making the assumption that the public key is included in the transaction (whether explicitly or implicitly through the signature) as in other blockchains such as Bitcoin and Ethereum.  Because the public key is stored on chain, it is okay for the public key to not hash to the address of an account, as the address is not pertinent to the signature checking process.\n\nTo build this system, we design a new Msg type as follows:\n\n```protobuf\nservice Msg {\n    rpc ChangePubKey(MsgChangePubKey) returns (MsgChangePubKeyResponse);\n}\n\nmessage MsgChangePubKey {\n  string address = 1;\n  google.protobuf.Any pub_key = 2;\n}\n\nmessage MsgChangePubKeyResponse {}\n```\n\nThe MsgChangePubKey transaction needs to be signed by the existing pubkey in state.\n\nOnce approved, the handler for this message type, which takes in the AccountKeeper, will update the in-state pubkey for the account and replace it with the pubkey from the Msg.\n\nAn account that has had its pubkey changed cannot be automatically pruned from state.  This is because if pruned, the original pubkey of the account would be needed to recreate the same address, but the owner of the address may not have the original pubkey anymore.  Currently, we do not automatically prune any accounts anyways, but we would like to keep this option open down the road (this is the purpose of account numbers).  To resolve this, we charge an additional gas fee for this operation to compensate for this externality (this bound gas amount is configured as a parameter `PubKeyChangeCost`). The bonus gas is charged inside the handler, using the `ConsumeGas` function.  Furthermore, in the future, we can allow accounts that have rekeyed manually prune themselves using a new Msg type such as `MsgDeleteAccount`.  Manually pruning accounts can give a gas refund as an incentive for performing the action.\n\n```go\n\tamount := ak.GetParams(ctx).PubKeyChangeCost\n\tctx.GasMeter().ConsumeGas(amount, \"pubkey change fee\")\n```\n\nEvery time a key for an address is changed, we will store a log of this change in the state of the chain, thus creating a stack of all previous keys for an address and the time intervals for which they were active.  This allows dapps and clients to easily query past keys for an account which may be useful for features such as verifying timestamped off-chain signed messages.\n\n## Consequences\n\n### Positive\n\n* Will allow users and validator operators to employ better operational security practices with key rotation.\n* Will allow organizations or groups to easily change and add/remove multisig signers.\n\n### Negative\n\nBreaks the current assumed relationship between address and pubkey as H(pubkey) = address. This has a couple of consequences.\n\n* This makes wallets that support this feature more complicated. For example, if an address on-chain was updated, the corresponding key in the CLI wallet also needs to be updated.\n* Cannot automatically prune accounts with 0 balance that have had their pubkey changed.\n\n### Neutral\n\n* While the purpose of this is intended to allow the owner of an account to update to a new pubkey they own, this could technically also be used to transfer ownership of an account to a new owner.  For example, this could be used to sell a staked position without unbonding or an account that has vesting tokens.  However, the friction of this is very high as this would essentially have to be done as a very specific OTC trade. Furthermore, additional constraints could be added to prevent accounts with Vesting tokens to use this feature.\n* Will require that PubKeys for an account are included in the genesis exports.\n\n## References\n\n* https://www.algorand.com/resources/blog/announcing-rekeying"
  },
  {
    "number": 35,
    "filename": "adr-035-rosetta-api-support.md",
    "title": "ADR 035: Rosetta API Support",
    "content": "# ADR 035: Rosetta API Support\n\n## Authors\n\n* Jonathan Gimeno (@jgimeno)\n* David Grierson (@senormonito)\n* Alessio Treglia (@alessio)\n* Frojdy Dymylja (@fdymylja)\n\n## Changelog\n\n* 2021-05-12: the external library [cosmos-rosetta-gateway](https://github.com/tendermint/cosmos-rosetta-gateway) has been moved within the Cosmos SDK.\n\n## Context\n\n[Rosetta API](https://www.rosetta-api.org/) is an open-source specification and set of tools developed by Coinbase to\nstandardise blockchain interactions.\n\nThrough the use of a standard API for integrating blockchain applications it will\n\n* Be easier for a user to interact with a given blockchain\n* Allow exchanges to integrate new blockchains quickly and easily\n* Enable application developers to build cross-blockchain applications such as block explorers, wallets and dApps at\n  considerably lower cost and effort.\n\n## Decision\n\nIt is clear that adding Rosetta API support to the Cosmos SDK will bring value to all the developers and\nCosmos SDK based chains in the ecosystem. How it is implemented is key.\n\nThe driving principles of the proposed design are:\n\n1. **Extensibility:** it must be as riskless and painless as possible for application developers to set-up network\n   configurations to expose Rosetta API-compliant services.\n2. **Long term support:** This proposal aims to provide support for all the Cosmos SDK release series.\n3. **Cost-efficiency:** Backporting changes to Rosetta API specifications from `master` to the various stable\n   branches of Cosmos SDK is a cost that needs to be reduced.\n\nWe will achieve these by delivering on these principles by the following:\n\n1. There will be a package `rosetta/lib`\n   for the implementation of the core Rosetta API features, particularly:\n   a. The types and interfaces (`Client`, `OfflineClient`...), this separates design from implementation detail.\n   b. The `Server` functionality as this is independent of the Cosmos SDK version.\n   c. The `Online/OfflineNetwork`, which is not exported, and implements the rosetta API using the `Client` interface to query the node, build tx and so on.\n   d. The `errors` package to extend rosetta errors.\n2. Due to differences between the Cosmos release series, each series will have its own specific implementation of `Client` interface.\n3. There will be two options for starting an API service in applications:\n   a. API shares the application process\n   b. API-specific process.\n\n## Architecture\n\n### The External Repo\n\nThis section will describe the proposed external library, including the service implementation, plus the defined types and interfaces.\n\n#### Server\n\n`Server` is a simple `struct` that is started and listens to the port specified in the settings. This is meant to be used across all the Cosmos SDK versions that are actively supported.\n\nThe constructor follows:\n\n`func NewServer(settings Settings) (Server, error)`\n\n`Settings`, which are used to construct a new server, are the following:\n\n```go\n// Settings define the rosetta server settings\ntype Settings struct {\n\t// Network contains the information regarding the network\n\tNetwork *types.NetworkIdentifier\n\t// Client is the online API handler\n\tClient crgtypes.Client\n\t// Listen is the address the handler will listen at\n\tListen string\n\t// Offline defines if the rosetta service should be exposed in offline mode\n\tOffline bool\n\t// Retries is the number of readiness checks that will be attempted when instantiating the handler\n\t// valid only for online API\n\tRetries int\n\t// RetryWait is the time that will be waited between retries\n\tRetryWait time.Duration\n}\n```\n\n#### Types\n\nPackage types uses a mixture of rosetta types and custom defined type wrappers, that the client must parse and return while executing operations.\n\n##### Interfaces\n\nEvery SDK version uses a different format to connect (rpc, gRPC, etc), query and build transactions, we have abstracted this in what is the `Client` interface.\nThe client uses rosetta types, whilst the `Online/OfflineNetwork` takes care of returning correctly parsed rosetta responses and errors.\n\nEach Cosmos SDK release series will have their own `Client` implementations.\nDevelopers can implement their own custom `Client`s as required.\n\n```go\n// Client defines the API the client implementation should provide.\ntype Client interface {\n\t// Needed if the client needs to perform some action before connecting.\n\tBootstrap() error\n\t// Ready checks if the servicer constraints for queries are satisfied\n\t// for example the node might still not be ready, it's useful in process\n\t// when the rosetta instance might come up before the node itself\n\t// the servicer must return nil if the node is ready\n\tReady() error\n\n\t// Data API\n\n\t// Balances fetches the balance of the given address\n\t// if height is not nil, then the balance will be displayed\n\t// at the provided height, otherwise last block balance will be returned\n\tBalances(ctx context.Context, addr string, height *int64) ([]*types.Amount, error)\n\t// BlockByHashAlt gets a block and its transaction at the provided height\n\tBlockByHash(ctx context.Context, hash string) (BlockResponse, error)\n\t// BlockByHeightAlt gets a block given its height, if height is nil then last block is returned\n\tBlockByHeight(ctx context.Context, height *int64) (BlockResponse, error)\n\t// BlockTransactionsByHash gets the block, parent block and transactions\n\t// given the block hash.\n\tBlockTransactionsByHash(ctx context.Context, hash string) (BlockTransactionsResponse, error)\n\t// BlockTransactionsByHeight gets the block, parent block and transactions\n\t// given the block height.\n\tBlockTransactionsByHeight(ctx context.Context, height *int64) (BlockTransactionsResponse, error)\n\t// GetTx gets a transaction given its hash\n\tGetTx(ctx context.Context, hash string) (*types.Transaction, error)\n\t// GetUnconfirmedTx gets an unconfirmed Tx given its hash\n\t// NOTE(fdymylja): NOT IMPLEMENTED YET!\n\tGetUnconfirmedTx(ctx context.Context, hash string) (*types.Transaction, error)\n\t// Mempool returns the list of the current non confirmed transactions\n\tMempool(ctx context.Context) ([]*types.TransactionIdentifier, error)\n\t// Peers gets the peers currently connected to the node\n\tPeers(ctx context.Context) ([]*types.Peer, error)\n\t// Status returns the node status, such as sync data, version etc\n\tStatus(ctx context.Context) (*types.SyncStatus, error)\n\n\t// Construction API\n\n\t// PostTx posts txBytes to the node and returns the transaction identifier plus metadata related\n\t// to the transaction itself.\n\tPostTx(txBytes []byte) (res *types.TransactionIdentifier, meta map[string]interface{}, err error)\n\t// ConstructionMetadataFromOptions\n\tConstructionMetadataFromOptions(ctx context.Context, options map[string]interface{}) (meta map[string]interface{}, err error)\n\tOfflineClient\n}\n\n// OfflineClient defines the functionalities supported without having access to the node\ntype OfflineClient interface {\n\tNetworkInformationProvider\n\t// SignedTx returns the signed transaction given the tx bytes (msgs) plus the signatures\n\tSignedTx(ctx context.Context, txBytes []byte, sigs []*types.Signature) (signedTxBytes []byte, err error)\n\t// TxOperationsAndSignersAccountIdentifiers returns the operations related to a transaction and the account\n\t// identifiers if the transaction is signed\n\tTxOperationsAndSignersAccountIdentifiers(signed bool, hexBytes []byte) (ops []*types.Operation, signers []*types.AccountIdentifier, err error)\n\t// ConstructionPayload returns the construction payload given the request\n\tConstructionPayload(ctx context.Context, req *types.ConstructionPayloadsRequest) (resp *types.ConstructionPayloadsResponse, err error)\n\t// PreprocessOperationsToOptions returns the options given the preprocess operations\n\tPreprocessOperationsToOptions(ctx context.Context, req *types.ConstructionPreprocessRequest) (options map[string]interface{}, err error)\n\t// AccountIdentifierFromPublicKey returns the account identifier given the public key\n\tAccountIdentifierFromPublicKey(pubKey *types.PublicKey) (*types.AccountIdentifier, error)\n}\n```\n\n### 2. Cosmos SDK Implementation\n\nThe Cosmos SDK implementation, based on version, takes care of satisfying the `Client` interface.\nIn Stargate, Launchpad and 0.37, we have introduced the concept of rosetta.Msg, this message is not in the shared repository as the sdk.Msg type differs between Cosmos SDK versions.\n\nThe rosetta.Msg interface follows:\n\n```go\n// Msg represents a cosmos-sdk message that can be converted from and to a rosetta operation.\ntype Msg interface {\n\tsdk.Msg\n\tToOperations(withStatus, hasError bool) []*types.Operation\n\tFromOperations(ops []*types.Operation) (sdk.Msg, error)\n}\n```\n\nHence developers who want to extend the rosetta set of supported operations just need to extend their module's sdk.Msgs with the `ToOperations` and `FromOperations` methods.\n\n### 3. API service invocation\n\nAs stated at the start, application developers will have two methods for invocation of the Rosetta API service:\n\n1. Shared process for both application and API\n2. Standalone API service\n\n#### Shared Process (Only Stargate)\n\nRosetta API service could run within the same execution process as the application. This would be enabled via app.toml settings, and if gRPC is not enabled the rosetta instance would be spun in offline mode (tx building capabilities only).\n\n#### Separate API service\n\nClient application developers can write a new command to launch a Rosetta API server as a separate process too, using the rosetta command contained in the `/server/rosetta` package. Construction of the command depends on Cosmos SDK version. Examples can be found inside `simd` for stargate, and `contrib/rosetta/simapp` for other release series.\n\n## Status\n\nProposed\n\n## Consequences\n\n### Positive\n\n* Out-of-the-box Rosetta API support within Cosmos SDK.\n* Blockchain interface standardisation\n\n## References\n\n* https://www.rosetta-api.org/"
  },
  {
    "number": 36,
    "filename": "adr-036-arbitrary-signature.md",
    "title": "ADR 036: Arbitrary Message Signature Specification",
    "content": "# ADR 036: Arbitrary Message Signature Specification\n\n## Changelog\n\n* 28/10/2020 - Initial draft\n\n## Authors\n\n* Antoine Herzog (@antoineherzog)\n* Zaki Manian (@zmanian)\n* Aleksandr Bezobchuk (alexanderbez) [1]\n* Frojdi Dymylja (@fdymylja)\n\n## Status\n\nDraft\n\n## Abstract\n\nCurrently, in the Cosmos SDK, there is no convention to sign arbitrary messages like in Ethereum. We propose with this specification, for Cosmos SDK ecosystem, a way to sign and validate off-chain arbitrary messages.\n\nThis specification serves the purpose of covering every use case; this means that Cosmos SDK application developers decide how to serialize and represent `Data` to users.\n\n## Context\n\nHaving the ability to sign messages off-chain has proven to be a fundamental aspect of nearly any blockchain. The notion of signing messages off-chain has many added benefits such as saving on computational costs and reducing transaction throughput and overhead. Within the context of the Cosmos, some of the major applications of signing such data include, but is not limited to, providing a cryptographic secure and verifiable means of proving validator identity and possibly associating it with some other framework or organization. In addition, having the ability to sign Cosmos messages with a Ledger or similar HSM device.\n\nFurther context and use cases can be found in the reference links.\n\n## Decision\n\nThe aim is being able to sign arbitrary messages, even using Ledger or similar HSM devices.\n\nAs a result, signed messages should look roughly like Cosmos SDK messages but **must not** be a valid on-chain transaction. `chain-id`, `account_number` and `sequence` can all be assigned invalid values.\n\nCosmos SDK 0.40 also introduces a concept of “auth_info” this can specify SIGN_MODES.\n\nA spec should include an `auth_info` that supports SIGN_MODE_DIRECT and SIGN_MODE_LEGACY_AMINO.\n\nTo create the `offchain` proto definitions, we extend the auth module with `offchain` package to offer functionalities to verify and sign offline messages.\n\nAn offchain transaction follows these rules:\n\n* the memo must be empty\n* nonce, sequence number must be equal to 0\n* chain-id must be equal to “”\n* fee gas must be equal to 0\n* fee amount must be an empty array\n\nVerification of an offchain transaction follows the same rules as an onchain one, except for the spec differences highlighted above.\n\nThe first message added to the `offchain` package is `MsgSignData`.\n\n`MsgSignData` allows developers to sign arbitrary bytes validatable offchain only. `Signer` is the account address of the signer. `Data` is arbitrary bytes which can represent `text`, `files`, `object`s. It's applications developers decision how `Data` should be deserialized, serialized and the object it can represent in their context.\n\nIt's applications developers decision how `Data` should be treated, by treated we mean the serialization and deserialization process and the Object `Data` should represent.\n\nProto definition:\n\n```protobuf\n// MsgSignData defines an arbitrary, general-purpose, off-chain message\nmessage MsgSignData {\n    // Signer is the sdk.AccAddress of the message signer\n    bytes Signer = 1 [(gogoproto.jsontag) = \"signer\", (gogoproto.casttype) = \"github.com/cosmos/cosmos-sdk/types.AccAddress\"];\n    // Data represents the raw bytes of the content that is signed (text, json, etc)\n    bytes Data = 2 [(gogoproto.jsontag) = \"data\"];\n}\n```\n\nSigned MsgSignData json example:\n\n```json\n{\n  \"type\": \"cosmos-sdk/StdTx\",\n  \"value\": {\n    \"msg\": [\n      {\n        \"type\": \"sign/MsgSignData\",\n        \"value\": {\n          \"signer\": \"cosmos1hftz5ugqmpg9243xeegsqqav62f8hnywsjr4xr\",\n          \"data\": \"cmFuZG9t\"\n        }\n      }\n    ],\n    \"fee\": {\n      \"amount\": [],\n      \"gas\": \"0\"\n    },\n    \"signatures\": [\n      {\n        \"pub_key\": {\n          \"type\": \"tendermint/PubKeySecp256k1\",\n          \"value\": \"AqnDSiRoFmTPfq97xxEb2VkQ/Hm28cPsqsZm9jEVsYK9\"\n        },\n        \"signature\": \"8y8i34qJakkjse9pOD2De+dnlc4KvFgh0wQpes4eydN66D9kv7cmCEouRrkka9tlW9cAkIL52ErB+6ye7X5aEg==\"\n      }\n    ],\n    \"memo\": \"\"\n  }\n}\n```\n\n## Consequences\n\nThere is a specification on how messages, that are not meant to be broadcast to a live chain, should be formed.\n\n### Backwards Compatibility\n\nBackwards compatibility is maintained as this is a new message spec definition.\n\n### Positive\n\n* A common format that can be used by multiple applications to sign and verify off-chain messages.\n* The specification is primitive which means it can cover every use case without limiting what is possible to fit inside it.\n* It gives room for other off-chain messages specifications that aim to target more specific and common use cases such as off-chain-based authN/authZ layers [2].\n\n### Negative\n\n* The current proposal requires a fixed relationship between an account address and a public key.\n* Doesn't work with multisig accounts.\n\n## Further discussion\n\n* Regarding security in `MsgSignData`, the developer using `MsgSignData` is in charge of making the content contained in `Data` non-replayable when, and if, needed.\n* The offchain package will be further extended with extra messages that target specific use cases such as, but not limited to, authentication in applications, payment channels, L2 solutions in general.\n\n## References\n\n1. https://github.com/cosmos/ics/pull/33\n2. https://github.com/cosmos/cosmos-sdk/pull/7727#discussion_r515668204\n3. https://github.com/cosmos/cosmos-sdk/pull/7727#issuecomment-722478477\n4. https://github.com/cosmos/cosmos-sdk/pull/7727#issuecomment-721062923"
  },
  {
    "number": 37,
    "filename": "adr-037-gov-split-vote.md",
    "title": "ADR 037: Governance split votes",
    "content": "# ADR 037: Governance split votes\n\n## Changelog\n\n* 2020/10/28: Initial draft\n\n## Status\n\nAccepted\n\n## Abstract\n\nThis ADR defines a modification to the governance module that would allow a staker to split their votes into several voting options. For example, it could use 70% of its voting power to vote Yes and 30% of its voting power to vote No.\n\n## Context\n\nCurrently, an address can cast a vote with only one option (Yes/No/Abstain/NoWithVeto) and use their full voting power behind that choice.\n\nHowever, oftentimes the entity owning that address might not be a single individual.  For example, a company might have different stakeholders who want to vote differently, and so it makes sense to allow them to split their voting power.  Another example use case is exchanges.  Many centralized exchanges often stake a portion of their users' tokens in their custody.  Currently, it is not possible for them to do \"passthrough voting\" and giving their users voting rights over their tokens.  However, with this system, exchanges can poll their users for voting preferences, and then vote on-chain proportionally to the results of the poll.\n\n## Decision\n\nWe modify the vote structs to be\n\n```go\ntype WeightedVoteOption struct {\n  Option string\n  Weight sdk.Dec\n}\n\ntype Vote struct {\n  ProposalID int64\n  Voter      sdk.Address\n  Options    []WeightedVoteOption\n}\n```\n\nAnd for backwards compatibility, we introduce `MsgVoteWeighted` while keeping `MsgVote`.\n\n```go\ntype MsgVote struct {\n  ProposalID int64\n  Voter      sdk.Address\n  Option     Option\n}\n\ntype MsgVoteWeighted struct {\n  ProposalID int64\n  Voter      sdk.Address\n  Options    []WeightedVoteOption\n}\n```\n\nThe `ValidateBasic` of a `MsgVoteWeighted` struct would require that\n\n1. The sum of all the rates is equal to 1.0\n2. No Option is repeated\n\nThe governance tally function will iterate over all the options in a vote and add to the tally the result of the voter's voting power * the rate for that option.\n\n```go\ntally() {\n    results := map[types.VoteOption]sdk.Dec\n\n    for _, vote := range votes {\n        for i, weightedOption := range vote.Options {\n            results[weightedOption.Option] += getVotingPower(vote.voter) * weightedOption.Weight\n        }\n    }\n}\n```\n\nThe CLI command for creating a multi-option vote would be as such:\n\n```shell\nsimd tx gov vote 1 \"yes=0.6,no=0.3,abstain=0.05,no_with_veto=0.05\" --from mykey\n```\n\nTo create a single-option vote a user can do either\n\n```shell\nsimd tx gov vote 1 \"yes=1\" --from mykey\n```\n\nor\n\n```shell\nsimd tx gov vote 1 yes --from mykey\n```\n\nto maintain backwards compatibility.\n\n## Consequences\n\n### Backwards Compatibility\n\n* Previous VoteMsg types will remain the same and so clients will not have to update their procedure unless they want to support the WeightedVoteMsg feature.\n* When querying a Vote struct from state, its structure will be different, and so clients wanting to display all voters and their respective votes will have to handle the new format and the fact that a single voter can have split votes.\n* The result of querying the tally function should have the same API for clients.\n\n### Positive\n\n* Can make the voting process more accurate for addresses representing multiple stakeholders, often some of the largest addresses.\n\n### Negative\n\n* Is more complex than simple voting, and so may be harder to explain to users.  However, this is mostly mitigated because the feature is opt-in.\n\n### Neutral\n\n* Relatively minor change to governance tally function."
  },
  {
    "number": 38,
    "filename": "adr-038-state-listening.md",
    "title": "ADR 038: KVStore state listening",
    "content": "# ADR 038: KVStore state listening\n\n## Changelog\n\n* 11/23/2020: Initial draft\n* 10/06/2022: Introduce plugin system based on hashicorp/go-plugin\n* 10/14/2022:\n    * Add `ListenCommit`, flatten the state writes in a block to a single batch.\n    * Remove listeners from cache stores, should only listen to `rootmulti.Store`.\n    * Remove `HaltAppOnDeliveryError()`, the errors are propagated by default, the implementations should return nil if don't want to propagate errors.\n* 26/05/2023: Update with ABCI 2.0\n\n## Status\n\nProposed\n\n## Abstract\n\nThis ADR defines a set of changes to enable listening to state changes of individual KVStores and exposing these data to consumers.\n\n## Context\n\nCurrently, KVStore data can be remotely accessed through [Queries](https://github.com/cosmos/cosmos-sdk/blob/master/docs/building-modules/messages-and-queries.md#queries)\nwhich proceed either through Tendermint and the ABCI, or through the gRPC server.\nIn addition to these request/response queries, it would be beneficial to have a means of listening to state changes as they occur in real time.\n\n## Decision\n\nWe will modify the `CommitMultiStore` interface and its concrete (`rootmulti`) implementations and introduce a new `listenkv.Store` to allow listening to state changes in underlying KVStores. We don't need to listen to cache stores, because we can't be sure that the writes will be committed eventually, and the writes are duplicated in `rootmulti.Store` eventually, so we should only listen to `rootmulti.Store`.\nWe will introduce a plugin system for configuring and running streaming services that write these state changes and their surrounding ABCI message context to different destinations.\n\n### Listening\n\nIn a new file, `store/types/listening.go`, we will create a `MemoryListener` struct for streaming out protobuf encoded KV pairs state changes from a KVStore.\nThe `MemoryListener` will be used internally by the concrete `rootmulti` implementation to collect state changes from KVStores.\n\n```go\n// MemoryListener listens to the state writes and accumulate the records in memory.\ntype MemoryListener struct {\n\tstateCache []StoreKVPair\n}\n\n// NewMemoryListener creates a listener that accumulate the state writes in memory.\nfunc NewMemoryListener() *MemoryListener {\n\treturn &MemoryListener{}\n}\n\n// OnWrite writes state change events to the internal cache\nfunc (fl *MemoryListener) OnWrite(storeKey StoreKey, key []byte, value []byte, delete bool) {\n\tfl.stateCache = append(fl.stateCache, StoreKVPair{\n\t\tStoreKey: storeKey.Name(),\n\t\tDelete:   delete,\n\t\tKey:      key,\n\t\tValue:    value,\n\t})\n}\n\n// PopStateCache returns the current state caches and set to nil\nfunc (fl *MemoryListener) PopStateCache() []StoreKVPair {\n\tres := fl.stateCache\n\tfl.stateCache = nil\n\treturn res\n}\n```\n\nWe will also define a protobuf type for the KV pairs. In addition to the key and value fields this message\nwill include the StoreKey for the originating KVStore so that we can collect information from separate KVStores and determine the source of each KV pair.\n\n```protobuf\nmessage StoreKVPair {\n  optional string store_key = 1; // the store key for the KVStore this pair originates from\n  required bool set = 2; // true indicates a set operation, false indicates a delete operation\n  required bytes key = 3;\n  required bytes value = 4;\n}\n```\n\n### ListenKVStore\n\nWe will create a new `Store` type `listenkv.Store` that the `rootmulti` store will use to wrap a `KVStore` to enable state listening.\nWe will configure the `Store` with a `MemoryListener` which will collect state changes for output to specific destinations.\n\n```go\n// Store implements the KVStore interface with listening enabled.\n// Operations are traced on each core KVStore call and written to any of the\n// underlying listeners with the proper key and operation permissions\ntype Store struct {\n    parent    types.KVStore\n    listener  *types.MemoryListener\n    parentStoreKey types.StoreKey\n}\n\n// NewStore returns a reference to a new traceKVStore given a parent\n// KVStore implementation and a buffered writer.\nfunc NewStore(parent types.KVStore, psk types.StoreKey, listener *types.MemoryListener) *Store {\n    return &Store{parent: parent, listener: listener, parentStoreKey: psk}\n}\n\n// Set implements the KVStore interface. It traces a write operation and\n// delegates the Set call to the parent KVStore.\nfunc (s *Store) Set(key []byte, value []byte) {\n    types.AssertValidKey(key)\n    s.parent.Set(key, value)\n    s.listener.OnWrite(s.parentStoreKey, key, value, false)\n}\n\n// Delete implements the KVStore interface. It traces a write operation and\n// delegates the Delete call to the parent KVStore.\nfunc (s *Store) Delete(key []byte) {\n    s.parent.Delete(key)\n    s.listener.OnWrite(s.parentStoreKey, key, nil, true)\n}\n```\n\n### MultiStore interface updates\n\nWe will update the `CommitMultiStore` interface to allow us to wrap a `Memorylistener` to a specific `KVStore`.\nNote that the `MemoryListener` will be attached internally by the concrete `rootmulti` implementation.\n\n```go\ntype CommitMultiStore interface {\n    ...\n\n    // AddListeners adds a listener for the KVStore belonging to the provided StoreKey\n    AddListeners(keys []StoreKey)\n\n    // PopStateCache returns the accumulated state change messages from MemoryListener\n    PopStateCache() []StoreKVPair\n}\n```\n\n\n### MultiStore implementation updates\n\nWe will adjust the `rootmulti` `GetKVStore` method to wrap the returned `KVStore` with a `listenkv.Store` if listening is turned on for that `Store`.\n\n```go\nfunc (rs *Store) GetKVStore(key types.StoreKey) types.KVStore {\n    store := rs.stores[key].(types.KVStore)\n\n    if rs.TracingEnabled() {\n        store = tracekv.NewStore(store, rs.traceWriter, rs.traceContext)\n    }\n    if rs.ListeningEnabled(key) {\n        store = listenkv.NewStore(store, key, rs.listeners[key])\n    }\n\n    return store\n}\n```\n\nWe will implement `AddListeners` to manage KVStore listeners internally and implement `PopStateCache`\nfor a means of retrieving the current state.\n\n```go\n// AddListeners adds state change listener for a specific KVStore\nfunc (rs *Store) AddListeners(keys []types.StoreKey) {\n\tlistener := types.NewMemoryListener()\n\tfor i := range keys {\n\t\trs.listeners[keys[i]] = listener\n\t}\n}\n```\n\n```go\nfunc (rs *Store) PopStateCache() []types.StoreKVPair {\n\tvar cache []types.StoreKVPair\n\tfor _, ls := range rs.listeners {\n\t\tcache = append(cache, ls.PopStateCache()...)\n\t}\n\tsort.SliceStable(cache, func(i, j int) bool {\n\t\treturn cache[i].StoreKey < cache[j].StoreKey\n\t})\n\treturn cache\n}\n```\n\nWe will also adjust the `rootmulti` `CacheMultiStore` and `CacheMultiStoreWithVersion` methods to enable listening in\nthe cache layer.\n\n```go\nfunc (rs *Store) CacheMultiStore() types.CacheMultiStore {\n    stores := make(map[types.StoreKey]types.CacheWrapper)\n    for k, v := range rs.stores {\n        store := v.(types.KVStore)\n        // Wire the listenkv.Store to allow listeners to observe the writes from the cache store,\n        // set same listeners on cache store will observe duplicated writes.\n        if rs.ListeningEnabled(k) {\n            store = listenkv.NewStore(store, k, rs.listeners[k])\n        }\n        stores[k] = store\n    }\n    return cachemulti.NewStore(rs.db, stores, rs.keysByName, rs.traceWriter, rs.getTracingContext())\n}\n```\n\n```go\nfunc (rs *Store) CacheMultiStoreWithVersion(version int64) (types.CacheMultiStore, error) {\n // ...\n\n        // Wire the listenkv.Store to allow listeners to observe the writes from the cache store,\n        // set same listeners on cache store will observe duplicated writes.\n        if rs.ListeningEnabled(key) {\n            cacheStore = listenkv.NewStore(cacheStore, key, rs.listeners[key])\n        }\n\n        cachedStores[key] = cacheStore\n    }\n\n    return cachemulti.NewStore(rs.db, cachedStores, rs.keysByName, rs.traceWriter, rs.getTracingContext()), nil\n}\n```\n\n### Exposing the data\n\n#### Streaming Service\n\nWe will introduce a new `ABCIListener` interface that plugs into the BaseApp and relays ABCI requests and responses\nso that the service can group the state changes with the ABCI requests.\n\n```go\n// baseapp/streaming.go\n\n// ABCIListener is the interface that we're exposing as a streaming service.\ntype ABCIListener interface {\n\t// ListenFinalizeBlock updates the streaming service with the latest FinalizeBlock messages\n\tListenFinalizeBlock(ctx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error\n\t// ListenCommit updates the steaming service with the latest Commit messages and state changes\n\tListenCommit(ctx context.Context, res abci.ResponseCommit, changeSet []*StoreKVPair) error\n}\n```\n\n#### BaseApp Registration\n\nWe will add a new method to the `BaseApp` to enable the registration of `StreamingService`s:\n\n ```go\n // SetStreamingService is used to set a streaming service into the BaseApp hooks and load the listeners into the multistore\nfunc (app *BaseApp) SetStreamingService(s ABCIListener) {\n    // register the StreamingService within the BaseApp\n    // BaseApp will pass BeginBlock, DeliverTx, and EndBlock requests and responses to the streaming services to update their ABCI context\n    app.abciListeners = append(app.abciListeners, s)\n}\n```\n\nWe will add two new fields to the `BaseApp` struct:\n\n```go\ntype BaseApp struct {\n\n    ...\n\n    // abciListenersAsync for determining if abciListeners will run asynchronously.\n    // When abciListenersAsync=false and stopNodeOnABCIListenerErr=false listeners will run synchronized but will not stop the node.\n    // When abciListenersAsync=true stopNodeOnABCIListenerErr will be ignored.\n    abciListenersAsync bool\n\n    // stopNodeOnABCIListenerErr halts the node when ABCI streaming service listening results in an error.\n    // stopNodeOnABCIListenerErr=true must be paired with abciListenersAsync=false.\n    stopNodeOnABCIListenerErr bool\n}\n```\n\n#### ABCI Event Hooks\n\nWe will modify the `FinalizeBlock` and `Commit` methods to pass ABCI requests and responses\nto any streaming service hooks registered with the `BaseApp`.\n\n```go\nfunc (app *BaseApp) FinalizeBlock(req abci.RequestFinalizeBlock) abci.ResponseFinalizeBlock {\n\n    var abciRes abci.ResponseFinalizeBlock\n    defer func() {\n        // call the streaming service hook with the FinalizeBlock messages\n        for _, abciListener := range app.abciListeners {\n            ctx := app.finalizeState.ctx\n            blockHeight := ctx.BlockHeight()\n            if app.abciListenersAsync {\n                go func(req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) {\n                    if err := app.abciListener.FinalizeBlock(blockHeight, req, res); err != nil {\n                        app.logger.Error(\"FinalizeBlock listening hook failed\", \"height\", blockHeight, \"err\", err)\n                    }\n                }(req, abciRes)\n            } else {\n                if err := app.abciListener.ListenFinalizeBlock(blockHeight, req, res); err != nil {\n                    app.logger.Error(\"FinalizeBlock listening hook failed\", \"height\", blockHeight, \"err\", err)\n                    if app.stopNodeOnABCIListenerErr {\n                        os.Exit(1)\n                    }\n                }\n            }\n        }\n    }()\n\n    ...\n\n    return abciRes\n}\n```\n\n```go\nfunc (app *BaseApp) Commit() abci.ResponseCommit {\n\n    ...\n\n    res := abci.ResponseCommit{\n        Data:         commitID.Hash,\n        RetainHeight: retainHeight,\n    }\n\n    // call the streaming service hook with the Commit messages\n    for _, abciListener := range app.abciListeners {\n        ctx := app.deliverState.ctx\n        blockHeight := ctx.BlockHeight()\n        changeSet := app.cms.PopStateCache()\n        if app.abciListenersAsync {\n            go func(res abci.ResponseCommit, changeSet []store.StoreKVPair) {\n                if err := app.abciListener.ListenCommit(ctx, res, changeSet); err != nil {\n                    app.logger.Error(\"ListenCommit listening hook failed\", \"height\", blockHeight, \"err\", err)\n                }\n            }(res, changeSet)\n        } else {\n            if err := app.abciListener.ListenCommit(ctx, res, changeSet); err != nil {\n                app.logger.Error(\"ListenCommit listening hook failed\", \"height\", blockHeight, \"err\", err)\n                if app.stopNodeOnABCIListenerErr {\n                    os.Exit(1)\n                }\n            }\n        }\n    }\n\n    ...\n\n    return res\n}\n```\n\n#### Go Plugin System\n\nWe propose a plugin architecture to load and run `Streaming` plugins and other types of implementations. We will introduce a plugin\nsystem over gRPC that is used to load and run Cosmos-SDK plugins. The plugin system uses [hashicorp/go-plugin](https://github.com/hashicorp/go-plugin).\nEach plugin must have a struct that implements the `plugin.Plugin` interface and an `Impl` interface for processing messages over gRPC.\nEach plugin must also have a message protocol defined for the gRPC service:\n\n```go\n// streaming/plugins/abci/{plugin_version}/interface.go\n\n// Handshake is a common handshake that is shared by streaming and host.\n// This prevents users from executing bad plugins or executing a plugin\n// directory. It is a UX feature, not a security feature.\nvar Handshake = plugin.HandshakeConfig{\n    ProtocolVersion:  1,\n    MagicCookieKey:   \"ABCI_LISTENER_PLUGIN\",\n    MagicCookieValue: \"ef78114d-7bdf-411c-868f-347c99a78345\",\n}\n\n// ListenerPlugin is the base struct for all kinds of go-plugin implementations\n// It will be included in interfaces of different Plugins\ntype ABCIListenerPlugin struct {\n    // GRPCPlugin must still implement the Plugin interface\n    plugin.Plugin\n    // Concrete implementation, written in Go. This is only used for plugins\n    // that are written in Go.\n    Impl baseapp.ABCIListener\n}\n\nfunc (p *ListenerGRPCPlugin) GRPCServer(_ *plugin.GRPCBroker, s *grpc.Server) error {\n    RegisterABCIListenerServiceServer(s, &GRPCServer{Impl: p.Impl})\n    return nil\n}\n\nfunc (p *ListenerGRPCPlugin) GRPCClient(\n    _ context.Context,\n    _ *plugin.GRPCBroker,\n    c *grpc.ClientConn,\n) (interface{}, error) {\n    return &GRPCClient{client: NewABCIListenerServiceClient(c)}, nil\n}\n```\n\nThe `plugin.Plugin` interface has two methods `Client` and `Server`. For our GRPC service these are `GRPCClient` and `GRPCServer`\nThe `Impl` field holds the concrete implementation of our `baseapp.ABCIListener` interface written in Go.\nNote: this is only used for plugin implementations written in Go.\n\nThe advantage of having such a plugin system is that within each plugin authors can define the message protocol in a way that fits their use case.\nFor example, when state change listening is desired, the `ABCIListener` message protocol can be defined as below (*for illustrative purposes only*).\nWhen state change listening is not desired than `ListenCommit` can be omitted from the protocol.\n\n```protobuf\nsyntax = \"proto3\";\n\n...\n\nmessage Empty {}\n\nmessage ListenFinalizeBlockRequest {\n  RequestFinalizeBlock  req = 1;\n  ResponseFinalizeBlock res = 2;\n}\nmessage ListenCommitRequest {\n  int64                block_height = 1;\n  ResponseCommit       res          = 2;\n  repeated StoreKVPair changeSet    = 3;\n}\n\n// plugin that listens to state changes\nservice ABCIListenerService {\n  rpc ListenFinalizeBlock(ListenFinalizeBlockRequest) returns (Empty);\n  rpc ListenCommit(ListenCommitRequest) returns (Empty);\n}\n```\n\n```protobuf\n...\n// plugin that doesn't listen to state changes\nservice ABCIListenerService {\n  rpc ListenFinalizeBlock(ListenFinalizeBlockRequest) returns (Empty);\n  rpc ListenCommit(ListenCommitRequest) returns (Empty);\n}\n```\n\nImplementing the service above:\n\n```go\n// streaming/plugins/abci/{plugin_version}/grpc.go\n\nvar (\n    _ baseapp.ABCIListener = (*GRPCClient)(nil)\n)\n\n// GRPCClient is an implementation of the ABCIListener and ABCIListenerPlugin interfaces that talks over RPC.\ntype GRPCClient struct {\n    client ABCIListenerServiceClient\n}\n\nfunc (m *GRPCClient) ListenFinalizeBlock(goCtx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error {\n    ctx := sdk.UnwrapSDKContext(goCtx)\n    _, err := m.client.ListenDeliverTx(ctx, &ListenDeliverTxRequest{BlockHeight: ctx.BlockHeight(), Req: req, Res: res})\n    return err\n}\n\nfunc (m *GRPCClient) ListenCommit(goCtx context.Context, res abci.ResponseCommit, changeSet []store.StoreKVPair) error {\n    ctx := sdk.UnwrapSDKContext(goCtx)\n    _, err := m.client.ListenCommit(ctx, &ListenCommitRequest{BlockHeight: ctx.BlockHeight(), Res: res, ChangeSet: changeSet})\n    return err\n}\n\n// GRPCServer is the gRPC server that GRPCClient talks to.\ntype GRPCServer struct {\n    // This is the real implementation\n    Impl baseapp.ABCIListener\n}\n\nfunc (m *GRPCServer) ListenFinalizeBlock(ctx context.Context, req *ListenFinalizeBlockRequest) (*Empty, error) {\n    return &Empty{}, m.Impl.ListenFinalizeBlock(ctx, req.Req, req.Res)\n}\n\nfunc (m *GRPCServer) ListenCommit(ctx context.Context, req *ListenCommitRequest) (*Empty, error) {\n    return &Empty{}, m.Impl.ListenCommit(ctx, req.Res, req.ChangeSet)\n}\n\n```\n\nAnd the pre-compiled Go plugin `Impl`(*this is only used for plugins that are written in Go*):\n\n```go\n// streaming/plugins/abci/{plugin_version}/impl/plugin.go\n\n// Plugins are pre-compiled and loaded by the plugin system\n\n// ABCIListener is the implementation of the baseapp.ABCIListener interface\ntype ABCIListener struct{}\n\nfunc (m *ABCIListenerPlugin) ListenFinalizeBlock(ctx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error {\n    // send data to external system\n}\n\nfunc (m *ABCIListenerPlugin) ListenCommit(ctx context.Context, res abci.ResponseCommit, changeSet []store.StoreKVPair) error {\n    // send data to external system\n}\n\nfunc main() {\n    plugin.Serve(&plugin.ServeConfig{\n        HandshakeConfig: grpc_abci_v1.Handshake,\n        Plugins: map[string]plugin.Plugin{\n           \"grpc_plugin_v1\": &grpc_abci_v1.ABCIListenerGRPCPlugin{Impl: &ABCIListenerPlugin{}},\n        },\n\n        // A non-nil value here enables gRPC serving for this streaming...\n        GRPCServer: plugin.DefaultGRPCServer,\n    })\n}\n```\n\nWe will introduce a plugin loading system that will return `(interface{}, error)`.\nThis provides the advantage of using versioned plugins where the plugin interface and gRPC protocol change over time.\nIn addition, it allows for building independent plugin that can expose different parts of the system over gRPC.\n\n```go\nfunc NewStreamingPlugin(name string, logLevel string) (interface{}, error) {\n    logger := hclog.New(&hclog.LoggerOptions{\n       Output: hclog.DefaultOutput,\n       Level:  toHclogLevel(logLevel),\n       Name:   fmt.Sprintf(\"plugin.%s\", name),\n    })\n\n    // We're a host. Start by launching the streaming process.\n    env := os.Getenv(GetPluginEnvKey(name))\n    client := plugin.NewClient(&plugin.ClientConfig{\n       HandshakeConfig: HandshakeMap[name],\n       Plugins:         PluginMap,\n       Cmd:             exec.Command(\"sh\", \"-c\", env),\n       Logger:          logger,\n       AllowedProtocols: []plugin.Protocol{\n           plugin.ProtocolNetRPC, plugin.ProtocolGRPC},\n    })\n\n    // Connect via RPC\n    rpcClient, err := client.Client()\n    if err != nil {\n       return nil, err\n    }\n\n    // Request streaming plugin\n    return rpcClient.Dispense(name)\n}\n\n```\n\nWe propose a `RegisterStreamingPlugin` function for the App to register `NewStreamingPlugin`s with the App's BaseApp.\nStreaming plugins can be of `Any` type; therefore, the function takes in an interface vs a concrete type.\nFor example, we could have plugins of `ABCIListener`, `WasmListener` or `IBCListener`. Note that `RegisterStreamingPluing` function\nis helper function and not a requirement. Plugin registration can easily be moved from the App to the BaseApp directly.\n\n```go\n// baseapp/streaming.go\n\n// RegisterStreamingPlugin registers streaming plugins with the App.\n// This method returns an error if a plugin is not supported.\nfunc RegisterStreamingPlugin(\n    bApp *BaseApp,\n    appOpts servertypes.AppOptions,\n    keys map[string]*types.KVStoreKey,\n    streamingPlugin interface{},\n) error {\n    switch t := streamingPlugin.(type) {\n    case ABCIListener:\n        registerABCIListenerPlugin(bApp, appOpts, keys, t)\n    default:\n        return fmt.Errorf(\"unexpected plugin type %T\", t)\n    }\n    return nil\n}\n```\n\n```go\nfunc registerABCIListenerPlugin(\n    bApp *BaseApp,\n    appOpts servertypes.AppOptions,\n    keys map[string]*store.KVStoreKey,\n    abciListener ABCIListener,\n) {\n    asyncKey := fmt.Sprintf(\"%s.%s.%s\", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIAsync)\n    async := cast.ToBool(appOpts.Get(asyncKey))\n    stopNodeOnErrKey := fmt.Sprintf(\"%s.%s.%s\", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIStopNodeOnErrTomlKey)\n    stopNodeOnErr := cast.ToBool(appOpts.Get(stopNodeOnErrKey))\n    keysKey := fmt.Sprintf(\"%s.%s.%s\", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIKeysTomlKey)\n    exposeKeysStr := cast.ToStringSlice(appOpts.Get(keysKey))\n    exposedKeys := exposeStoreKeysSorted(exposeKeysStr, keys)\n    bApp.cms.AddListeners(exposedKeys)\n    app.SetStreamingManager(\n\t\tstoretypes.StreamingManager{\n\t\t\tABCIListeners: []storetypes.ABCIListener{abciListener},\n\t\t\tStopNodeOnErr: stopNodeOnErr,\n\t\t},\n\t)\n}\n```\n\n```go\nfunc exposeAll(list []string) bool {\n    for _, ele := range list {\n        if ele == \"*\" {\n            return true\n        }\n    }\n    return false\n}\n\nfunc exposeStoreKeys(keysStr []string, keys map[string]*types.KVStoreKey) []types.StoreKey {\n    var exposeStoreKeys []types.StoreKey\n    if exposeAll(keysStr) {\n        exposeStoreKeys = make([]types.StoreKey, 0, len(keys))\n        for _, storeKey := range keys {\n            exposeStoreKeys = append(exposeStoreKeys, storeKey)\n        }\n    } else {\n        exposeStoreKeys = make([]types.StoreKey, 0, len(keysStr))\n        for _, keyStr := range keysStr {\n            if storeKey, ok := keys[keyStr]; ok {\n                exposeStoreKeys = append(exposeStoreKeys, storeKey)\n            }\n        }\n    }\n    // sort storeKeys for deterministic output\n    sort.SliceStable(exposeStoreKeys, func(i, j int) bool {\n        return exposeStoreKeys[i].Name() < exposeStoreKeys[j].Name()\n    })\n\n    return exposeStoreKeys\n}\n```\n\nThe `NewStreamingPlugin` and `RegisterStreamingPlugin` functions are used to register a plugin with the App's BaseApp.\n\ne.g. in `NewSimApp`:\n\n```go\nfunc NewSimApp(\n    logger log.Logger,\n    db dbm.DB,\n    traceStore io.Writer,\n    loadLatest bool,\n    appOpts servertypes.AppOptions,\n    baseAppOptions ...func(*baseapp.BaseApp),\n) *SimApp {\n\n    ...\n\n    keys := sdk.NewKVStoreKeys(\n       authtypes.StoreKey, banktypes.StoreKey, stakingtypes.StoreKey,\n       minttypes.StoreKey, distrtypes.StoreKey, slashingtypes.StoreKey,\n       govtypes.StoreKey, paramstypes.StoreKey, ibchost.StoreKey, upgradetypes.StoreKey,\n       evidencetypes.StoreKey, ibctransfertypes.StoreKey, capabilitytypes.StoreKey,\n    )\n\n    ...\n\n    // register streaming services\n    streamingCfg := cast.ToStringMap(appOpts.Get(baseapp.StreamingTomlKey))\n    for service := range streamingCfg {\n        pluginKey := fmt.Sprintf(\"%s.%s.%s\", baseapp.StreamingTomlKey, service, baseapp.StreamingPluginTomlKey)\n        pluginName := strings.TrimSpace(cast.ToString(appOpts.Get(pluginKey)))\n        if len(pluginName) > 0 {\n            logLevel := cast.ToString(appOpts.Get(flags.FlagLogLevel))\n            plugin, err := streaming.NewStreamingPlugin(pluginName, logLevel)\n            if err != nil {\n                tmos.Exit(err.Error())\n            }\n            if err := baseapp.RegisterStreamingPlugin(bApp, appOpts, keys, plugin); err != nil {\n                tmos.Exit(err.Error())\n            }\n        }\n    }\n\n    return app\n```\n\n#### Configuration\n\nThe plugin system will be configured within an App's TOML configuration files.\n\n```toml\n# gRPC streaming\n[streaming]\n\n# ABCI streaming service\n[streaming.abci]\n\n# The plugin version to use for ABCI listening\nplugin = \"abci_v1\"\n\n# List of kv store keys to listen to for state changes.\n# Set to [\"*\"] to expose all keys.\nkeys = [\"*\"]\n\n# Enable abciListeners to run asynchronously.\n# When abciListenersAsync=false and stopNodeOnABCIListenerErr=false listeners will run synchronized but will not stop the node.\n# When abciListenersAsync=true stopNodeOnABCIListenerErr will be ignored.\nasync = false\n\n# Whether to stop the node on message deliver error.\nstop-node-on-err = true\n```\n\nThere will be four parameters for configuring `ABCIListener` plugin: `streaming.abci.plugin`, `streaming.abci.keys`, `streaming.abci.async` and `streaming.abci.stop-node-on-err`.\n`streaming.abci.plugin` is the name of the plugin we want to use for streaming, `streaming.abci.keys` is a set of store keys for stores it listens to,\n`streaming.abci.async` is bool enabling asynchronous listening and `streaming.abci.stop-node-on-err` is a bool that stops the node when true and when operating\non synchronized mode `streaming.abci.async=false`. Note that `streaming.abci.stop-node-on-err=true` will be ignored if `streaming.abci.async=true`.\n\nThe configuration above support additional streaming plugins by adding the plugin to the `[streaming]` configuration section\nand registering the plugin with `RegisterStreamingPlugin` helper function.\n\nNote the that each plugin must include `streaming.{service}.plugin` property as it is a requirement for doing the lookup and registration of the plugin\nwith the App. All other properties are unique to the individual services.\n\n#### Encoding and decoding streams\n\nADR-038 introduces the interfaces and types for streaming state changes out from KVStores, associating this\ndata with their related ABCI requests and responses, and registering a service for consuming this data and streaming it to some destination in a final format.\nInstead of prescribing a final data format in this ADR, it is left to a specific plugin implementation to define and document this format.\nWe take this approach because flexibility in the final format is necessary to support a wide range of streaming service plugins. For example,\nthe data format for a streaming service that writes the data out to a set of files will differ from the data format that is written to a Kafka topic.\n\n## Consequences\n\nThese changes will provide a means of subscribing to KVStore state changes in real time.\n\n### Backwards Compatibility\n\n* This ADR changes the `CommitMultiStore` interface, implementations supporting the previous version of this interface will not support the new one\n\n### Positive\n\n* Ability to listen to KVStore state changes in real time and expose these events to external consumers\n\n### Negative\n\n* Changes `CommitMultiStore` interface and its implementations\n\n### Neutral\n\n* Introduces additional- but optional- complexity to configuring and running a cosmos application\n* If an application developer opts to use these features to expose data, they need to be aware of the ramifications/risks of that data exposure as it pertains to the specifics of their application"
  },
  {
    "number": 39,
    "filename": "adr-039-epoched-staking.md",
    "title": "ADR 039: Epoched Staking",
    "content": "# ADR 039: Epoched Staking\n\n## Changelog\n\n* 10-Feb-2021: Initial Draft\n\n## Authors\n\n* Dev Ojha (@valardragon)\n* Sunny Aggarwal (@sunnya97)\n\n## Status\n\nProposed\n\n## Abstract\n\nThis ADR updates the proof of stake module to buffer the staking weight updates for a number of blocks before updating the consensus' staking weights. The length of the buffer is dubbed an epoch. The prior functionality of the staking module is then a special case of the abstracted module, with the epoch being set to 1 block.\n\n## Context\n\nThe current proof of stake module takes the design decision to apply staking weight changes to the consensus engine immediately. This means that delegations and unbonds get applied immediately to the validator set. This decision was primarily done as it was the simplest from an implementation perspective, and because we at the time believed that this would lead to better UX for clients.\n\nAn alternative design choice is to allow buffering staking updates (delegations, unbonds, validators joining) for a number of blocks. This epoched proof of stake consensus provides the guarantee that the consensus weights for validators will not change mid-epoch, except in the event of a slash condition.\n\nAdditionally, the UX hurdle may not be as significant as was previously thought. This is because it is possible to provide users immediate acknowledgement that their bond was recorded and will be executed.\n\nFurthermore, it has become clearer over time that immediate execution of staking events comes with limitations, such as:\n\n* Threshold based cryptography. One of the main limitations is that because the validator set can change so regularly, it makes the running of multiparty computation by a fixed validator set difficult. Many threshold-based cryptographic features for blockchains such as randomness beacons and threshold decryption require a computationally-expensive DKG process (will take much longer than 1 block to create). To productively use these, we need to guarantee that the result of the DKG will be used for a reasonably long time. It wouldn't be feasible to rerun the DKG every block. By epoching staking, it guarantees we'll only need to run a new DKG once every epoch.\n\n* Light client efficiency. This would lessen the overhead for IBC when there is high churn in the validator set. In the Tendermint light client bisection algorithm, the number of headers you need to verify is related to bounding the difference in validator sets between a trusted header and the latest header. If the difference is too great, you verify more headers in between the two. By limiting the frequency of validator set changes, we can reduce the worst case size of IBC lite client proofs, which occurs when a validator set has high churn.\n\n* Fairness of deterministic leader election. Currently we have no ways of reasoning about fairness of deterministic leader election in the presence of staking changes without epochs (tendermint/spec#217). Breaking fairness of leader election is profitable for validators, as they earn additional rewards from being the proposer. Adding epochs at least makes it easier for our deterministic leader election to match something we can prove secure. (Albeit, we still haven’t proven if our current algorithm is fair with > 2 validators in the presence of stake changes)\n\n* Staking derivative design. Currently, reward distribution is done lazily using the F1 fee distribution. While saving computational complexity, lazy accounting requires a more stateful staking implementation. Right now, each delegation entry has to track the time of last withdrawal. Handling this can be a challenge for some staking derivatives designs that seek to provide fungibility for all tokens staked to a single validator. Force-withdrawing rewards to users can help solve this, however it is infeasible to force-withdraw rewards to users on a per block basis. With epochs, a chain could more easily alter the design to have rewards be forcefully withdrawn (iterating over delegator accounts only once per-epoch), and can thus remove delegation timing from state. This may be useful for certain staking derivative designs.\n\n## Design considerations\n\n### Slashing\n\nThere is a design consideration for whether to apply a slash immediately or at the end of an epoch. A slash event should apply to only members who are actually staked during the time of the infraction, namely during the epoch the slash event occurred.\n\nApplying it immediately can be viewed as offering greater consensus layer security, at potential costs to the aforementioned use cases. The benefits of immediate slashing for consensus layer security can be all be obtained by executing the validator jailing immediately (thus removing it from the validator set), and delaying the actual slash change to the validator's weight until the epoch boundary. For the use cases mentioned above, workarounds can be integrated to avoid problems, as follows:\n\n* For threshold based cryptography, this setting will have the threshold cryptography use the original epoch weights, while consensus has an update that lets it more rapidly benefit from additional security. If the threshold based cryptography blocks liveness of the chain, then we have effectively raised the liveness threshold of the remaining validators for the rest of the epoch. (Alternatively, jailed nodes could still contribute shares) This plan will fail in the extreme case that more than 1/3rd of the validators have been jailed within a single epoch. For such an extreme scenario, the chain already have its own custom incident response plan, and defining how to handle the threshold cryptography should be a part of that.\n* For light client efficiency, there can be a bit included in the header indicating an intra-epoch slash (ala https://github.com/tendermint/spec/issues/199).\n* For fairness of deterministic leader election, applying a slash or jailing within an epoch would break the guarantee we were seeking to provide. This then re-introduces a new (but significantly simpler) problem for trying to provide fairness guarantees. Namely, that validators can adversarially elect to remove themselves from the set of proposers. From a security perspective, this could potentially be handled by two different mechanisms (or prove to still be too difficult to achieve). One is making a security statement acknowledging the ability for an adversary to force an ahead-of-time fixed threshold of users to drop out of the proposer set within an epoch. The second method would be to parameterize such that the cost of a slash within the epoch far outweighs benefits due to being a proposer. However, this latter criterion is quite dubious, since being a proposer can have many advantageous side-effects in chains with complex state machines. (Namely, DeFi games such as Fomo3D)\n* For staking derivative design, there is no issue introduced. This does not increase the state size of staking records, since whether a slash has occurred is fully queryable given the validator address.\n\n### Token lockup\n\nWhen someone makes a transaction to delegate, even though they are not immediately staked, their tokens should be moved into a pool managed by the staking module which will then be used at the end of an epoch. This prevents concerns where they stake, and then spend those tokens not realizing they were already allocated for staking, and thus having their staking tx fail.\n\n### Pipelining the epochs\n\nFor threshold based cryptography in particular, we need a pipeline for epoch changes. This is because when we are in epoch N, we want the epoch N+1 weights to be fixed so that the validator set can do the DKG accordingly. So if we are currently in epoch N, the stake weights for epoch N+1 should already be fixed, and new stake changes should be getting applied to epoch N + 2.\n\nThis can be handled by making a parameter for the epoch pipeline length. This parameter should not be alterable except during hard forks, to mitigate implementation complexity of switching the pipeline length.\n\nWith pipeline length 1, if I redelegate during epoch N, then my redelegation is applied prior to the beginning of epoch N+1.\nWith pipeline length 2, if I redelegate during epoch N, then my redelegation is applied prior to the beginning of epoch N+2.\n\n### Rewards\n\nEven though all staking updates are applied at epoch boundaries, rewards can still be distributed immediately when they are claimed. This is because they do not affect the current stake weights, as we do not implement auto-bonding of rewards. If such a feature were to be implemented, it would have to be setup so that rewards are auto-bonded at the epoch boundary.\n\n### Parameterizing the epoch length\n\nWhen choosing the epoch length, there is a trade-off between queued state/computation buildup, and countering the previously discussed limitations of immediate execution if they apply to a given chain.\n\nUntil an ABCI mechanism for variable block times is introduced, it is ill-advised to be using high epoch lengths due to the computation buildup. This is because when a block's execution time is greater than the expected block time from Tendermint, rounds may increment.\n\n## Decision\n\n**Step-1**:  Implement buffering of all staking and slashing messages.\n\nFirst we create a pool for storing tokens that are being bonded, but should be applied at the epoch boundary called the `EpochDelegationPool`. Then, we have two separate queues, one for staking, one for slashing. We describe what happens on each message being delivered below:\n\n### Staking messages\n\n* **MsgCreateValidator**: Move user's self-bond to `EpochDelegationPool` immediately. Queue a message for the epoch boundary to handle the self-bond, taking the funds from the `EpochDelegationPool`. If Epoch execution fails, return back funds from `EpochDelegationPool` to user's account.\n* **MsgEditValidator**: Validate message and if valid queue the message for execution at the end of the Epoch.\n* **MsgDelegate**: Move user's funds to `EpochDelegationPool` immediately. Queue a message for the epoch boundary to handle the delegation, taking the funds from the `EpochDelegationPool`. If Epoch execution fails, return back funds from `EpochDelegationPool` to user's account.\n* **MsgBeginRedelegate**: Validate message and if valid queue the message for execution at the end of the Epoch.\n* **MsgUndelegate**: Validate message and if valid queue the message for execution at the end of the Epoch.\n\n### Slashing messages\n\n* **MsgUnjail**: Validate message and if valid queue the message for execution at the end of the Epoch.\n* **Slash Event**: Whenever a slash event is created, it gets queued in the slashing module to apply at the end of the epoch. The queues should be set up such that this slash applies immediately.\n\n### Evidence Messages\n\n* **MsgSubmitEvidence**: This gets executed immediately, and the validator gets jailed immediately. However in slashing, the actual slash event gets queued.\n\nThen we add methods to the end blockers, to ensure that at the epoch boundary the queues are cleared and delegation updates are applied.\n\n**Step-2**: Implement querying of queued staking txs.\n\nWhen querying the staking activity of a given address, the status should return not only the amount of tokens staked, but also if there are any queued stake events for that address. This will require more work to be done in the querying logic, to trace the queued upcoming staking events.\n\nAs an initial implementation, this can be implemented as a linear search over all queued staking events. However, for chains that need long epochs, they should eventually build additional support for nodes that support querying to be able to produce results in constant time. (This is doable by maintaining an auxiliary hashmap for indexing upcoming staking events by address)\n\n**Step-3**: Adjust gas\n\nCurrently gas represents the cost of executing a transaction when its done immediately. (Merging together costs of p2p overhead, state access overhead, and computational overhead) However, now a transaction can cause computation in a future block, namely at the epoch boundary.\n\nTo handle this, we should initially include parameters for estimating the amount of future computation (denominated in gas), and add that as a flat charge needed for the message.\nWe leave it out of scope for how to weight future computation versus current computation in gas pricing, and have it set such that they are weighted equally for now.\n\n## Consequences\n\n### Positive\n\n* Abstracts the proof of stake module that allows retaining the existing functionality\n* Enables new features such as validator-set based threshold cryptography\n\n### Negative\n\n* Increases complexity of integrating more complex gas pricing mechanisms, as they now have to consider future execution costs as well.\n* When epoch > 1, validators can no longer leave the network immediately, and must wait until an epoch boundary."
  },
  {
    "number": 40,
    "filename": "adr-040-storage-and-smt-state-commitments.md",
    "title": "ADR 040: Storage and SMT State Commitments",
    "content": "# ADR 040: Storage and SMT State Commitments\n\n## Changelog\n\n* 2020-01-15: Draft\n\n## Status\n\nDRAFT Not Implemented\n\n## Abstract\n\nSparse Merkle Tree ([SMT](https://osf.io/8mcnh/)) is a version of a Merkle Tree with various storage and performance optimizations. This ADR defines a separation of state commitments from data storage and the Cosmos SDK transition from IAVL to SMT.\n\n## Context\n\nCurrently, Cosmos SDK uses IAVL for both state [commitments](https://cryptography.fandom.com/wiki/Commitment_scheme) and data storage.\n\nIAVL has effectively become an orphaned project within the Cosmos ecosystem and it's proven to be an inefficient state commitment data structure.\nIn the current design, IAVL is used for both data storage and as a Merkle Tree for state commitments. IAVL is meant to be a standalone Merkleized key/value database, however it's using a KV DB engine to store all tree nodes. So, each node is stored in a separate record in the KV DB. This causes many inefficiencies and problems:\n\n* Each object query requires a tree traversal from the root. Subsequent queries for the same object are cached on the Cosmos SDK level.\n* Each edge traversal requires a DB query.\n* Creating snapshots is [expensive](https://github.com/cosmos/cosmos-sdk/issues/7215#issuecomment-684804950). It takes about 30 seconds to export less than 100 MB of state (as of March 2020).\n* Updates in IAVL may trigger tree reorganization and possible O(log(n)) hashes re-computation, which can become a CPU bottleneck.\n* The node structure is pretty expensive - it contains a standard tree node elements (key, value, left and right element) and additional metadata such as height, version (which is not required by the Cosmos SDK). The entire node is hashed, and that hash is used as the key in the underlying database, [ref](https://github.com/cosmos/iavl/blob/master/docs/node/node.md\n).\n\nMoreover, the IAVL project lacks support and a maintainer and we already see better and well-established alternatives. Instead of optimizing the IAVL, we are looking into other solutions for both storage and state commitments.\n\n## Decision\n\nWe propose to separate the concerns of state commitment (**SC**), needed for consensus, and state storage (**SS**), needed for state machine. Finally we replace IAVL with [Celestia's SMT](https://github.com/lazyledger/smt). Celestia SMT is based on Diem (called jellyfish) design [*] - it uses a compute-optimized SMT by replacing subtrees with only default values with a single node (same approach is used by Ethereum2) and implements compact proofs.\n\nThe storage model presented here doesn't deal with data structure nor serialization. It's a Key-Value database, where both key and value are binaries. The storage user is responsible for data serialization.\n\n### Decouple state commitment from storage\n\nSeparation of storage and commitment (by the SMT) will allow the optimization of different components according to their usage and access patterns.\n\n`SC` (SMT) is used to commit to a data and compute Merkle proofs. `SS` is used to directly access data. To avoid collisions, both `SS` and `SC` will use a separate storage namespace (they could use the same database underneath). `SS` will store each record directly (mapping `(key, value)` as `key → value`).\n\nSMT is a merkle tree structure: we don't store keys directly. For every `(key, value)` pair, `hash(key)` is used as leaf path (we hash a key to uniformly distribute leaves in the tree) and `hash(value)` as the leaf contents. The tree structure is specified in more depth [below](#smt-for-state-commitment).\n\nFor data access we propose 2 additional KV buckets (implemented as namespaces for the key-value pairs, sometimes called [column family](https://github.com/facebook/rocksdb/wiki/Terminology)):\n\n1. B1: `key → value`: the principal object storage, used by a state machine, behind the Cosmos SDK `KVStore` interface: provides direct access by key and allows prefix iteration (KV DB backend must support it).\n2. B2: `hash(key) → key`: a reverse index to get a key from an SMT path. Internally the SMT will store `(key, value)` as `prefix || hash(key) || hash(value)`. So, we can get an object value by composing `hash(key) → B2 → B1`.\n3. We could use more buckets to optimize the app usage if needed.\n\nWe propose to use a KV database for both `SS` and `SC`. The store interface will allow to use the same physical DB backend for both `SS` and `SC` as well two separate DBs. The latter option allows for the separation of `SS` and `SC` into different hardware units, providing support for more complex setup scenarios and improving overall performance: one can use different backends (eg RocksDB and Badger) as well as independently tuning the underlying DB configuration.\n\n### Requirements\n\nState Storage requirements:\n\n* range queries\n* quick (key, value) access\n* creating a snapshot\n* historical versioning\n* pruning (garbage collection)\n\nState Commitment requirements:\n\n* fast updates\n* tree path should be short\n* query historical commitment proofs using ICS-23 standard\n* pruning (garbage collection)\n\n### SMT for State Commitment\n\nA Sparse Merkle tree is based on the idea of a complete Merkle tree of an intractable size. The assumption here is that as the size of the tree is intractable, there would only be a few leaf nodes with valid data blocks relative to the tree size, rendering a sparse tree.\n\nThe full specification can be found at [Celestia](https://github.com/celestiaorg/celestia-specs/blob/ec98170398dfc6394423ee79b00b71038879e211/src/specs/data_structures.md#sparse-merkle-tree). In summary:\n\n* The SMT consists of a binary Merkle tree, constructed in the same fashion as described in [Certificate Transparency (RFC-6962)](https://tools.ietf.org/html/rfc6962), but using as the hashing function SHA-2-256 as defined in [FIPS 180-4](https://doi.org/10.6028/NIST.FIPS.180-4).\n* Leaves and internal nodes are hashed differently: the one-byte `0x00` is prepended for leaf nodes while `0x01` is prepended for internal nodes.\n* Default values are given to leaf nodes with empty leaves.\n* While the above rule is sufficient to pre-compute the values of intermediate nodes that are roots of empty subtrees, a further simplification is to extend this default value to all nodes that are roots of empty subtrees. The 32-byte zero is used as the default value. This rule takes precedence over the above one.\n* An internal node that is the root of a subtree that contains exactly one non-empty leaf is replaced by that leaf's leaf node.\n\n### Snapshots for storage sync and state versioning\n\nBelow, with simple _snapshot_ we refer to a database snapshot mechanism, not to a _ABCI snapshot sync_. The latter will be referred as _snapshot sync_ (which will directly use DB snapshot as described below).\n\nDatabase snapshot is a view of DB state at a certain time or transaction. It's not a full copy of a database (it would be too big). Usually a snapshot mechanism is based on a _copy on write_ and it allows DB state to be efficiently delivered at a certain stage.\nSome DB engines support snapshotting. Hence, we propose to reuse that functionality for the state sync and versioning (described below). We limit the supported DB engines to ones which efficiently implement snapshots. In a final section we discuss the evaluated DBs.\n\nOne of the Stargate core features is a _snapshot sync_ delivered in the `/snapshot` package. It provides a way to trustlessly sync a blockchain without repeating all transactions from the genesis. This feature is implemented in Cosmos SDK and requires storage support. Currently IAVL is the only supported backend. It works by streaming to a client a snapshot of a `SS` at a certain version together with a header chain.\n\nA new database snapshot will be created in every `EndBlocker` and identified by a block height. The `root` store keeps track of the available snapshots to offer `SS` at a certain version. The `root` store implements the `RootStore` interface described below. In essence, `RootStore` encapsulates a `Committer` interface. `Committer` has a `Commit`, `SetPruning`, `GetPruning` functions which will be used for creating and removing snapshots. The `rootStore.Commit` function creates a new snapshot and increments the version on each call, and checks if it needs to remove old versions. We will need to update the SMT interface to implement the `Committer` interface.\nNOTE: `Commit` must be called exactly once per block. Otherwise we risk going out of sync for the version number and block height.\nNOTE: For the Cosmos SDK storage, we may consider splitting that interface into `Committer` and `PruningCommitter` - only the multiroot should implement `PruningCommitter` (cache and prefix store don't need pruning).\n\nNumber of historical versions for `abci.RequestQuery` and state sync snapshots is part of a node configuration, not a chain configuration (configuration implied by the blockchain consensus). A configuration should allow to specify number of past blocks and number of past blocks modulo some number (eg: 100 past blocks and one snapshot every 100 blocks for past 2000 blocks). Archival nodes can keep all past versions.\n\nPruning old snapshots is effectively done by a database. Whenever we update a record in `SC`, SMT won't update nodes - instead it creates new nodes on the update path, without removing the old one. Since we are snapshotting each block, we need to change that mechanism to immediately remove orphaned nodes from the database. This is a safe operation - snapshots will keep track of the records and make it available when accessing past versions.\n\nTo manage the active snapshots we will either use a DB _max number of snapshots_ option (if available), or we will remove DB snapshots in the `EndBlocker`. The latter option can be done efficiently by identifying snapshots with block height and calling a store function to remove past versions.\n\n#### Accessing old state versions\n\nOne of the functional requirements is to access old state. This is done through `abci.RequestQuery` structure.  The version is specified by a block height (so we query for an object by a key `K` at block height `H`). The number of old versions supported for `abci.RequestQuery` is configurable. Accessing an old state is done by using available snapshots.\n`abci.RequestQuery` doesn't need old state of `SC` unless the `prove=true` parameter is set. The SMT merkle proof must be included in the `abci.ResponseQuery` only if both `SC` and `SS` have a snapshot for requested version.\n\nMoreover, Cosmos SDK could provide a way to directly access a historical state. However, a state machine shouldn't do that - since the number of snapshots is configurable, it would lead to nondeterministic execution.\n\nWe positively [validated](https://github.com/cosmos/cosmos-sdk/discussions/8297) a versioning and snapshot mechanism for querying old state with regards to the database we evaluated.\n\n### State Proofs\n\nFor any object stored in State Store (SS), we have corresponding object in `SC`. A proof for object `V` identified by a key `K` is a branch of `SC`, where the path corresponds to the key `hash(K)`, and the leaf is `hash(K, V)`.\n\n### Rollbacks\n\nWe need to be able to process transactions and roll-back state updates if a transaction fails. This can be done in the following way: during transaction processing, we keep all state change requests (writes) in a `CacheWrapper` abstraction (as it's done today). Once we finish the block processing, in the `Endblocker`,  we commit a root store - at that time, all changes are written to the SMT and to the `SS` and a snapshot is created.\n\n### Committing to an object without saving it\n\nWe identified use-cases, where modules will need to save an object commitment without storing an object itself. Sometimes clients are receiving complex objects, and they have no way to prove a correctness of that object without knowing the storage layout. For those use cases it would be easier to commit to the object without storing it directly.\n\n### Refactor MultiStore\n\nThe Stargate `/store` implementation (store/v1) adds an additional layer in the SDK store construction - the `MultiStore` structure. The multistore exists to support the modularity of the Cosmos SDK - each module is using its own instance of IAVL, but in the current implementation, all instances share the same database. The latter indicates, however, that the implementation doesn't provide true modularity. Instead it causes problems related to race condition and atomic DB commits (see: [\\#6370](https://github.com/cosmos/cosmos-sdk/issues/6370) and [discussion](https://github.com/cosmos/cosmos-sdk/discussions/8297#discussioncomment-757043)).\n\nWe propose to reduce the multistore concept from the SDK, and to use a single instance of `SC` and `SS` in a `RootStore` object. To avoid confusion, we should rename the `MultiStore` interface to `RootStore`. The `RootStore` will have the following interface; the methods for configuring tracing and listeners are omitted for brevity.\n\n```go\n// Used where read-only access to versions is needed.\ntype BasicRootStore interface {\n    Store\n    GetKVStore(StoreKey) KVStore\n    CacheRootStore() CacheRootStore\n}\n\n// Used as the main app state, replacing CommitMultiStore.\ntype CommitRootStore interface {\n    BasicRootStore\n    Committer\n    Snapshotter\n\n    GetVersion(uint64) (BasicRootStore, error)\n    SetInitialVersion(uint64) error\n\n    ... // Trace and Listen methods\n}\n\n// Replaces CacheMultiStore for branched state.\ntype CacheRootStore interface {\n    BasicRootStore\n    Write()\n\n    ... // Trace and Listen methods\n}\n\n// Example of constructor parameters for the concrete type.\ntype RootStoreConfig struct {\n    Upgrades        *StoreUpgrades\n    InitialVersion  uint64\n\n    ReservePrefix(StoreKey, StoreType)\n}\n```\n\n<!-- TODO: Review whether these types can be further reduced or simplified -->\n<!-- TODO: RootStorePersistentCache type -->\n\nIn contrast to `MultiStore`, `RootStore` doesn't allow to dynamically mount sub-stores or provide an arbitrary backing DB for individual sub-stores.\n\nNOTE: modules will be able to use a special commitment and their own DBs. For example: a module which will use ZK proofs for state can store and commit this proof in the `RootStore` (usually as a single record) and manage the specialized store privately or using the `SC` low level interface.\n\n#### Compatibility support\n\nTo ease the transition to this new interface for users, we can create a shim which wraps a `CommitMultiStore` but provides a `CommitRootStore` interface, and expose functions to safely create and access the underlying `CommitMultiStore`.\n\nThe new `RootStore` and supporting types can be implemented in a `store/v2alpha1` package to avoid breaking existing code.\n\n#### Merkle Proofs and IBC\n\nCurrently, an IBC (v1.0) Merkle proof path consists of two elements (`[\"<store-key>\", \"<record-key>\"]`), with each key corresponding to a separate proof. These are each verified according to individual [ICS-23 specs](https://github.com/cosmos/ibc-go/blob/f7051429e1cf833a6f65d51e6c3df1609290a549/modules/core/23-commitment/types/merkle.go#L17), and the result hash of each step is used as the committed value of the next step, until a root commitment hash is obtained.\nThe root hash of the proof for `\"<record-key>\"` is hashed with the `\"<store-key>\"` to validate against the App Hash.\n\nThis is not compatible with the `RootStore`, which stores all records in a single Merkle tree structure, and won't produce separate proofs for the store- and record-key. Ideally, the store-key component of the proof could just be omitted, and updated to use a \"no-op\" spec, so only the record-key is used. However, because the IBC verification code hardcodes the `\"ibc\"` prefix and applies it to the SDK proof as a separate element of the proof path, this isn't possible without a breaking change. Breaking this behavior would severely impact the Cosmos ecosystem which already widely adopts the IBC module. Requesting an update of the IBC module across the chains is a time consuming effort and not easily feasible.\n\nAs a workaround, the `RootStore` will have to use two separate SMTs (they could use the same underlying DB): one for IBC state and one for everything else. A simple Merkle map that reference these SMTs will act as a Merkle Tree to create a final App hash. The Merkle map is not stored in a DBs - it's constructed in the runtime. The IBC substore key must be `\"ibc\"`.\n\nThe workaround can still guarantee atomic syncs: the [proposed DB backends](#evaluated-kv-databases) support atomic transactions and efficient rollbacks, which will be used in the commit phase.\n\nThe presented workaround can be used until the IBC module is fully upgraded to supports single-element commitment proofs.\n\n### Optimization: compress module key prefixes\n\nWe consider a compression of prefix keys by creating a mapping from module key to an integer, and serializing the integer using varint coding. Varint coding assures that different values don't have common byte prefix. For Merkle Proofs we can't use prefix compression - so it should only apply for the `SS` keys. Moreover, the prefix compression should be only applied for the module namespace. More precisely:\n\n* each module has it's own namespace;\n* when accessing a module namespace we create a KVStore with embedded prefix;\n* that prefix will be compressed only when accessing and managing `SS`.\n\nWe need to assure that the codes won't change. We can fix the mapping in a static variable (provided by an app) or SS state under a special key.\n\nTODO: need to make decision about the key compression.\n\n## Optimization: SS key compression\n\nSome objects may be saved with key, which contains a Protobuf message type. Such keys are long. We could save a lot of space if we can map Protobuf message types in varints.\n\nTODO: finalize this or move to another ADR.\n\n## Migration\n\nUsing the new store will require a migration. 2 Migrations are proposed:\n\n1. Genesis export -- it will reset the blockchain history.\n2. In place migration: we can reuse `UpgradeKeeper.SetUpgradeHandler` to provide the migration logic:\n\n```go \napp.UpgradeKeeper.SetUpgradeHandler(\"adr-40\", func(ctx sdk.Context, plan upgradetypes.Plan, vm module.VersionMap) (module.VersionMap, error) {\n\n    storev2.Migrate(iavlstore, v2.store)\n\n    // RunMigrations returns the VersionMap\n    // with the updated module ConsensusVersions\n    return app.mm.RunMigrations(ctx, vm)\n})\n```\n\nThe `Migrate` function will read all entries from a store/v1 DB and save them to the AD-40 combined KV store. \nCache layer should not be used and the operation must finish with a single Commit call.\n\nInserting records to the `SC` (SMT) component is the bottleneck. Unfortunately SMT doesn't support batch transactions. \nAdding batch transactions to `SC` layer is considered as a feature after the main release.\n\n## Consequences\n\n### Backwards Compatibility\n\nThis ADR doesn't introduce any Cosmos SDK level API changes.\n\nWe change the storage layout of the state machine, a storage hard fork and network upgrade is required to incorporate these changes. SMT provides a merkle proof functionality, however it is not compatible with ICS23. Updating the proofs for ICS23 compatibility is required.\n\n### Positive\n\n* Decoupling state from state commitment introduce better engineering opportunities for further optimizations and better storage patterns.\n* Performance improvements.\n* Joining SMT based camp which has wider and proven adoption than IAVL. Example projects which decided on SMT: Ethereum2, Diem (Libra), Trillan, Tezos, Celestia.\n* Multistore removal fixes a longstanding issue with the current MultiStore design.\n* Simplifies merkle proofs - all modules, except IBC, have only one pass for merkle proof.\n\n### Negative\n\n* Storage migration\n* LL SMT doesn't support pruning - we will need to add and test that functionality.\n* `SS` keys will have an overhead of a key prefix. This doesn't impact `SC` because all keys in `SC` have same size (they are hashed).\n\n### Neutral\n\n* Deprecating IAVL, which is one of the core proposals of Cosmos Whitepaper.\n\n## Alternative designs\n\nMost of the alternative designs were evaluated in [state commitments and storage report](https://paper.dropbox.com/published/State-commitments-and-storage-review--BDvA1MLwRtOx55KRihJ5xxLbBw-KeEB7eOd11pNrZvVtqUgL3h).\n\nEthereum research published [Verkle Trie](https://dankradfeist.de/ethereum/2021/06/18/verkle-trie-for-eth1.html) - an idea of combining polynomial commitments with merkle tree in order to reduce the tree height. This concept has a very good potential, but we think it's too early to implement it. The current, SMT based design could be easily updated to the Verkle Trie once other research implement all necessary libraries. The main advantage of the design described in this ADR is the separation of state commitments from the data storage and designing a more powerful interface.\n\n## Further Discussions\n\n### Evaluated KV Databases\n\nWe verified existing databases KV databases for evaluating snapshot support. The following databases provide efficient snapshot mechanism: Badger, RocksDB, [Pebble](https://github.com/cockroachdb/pebble). Databases which don't provide such support or are not production ready: boltdb, leveldb, goleveldb, membdb, lmdb.\n\n### RDBMS\n\nUse of RDBMS instead of simple KV store for state. Use of RDBMS will require a Cosmos SDK API breaking change (`KVStore` interface) and will allow better data extraction and indexing solutions. Instead of saving an object as a single blob of bytes, we could save it as record in a table in the state storage layer, and as a `hash(key, protobuf(object))` in the SMT as outlined above. To verify that an object registered in RDBMS is same as the one committed to SMT, one will need to load it from RDBMS, marshal using protobuf, hash and do SMT search.\n\n### Off Chain Store\n\nWe were discussing use case where modules can use a support database, which is not automatically committed. Module will responsible for having a sound storage model and can optionally use the feature discussed in __Committing to an object without saving it_ section.\n\n## References\n\n* [IAVL What's Next?](https://github.com/cosmos/cosmos-sdk/issues/7100)\n* [IAVL overview](https://docs.google.com/document/d/16Z_hW2rSAmoyMENO-RlAhQjAG3mSNKsQueMnKpmcBv0/edit#heading=h.yd2th7x3o1iv) of it's state v0.15\n* [State commitments and storage report](https://paper.dropbox.com/published/State-commitments-and-storage-review--BDvA1MLwRtOx55KRihJ5xxLbBw-KeEB7eOd11pNrZvVtqUgL3h)\n* [Celestia (LazyLedger) SMT](https://github.com/lazyledger/smt)\n* Facebook Diem (Libra) SMT [design](https://developers.diem.com/papers/jellyfish-merkle-tree/2021-01-14.pdf)\n* [Trillian Revocation Transparency](https://github.com/google/trillian/blob/master/docs/papers/RevocationTransparency.pdf), [Trillian Verifiable Data Structures](https://github.com/google/trillian/blob/master/docs/papers/VerifiableDataStructures.pdf).\n* Design and implementation [discussion](https://github.com/cosmos/cosmos-sdk/discussions/8297).\n* [How to Upgrade IBC Chains and their Clients](https://ibc.cosmos.network/main/ibc/upgrades/quick-guide/)\n* [ADR-40 Effect on IBC](https://github.com/cosmos/ibc-go/discussions/256)"
  },
  {
    "number": 41,
    "filename": "adr-041-in-place-store-migrations.md",
    "title": "ADR 041: In-Place Store Migrations",
    "content": "# ADR 041: In-Place Store Migrations\n\n## Changelog\n\n* 17.02.2021: Initial Draft\n\n## Status\n\nAccepted\n\n## Abstract\n\nThis ADR introduces a mechanism to perform in-place state store migrations during chain software upgrades.\n\n## Context\n\nWhen a chain upgrade introduces state-breaking changes inside modules, the current procedure consists of exporting the whole state into a JSON file (via the `simd export` command), running migration scripts on the JSON file (`simd genesis migrate` command), clearing the stores (`simd unsafe-reset-all` command), and starting a new chain with the migrated JSON file as new genesis (optionally with a custom initial block height). An example of such a procedure can be seen [in the Cosmos Hub 3->4 migration guide](https://github.com/cosmos/gaia/blob/v4.0.3/docs/migration/cosmoshub-3.md#upgrade-procedure).\n\nThis procedure is cumbersome for multiple reasons:\n\n* The procedure takes time. It can take hours to run the `export` command, plus some additional hours to run `InitChain` on the fresh chain using the migrated JSON.\n* The exported JSON file can be heavy (~100MB-1GB), making it difficult to view, edit and transfer, which in turn introduces additional work to solve these problems (such as [streaming genesis](https://github.com/cosmos/cosmos-sdk/issues/6936)).\n\n## Decision\n\nWe propose a migration procedure based on modifying the KV store in-place without involving the JSON export-process-import flow described above.\n\n### Module `ConsensusVersion`\n\nWe introduce a new method on the `AppModule` interface:\n\n```go\ntype AppModule interface {\n    // --snip--\n    ConsensusVersion() uint64\n}\n```\n\nThis methods returns an `uint64` which serves as state-breaking version of the module. It MUST be incremented on each consensus-breaking change introduced by the module. To avoid potential errors with default values, the initial version of a module MUST be set to 1. In the Cosmos SDK, version 1 corresponds to the modules in the v0.41 series.\n\n### Module-Specific Migration Functions\n\nFor each consensus-breaking change introduced by the module, a migration script from ConsensusVersion `N` to version `N+1` MUST be registered in the `Configurator` using its newly-added `RegisterMigration` method. All modules receive a reference to the configurator in their `RegisterServices` method on `AppModule`, and this is where the migration functions should be registered. The migration functions should be registered in increasing order.\n\n```go\nfunc (am AppModule) RegisterServices(cfg module.Configurator) {\n    // --snip--\n    cfg.RegisterMigration(types.ModuleName, 1, func(ctx sdk.Context) error {\n        // Perform in-place store migrations from ConsensusVersion 1 to 2.\n    })\n     cfg.RegisterMigration(types.ModuleName, 2, func(ctx sdk.Context) error {\n        // Perform in-place store migrations from ConsensusVersion 2 to 3.\n    })\n    // etc.\n}\n```\n\nFor example, if the new ConsensusVersion of a module is `N` , then `N-1` migration functions MUST be registered in the configurator.\n\nIn the Cosmos SDK, the migration functions are handled by each module's keeper, because the keeper holds the `sdk.StoreKey` used to perform in-place store migrations. To not overload the keeper, a `Migrator` wrapper is used by each module to handle the migration functions:\n\n```go\n// Migrator is a struct for handling in-place store migrations.\ntype Migrator struct {\n  BaseKeeper\n}\n```\n\nMigration functions should live inside the `migrations/` folder of each module, and be called by the Migrator's methods. We propose the format `Migrate{M}to{N}` for method names.\n\n```go\n// Migrate1to2 migrates from version 1 to 2.\nfunc (m Migrator) Migrate1to2(ctx sdk.Context) error {\n\treturn v2bank.MigrateStore(ctx, m.keeper.storeKey) // v043bank is package `x/bank/migrations/v2`.\n}\n```\n\nEach module's migration functions are specific to the module's store evolutions, and are not described in this ADR. An example of x/bank store key migrations after the introduction of ADR-028 length-prefixed addresses can be seen in this [store.go code](https://github.com/cosmos/cosmos-sdk/blob/36f68eb9e041e20a5bb47e216ac5eb8b91f95471/x/bank/legacy/v043/store.go#L41-L62).\n\n### Tracking Module Versions in `x/upgrade`\n\nWe introduce a new prefix store in `x/upgrade`'s store. This store will track each module's current version, it can be modelized as a `map[string]uint64` of module name to module ConsensusVersion, and will be used when running the migrations (see next section for details). The key prefix used is `0x1`, and the key/value format is:\n\n```text\n0x2 | {bytes(module_name)} => BigEndian(module_consensus_version)\n```\n\nThe initial state of the store is set from `app.go`'s `InitChainer` method.\n\nThe UpgradeHandler signature needs to be updated to take a `VersionMap`, as well as return an upgraded `VersionMap` and an error:\n\n```diff\n- type UpgradeHandler func(ctx sdk.Context, plan Plan)\n+ type UpgradeHandler func(ctx sdk.Context, plan Plan, versionMap VersionMap) (VersionMap, error)\n```\n\nTo apply an upgrade, we query the `VersionMap` from the `x/upgrade` store and pass it into the handler. The handler runs the actual migration functions (see next section), and if successful, returns an updated `VersionMap` to be stored in state.\n\n```diff\nfunc (k UpgradeKeeper) ApplyUpgrade(ctx sdk.Context, plan types.Plan) {\n    // --snip--\n-   handler(ctx, plan)\n+   updatedVM, err := handler(ctx, plan, k.GetModuleVersionMap(ctx)) // k.GetModuleVersionMap() fetches the VersionMap stored in state.\n+   if err != nil {\n+       return err\n+   }\n+\n+   // Set the updated consensus versions to state\n+   k.SetModuleVersionMap(ctx, updatedVM)\n}\n```\n\nA gRPC query endpoint to query the `VersionMap` stored in `x/upgrade`'s state will also be added, so that app developers can double-check the `VersionMap` before the upgrade handler runs.\n\n### Running Migrations\n\nOnce all the migration handlers are registered inside the configurator (which happens at startup), running migrations can happen by calling the `RunMigrations` method on `module.Manager`. This function will loop through all modules, and for each module:\n\n* Get the old ConsensusVersion of the module from its `VersionMap` argument (let's call it `M`).\n* Fetch the new ConsensusVersion of the module from the `ConsensusVersion()` method on `AppModule` (call it `N`).\n* If `N>M`, run all registered migrations for the module sequentially `M -> M+1 -> M+2...` until `N`.\n    * There is a special case where there is no ConsensusVersion for the module, as this means that the module has been newly added during the upgrade. In this case, no migration function is run, and the module's current ConsensusVersion is saved to `x/upgrade`'s store.\n\nIf a required migration is missing (e.g. if it has not been registered in the `Configurator`), then the `RunMigrations` function will error.\n\nIn practice, the `RunMigrations` method should be called from inside an `UpgradeHandler`.\n\n```go\napp.UpgradeKeeper.SetUpgradeHandler(\"my-plan\", func(ctx sdk.Context, plan upgradetypes.Plan, vm module.VersionMap)  (module.VersionMap, error) {\n    return app.mm.RunMigrations(ctx, vm)\n})\n```\n\nAssuming a chain upgrades at block `n`, the procedure should run as follows:\n\n* the old binary will halt in `BeginBlock` when starting block `N`. In its store, the ConsensusVersions of the old binary's modules are stored.\n* the new binary will start at block `N`. The UpgradeHandler is set in the new binary, so will run at `BeginBlock` of the new binary. Inside `x/upgrade`'s `ApplyUpgrade`, the `VersionMap` will be retrieved from the (old binary's) store, and passed into the `RunMigrations` function, migrating all module stores in-place before the modules' own `BeginBlock`s.\n\n## Consequences\n\n### Backwards Compatibility\n\nThis ADR introduces a new method `ConsensusVersion()` on `AppModule`, which all modules need to implement. It also alters the UpgradeHandler function signature. As such, it is not backwards-compatible.\n\nWhile modules MUST register their migration functions when bumping ConsensusVersions, running those scripts using an upgrade handler is optional. An application may perfectly well decide to not call the `RunMigrations` inside its upgrade handler, and continue using the legacy JSON migration path.\n\n### Positive\n\n* Perform chain upgrades without manipulating JSON files.\n* While no benchmark has been made yet, it is probable that in-place store migrations will take less time than JSON migrations. The main reason supporting this claim is that both the `simd export` command on the old binary and the `InitChain` function on the new binary will be skipped.\n\n### Negative\n\n* Module developers MUST correctly track consensus-breaking changes in their modules. If a consensus-breaking change is introduced in a module without its corresponding `ConsensusVersion()` bump, then the `RunMigrations` function won't detect the migration, and the chain upgrade might be unsuccessful. Documentation should clearly reflect this.\n\n### Neutral\n\n* The Cosmos SDK will continue to support JSON migrations via the existing `simd export` and `simd genesis migrate` commands.\n* The current ADR does not allow creating, renaming or deleting stores, only modifying existing store keys and values. The Cosmos SDK already has the `StoreLoader` for those operations.\n\n## Further Discussions\n\n## References\n\n* Initial discussion: https://github.com/cosmos/cosmos-sdk/discussions/8429\n* Implementation of `ConsensusVersion` and `RunMigrations`: https://github.com/cosmos/cosmos-sdk/pull/8485\n* Issue discussing `x/upgrade` design: https://github.com/cosmos/cosmos-sdk/issues/8514"
  },
  {
    "number": 42,
    "filename": "adr-042-group-module.md",
    "title": "ADR 042: Group Module",
    "content": "# ADR 042: Group Module\n\n## Changelog\n\n* 2020/04/09: Initial Draft\n\n## Status\n\nDraft\n\n## Abstract\n\nThis ADR defines the `x/group` module which allows the creation and management of on-chain multi-signature accounts and enables voting for message execution based on configurable decision policies.\n\n## Context\n\nThe legacy amino multi-signature mechanism of the Cosmos SDK has certain limitations:\n\n* Key rotation is not possible, although this can be solved with [account rekeying](adr-034-account-rekeying.md).\n* Thresholds can't be changed.\n* UX is cumbersome for non-technical users ([#5661](https://github.com/cosmos/cosmos-sdk/issues/5661)).\n* It requires `legacy_amino` sign mode ([#8141](https://github.com/cosmos/cosmos-sdk/issues/8141)).\n\nWhile the group module is not meant to be a total replacement for the current multi-signature accounts, it provides a solution to the limitations described above, with a more flexible key management system where keys can be added, updated or removed, as well as configurable thresholds.\nIt's meant to be used with other access control modules such as [`x/feegrant`](./adr-029-fee-grant-module.md) and [`x/authz`](adr-030-authz-module.md) to simplify key management for individuals and organizations.\n\nThe proof of concept of the group module can be found in https://github.com/cosmos/cosmos-sdk/tree/main/proto/cosmos/group/v1 and https://github.com/cosmos/cosmos-sdk/tree/main/x/group.\n\n## Decision\n\nWe propose merging the `x/group` module with its supporting [ORM/Table Store package](https://github.com/cosmos/cosmos-sdk/tree/main/x/group/internal/orm) ([#7098](https://github.com/cosmos/cosmos-sdk/issues/7098)) into the Cosmos SDK and continuing development here. There will be a dedicated ADR for the ORM package.\n\n### Group\n\nA group is a composition of accounts with associated weights. It is not\nan account and doesn't have a balance. It doesn't in and of itself have any\nsort of voting or decision weight.\nGroup members can create proposals and vote on them through group accounts using different decision policies.\n\nIt has an `admin` account which can manage members in the group, update the group\nmetadata and set a new admin.\n\n```protobuf\nmessage GroupInfo {\n\n    // group_id is the unique ID of this group.\n    uint64 group_id = 1;\n\n    // admin is the account address of the group's admin.\n    string admin = 2;\n\n    // metadata is any arbitrary metadata to attached to the group.\n    bytes metadata = 3;\n\n    // version is used to track changes to a group's membership structure that\n    // would break existing proposals. Whenever a member weight has changed,\n    // or any member is added or removed, the version is incremented and will\n    // invalidate all proposals from older versions.\n    uint64 version = 4;\n\n    // total_weight is the sum of the group members' weights.\n    string total_weight = 5;\n}\n```\n\n```protobuf\nmessage GroupMember {\n\n    // group_id is the unique ID of the group.\n    uint64 group_id = 1;\n\n    // member is the member data.\n    Member member = 2;\n}\n\n// Member represents a group member with an account address,\n// non-zero weight and metadata.\nmessage Member {\n\n    // address is the member's account address.\n    string address = 1;\n\n    // weight is the member's voting weight that should be greater than 0.\n    string weight = 2;\n\n    // metadata is any arbitrary metadata to attached to the member.\n    bytes metadata = 3;\n}\n```\n\n### Group Account\n\nA group account is an account associated with a group and a decision policy.\nA group account does have a balance.\n\nGroup accounts are abstracted from groups because a single group may have\nmultiple decision policies for different types of actions. Managing group\nmembership separately from decision policies results in the least overhead\nand keeps membership consistent across different policies. The pattern that\nis recommended is to have a single master group account for a given group,\nand then to create separate group accounts with different decision policies\nand delegate the desired permissions from the master account to\nthose \"sub-accounts\" using the [`x/authz` module](adr-030-authz-module.md).\n\n```protobuf\nmessage GroupAccountInfo {\n\n    // address is the group account address.\n    string address = 1;\n\n    // group_id is the ID of the Group the GroupAccount belongs to.\n    uint64 group_id = 2;\n\n    // admin is the account address of the group admin.\n    string admin = 3;\n\n    // metadata is any arbitrary metadata of this group account.\n    bytes metadata = 4;\n\n    // version is used to track changes to a group's GroupAccountInfo structure that\n    // invalidates active proposal from old versions.\n    uint64 version = 5;\n\n    // decision_policy specifies the group account's decision policy.\n    google.protobuf.Any decision_policy = 6 [(cosmos_proto.accepts_interface) = \"cosmos.group.v1.DecisionPolicy\"];\n}\n```\n\nSimilarly to a group admin, a group account admin can update its metadata, decision policy or set a new group account admin.\n\nA group account can also be an admin or a member of a group.\nFor instance, a group admin could be another group account which could \"elects\" the members or it could be the same group that elects itself.\n\n### Decision Policy\n\nA decision policy is the mechanism by which members of a group can vote on\nproposals.\n\nAll decision policies should have a minimum and maximum voting window.\nThe minimum voting window is the minimum duration that must pass in order\nfor a proposal to potentially pass, and it may be set to 0. The maximum voting\nwindow is the maximum time that a proposal may be voted on and executed if\nit reached enough support before it is closed.\nBoth of these values must be less than a chain-wide max voting window parameter.\n\nWe define the `DecisionPolicy` interface that all decision policies must implement:\n\n```go\ntype DecisionPolicy interface {\n\tcodec.ProtoMarshaler\n\n\tValidateBasic() error\n\tGetTimeout() types.Duration\n\tAllow(tally Tally, totalPower string, votingDuration time.Duration) (DecisionPolicyResult, error)\n\tValidate(g GroupInfo) error\n}\n\ntype DecisionPolicyResult struct {\n\tAllow bool\n\tFinal bool\n}\n```\n\n#### Threshold decision policy\n\nA threshold decision policy defines a minimum support votes (_yes_), based on a tally\nof voter weights, for a proposal to pass. For\nthis decision policy, abstain and veto are treated as no support (_no_).\n\n```protobuf\nmessage ThresholdDecisionPolicy {\n\n    // threshold is the minimum weighted sum of support votes for a proposal to succeed.\n    string threshold = 1;\n\n    // voting_period is the duration from submission of a proposal to the end of voting period\n    // Within this period, votes and exec messages can be submitted.\n    google.protobuf.Duration voting_period = 2 [(gogoproto.nullable) = false];\n}\n```\n\n### Proposal\n\nAny member of a group can submit a proposal for a group account to decide upon.\nA proposal consists of a set of `sdk.Msg`s that will be executed if the proposal\npasses as well as any metadata associated with the proposal. These `sdk.Msg`s get validated as part of the `Msg/CreateProposal` request validation. They should also have their signer set as the group account.\n\nInternally, a proposal also tracks:\n\n* its current `Status`: submitted, closed or aborted\n* its `Result`: unfinalized, accepted or rejected\n* its `VoteState` in the form of a `Tally`, which is calculated on new votes and when executing the proposal.\n\n```protobuf\n// Tally represents the sum of weighted votes.\nmessage Tally {\n    option (gogoproto.goproto_getters) = false;\n\n    // yes_count is the weighted sum of yes votes.\n    string yes_count = 1;\n\n    // no_count is the weighted sum of no votes.\n    string no_count = 2;\n\n    // abstain_count is the weighted sum of abstainers.\n    string abstain_count = 3;\n\n    // veto_count is the weighted sum of vetoes.\n    string veto_count = 4;\n}\n```\n\n### Voting\n\nMembers of a group can vote on proposals. There are four choices to choose while voting - yes, no, abstain and veto. Not\nall decision policies will support them. Votes can contain some optional metadata.\nIn the current implementation, the voting window begins as soon as a proposal\nis submitted.\n\nVoting internally updates the proposal `VoteState` as well as `Status` and `Result` if needed.\n\n### Executing Proposals\n\nProposals will not be automatically executed by the chain in this current design,\nbut rather a user must submit a `Msg/Exec` transaction to attempt to execute the\nproposal based on the current votes and decision policy. A future upgrade could\nautomate this and have the group account (or a fee granter) pay.\n\n#### Changing Group Membership\n\nIn the current implementation, updating a group or a group account after submitting a proposal will make it invalid. It will simply fail if someone calls `Msg/Exec` and will eventually be garbage collected.\n\n### Notes on current implementation\n\nThis section outlines the current implementation used in the proof of concept of the group module but this could be subject to changes and iterated on.\n\n#### ORM\n\nThe [ORM package](https://github.com/cosmos/cosmos-sdk/discussions/9156) defines tables, sequences and secondary indexes which are used in the group module.\n\nGroups are stored in state as part of a `groupTable`, the `group_id` being an auto-increment integer. Group members are stored in a `groupMemberTable`.\n\nGroup accounts are stored in a `groupAccountTable`. The group account address is generated based on an auto-increment integer which is used to derive the group module `RootModuleKey` into a `DerivedModuleKey`, as stated in [ADR-033](adr-033-protobuf-inter-module-comm.md#modulekeys-and-moduleids). The group account is added as a new `ModuleAccount` through `x/auth`.\n\nProposals are stored as part of the `proposalTable` using the `Proposal` type. The `proposal_id` is an auto-increment integer.\n\nVotes are stored in the `voteTable`. The primary key is based on the vote's `proposal_id` and `voter` account address.\n\n#### ADR-033 to route proposal messages\n\nInter-module communication introduced by [ADR-033](adr-033-protobuf-inter-module-comm.md) can be used to route a proposal's messages using the `DerivedModuleKey` corresponding to the proposal's group account.\n\n## Consequences\n\n### Positive\n\n* Improved UX for multi-signature accounts allowing key rotation and custom decision policies.\n\n### Negative\n\n### Neutral\n\n* It uses ADR 033 so it will need to be implemented within the Cosmos SDK, but this doesn't imply necessarily any large refactoring of existing Cosmos SDK modules.\n* The current implementation of the group module uses the ORM package.\n\n## Further Discussions\n\n* Convergence of `/group` and `x/gov` as both support proposals and voting: https://github.com/cosmos/cosmos-sdk/discussions/9066\n* `x/group` possible future improvements:\n    * Execute proposals on submission (https://github.com/regen-network/regen-ledger/issues/288)\n    * Withdraw a proposal (https://github.com/regen-network/cosmos-modules/issues/41)\n    * Make `Tally` more flexible and support non-binary choices\n\n## References\n\n* Initial specification:\n    * https://gist.github.com/aaronc/b60628017352df5983791cad30babe56#group-module\n    * [#5236](https://github.com/cosmos/cosmos-sdk/pull/5236)\n* Proposal to add `x/group` into the Cosmos SDK: [#7633](https://github.com/cosmos/cosmos-sdk/issues/7633)"
  },
  {
    "number": 43,
    "filename": "adr-043-nft-module.md",
    "title": "ADR 43: NFT Module",
    "content": "# ADR 43: NFT Module\n\n## Changelog\n\n* 2021-05-01: Initial Draft\n* 2021-07-02: Review updates\n* 2022-06-15: Add batch operation\n* 2022-11-11: Remove strict validation of classID and tokenID\n\n## Status\n\nPROPOSED\n\n## Abstract\n\nThis ADR defines the `x/nft` module which is a generic implementation of NFTs, roughly \"compatible\" with ERC721. **Applications using the `x/nft` module must implement the following functions**:\n\n* `MsgNewClass` - Receive the user's request to create a class, and call the `NewClass` of the `x/nft` module.\n* `MsgUpdateClass` - Receive the user's request to update a class, and call the `UpdateClass` of the `x/nft` module.\n* `MsgMintNFT` - Receive the user's request to mint a nft, and call the `MintNFT` of the `x/nft` module.\n* `BurnNFT` - Receive the user's request to burn a nft, and call the `BurnNFT` of the `x/nft` module.\n* `UpdateNFT` - Receive the user's request to update a nft, and call the `UpdateNFT` of the `x/nft` module.\n\n## Context\n\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https://github.com/cosmos/cosmos-sdk/discussions/9065.\n\nAs discussed in [#9065](https://github.com/cosmos/cosmos-sdk/discussions/9065), several potential solutions can be considered:\n\n* irismod/nft and modules/incubator/nft\n* CW721\n* DID NFTs\n* interNFT\n\nSince functions/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\n\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\n\nThe current design is based on the work done by [IRISnet team](https://github.com/irisnet/irismod/tree/master/modules/nft) and an older implementation in the [Cosmos repository](https://github.com/cosmos/modules/tree/master/incubator/nft).\n\n## Decision\n\nWe create a `x/nft` module, which contains the following functionality:\n\n* Store NFTs and track their ownership.\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\n* Query NFTs and their supply information.\n\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\n\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\n\n### Types\n\nWe propose two main types:\n\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\n\n#### Class\n\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\n\n```protobuf\nmessage Class {\n  string id          = 1;\n  string name        = 2;\n  string symbol      = 3;\n  string description = 4;\n  string uri         = 5;\n  string uri_hash    = 6;\n  google.protobuf.Any data = 7;\n}\n```\n\n* `id` is used as the primary index for storing the class; _required_\n* `name` is a descriptive name of the NFT class; _optional_\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\n* `description` is a detailed description of the NFT class; _optional_\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https://docs.opensea.io/docs/contract-level-metadata)); _optional_\n* `uri_hash` is a hash of the document pointed by uri; _optional_\n* `data` is app specific metadata of the class; _optional_\n\n#### NFT\n\nWe define a general model for `NFT` as follows.\n\n```protobuf\nmessage NFT {\n  string class_id           = 1;\n  string id                 = 2;\n  string uri                = 3;\n  string uri_hash           = 4;\n  google.protobuf.Any data  = 10;\n}\n```\n\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\n\n  ```text\n  {class_id}/{id} --> NFT (bytes)\n  ```\n\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https://docs.opensea.io/docs/metadata-standards)); _required_\n* `uri_hash` is a hash of the document pointed by uri; _optional_\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\n\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational/UI functionality.\n\n### `Keeper` Interface\n\n```go\ntype Keeper interface {\n  NewClass(ctx sdk.Context,class Class)\n  UpdateClass(ctx sdk.Context,class Class)\n\n  Mint(ctx sdk.Context,nft NFT，receiver sdk.AccAddress)   // updates totalSupply\n  BatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\n\n  Burn(ctx sdk.Context, classId string, nftId string)    // updates totalSupply\n  BatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\n\n  Update(ctx sdk.Context, nft NFT)\n  BatchUpdate(ctx sdk.Context, tokens []NFT) error\n\n  Transfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\n  BatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\n\n  GetClass(ctx sdk.Context, classId string) Class\n  GetClasses(ctx sdk.Context) []Class\n\n  GetNFT(ctx sdk.Context, classId string, nftId string) NFT\n  GetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\n  GetNFTsOfClass(ctx sdk.Context, classId string) []NFT\n\n  GetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\n  GetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\n  GetTotalSupply(ctx sdk.Context, classId string) uint64\n}\n```\n\nOther business logic implementations should be defined in composing modules that import `x/nft` and use its `Keeper`.\n\n### `Msg` Service\n\n```protobuf\nservice Msg {\n  rpc Send(MsgSend)         returns (MsgSendResponse);\n}\n\nmessage MsgSend {\n  string class_id = 1;\n  string id       = 2;\n  string sender   = 3;\n  string receiver = 4;\n}\nmessage MsgSendResponse {}\n```\n\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\n\nThe implementation outline of the server is as follows:\n\n```go\ntype msgServer struct{\n  k Keeper\n}\n\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\n  // check current ownership\n  assertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\n\n  // transfer ownership\n  m.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\n\n  return &types.MsgSendResponse{}, nil\n}\n```\n\nThe query service methods for the `x/nft` module are:\n\n```protobuf\nservice Query {\n  // Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\n  rpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\n    option (google.api.http).get = \"/cosmos/nft/v1beta1/balance/{owner}/{class_id}\";\n  }\n\n  // Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\n  rpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\n    option (google.api.http).get = \"/cosmos/nft/v1beta1/owner/{class_id}/{id}\";\n  }\n\n  // Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\n  rpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\n    option (google.api.http).get = \"/cosmos/nft/v1beta1/supply/{class_id}\";\n  }\n\n  // NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\n  rpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\n    option (google.api.http).get = \"/cosmos/nft/v1beta1/nfts\";\n  }\n\n  // NFT queries an NFT based on its class and id.\n  rpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\n    option (google.api.http).get = \"/cosmos/nft/v1beta1/nfts/{class_id}/{id}\";\n  }\n\n  // Class queries an NFT class based on its id\n  rpc Class(QueryClassRequest) returns (QueryClassResponse) {\n    option (google.api.http).get = \"/cosmos/nft/v1beta1/classes/{class_id}\";\n  }\n\n  // Classes queries all NFT classes\n  rpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\n    option (google.api.http).get = \"/cosmos/nft/v1beta1/classes\";\n  }\n}\n\n// QueryBalanceRequest is the request type for the Query/Balance RPC method\nmessage QueryBalanceRequest {\n  string class_id = 1;\n  string owner    = 2;\n}\n\n// QueryBalanceResponse is the response type for the Query/Balance RPC method\nmessage QueryBalanceResponse {\n  uint64 amount = 1;\n}\n\n// QueryOwnerRequest is the request type for the Query/Owner RPC method\nmessage QueryOwnerRequest {\n  string class_id = 1;\n  string id       = 2;\n}\n\n// QueryOwnerResponse is the response type for the Query/Owner RPC method\nmessage QueryOwnerResponse {\n  string owner = 1;\n}\n\n// QuerySupplyRequest is the request type for the Query/Supply RPC method\nmessage QuerySupplyRequest {\n  string class_id = 1;\n}\n\n// QuerySupplyResponse is the response type for the Query/Supply RPC method\nmessage QuerySupplyResponse {\n  uint64 amount = 1;\n}\n\n// QueryNFTsRequest is the request type for the Query/NFTs RPC method\nmessage QueryNFTsRequest {\n  string                                class_id   = 1;\n  string                                owner      = 2;\n  cosmos.base.query.v1beta1.PageRequest pagination = 3;\n}\n\n// QueryNFTsResponse is the response type for the Query/NFTs RPC methods\nmessage QueryNFTsResponse {\n  repeated cosmos.nft.v1beta1.NFT        nfts       = 1;\n  cosmos.base.query.v1beta1.PageResponse pagination = 2;\n}\n\n// QueryNFTRequest is the request type for the Query/NFT RPC method\nmessage QueryNFTRequest {\n  string class_id = 1;\n  string id       = 2;\n}\n\n// QueryNFTResponse is the response type for the Query/NFT RPC method\nmessage QueryNFTResponse {\n  cosmos.nft.v1beta1.NFT nft = 1;\n}\n\n// QueryClassRequest is the request type for the Query/Class RPC method\nmessage QueryClassRequest {\n  string class_id = 1;\n}\n\n// QueryClassResponse is the response type for the Query/Class RPC method\nmessage QueryClassResponse {\n  cosmos.nft.v1beta1.Class class = 1;\n}\n\n// QueryClassesRequest is the request type for the Query/Classes RPC method\nmessage QueryClassesRequest {\n  // pagination defines an optional pagination for the request.\n  cosmos.base.query.v1beta1.PageRequest pagination = 1;\n}\n\n// QueryClassesResponse is the response type for the Query/Classes RPC method\nmessage QueryClassesResponse {\n  repeated cosmos.nft.v1beta1.Class      classes    = 1;\n  cosmos.base.query.v1beta1.PageResponse pagination = 2;\n}\n```\n\n### Interoperability\n\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x/nft. This requires creation of a new IBC standard and implementation of it.\n\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x/nft interoperability, custom NFT implementations (example: x/cryptokitty) should use the canonical x/nft module and proxy all NFT balance keeping functionality to x/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x/nft becomes the standard NFT registry for all Cosmos NFTs (example: x/cryptokitty will register a kitty NFT in x/nft and use x/nft for book keeping). This was [discussed](https://github.com/cosmos/cosmos-sdk/discussions/9065#discussioncomment-873206) in the context of using x/bank as a general asset balance book. Not using x/nft will require implementing another module for IBC.\n\n## Consequences\n\n### Backward Compatibility\n\nNo backward incompatibilities.\n\n### Forward Compatibility\n\nThis specification conforms to the ERC-721 smart contract specification for NFT identifiers. Note that ERC-721 defines uniqueness based on (contract address, uint256 tokenId), and we conform to this implicitly because a single module is currently aimed to track NFT identifiers. Note: use of the (mutable) data field to determine uniqueness is not safe.\n\n### Positive\n\n* NFT identifiers available on Cosmos Hub.\n* Ability to build different NFT modules for the Cosmos Hub, e.g., ERC-721.\n* NFT module which supports interoperability with IBC and other cross-chain infrastructures like Gravity Bridge\n\n### Negative\n\n* New IBC app is required for x/nft\n* CW721 adapter is required\n\n### Neutral\n\n* Other functions need more modules. For example, a custody module is needed for NFT trading function, a collectible module is needed for defining NFT properties.\n\n## Further Discussions\n\nFor other kinds of applications on the Hub, more app-specific modules can be developed in the future:\n\n* `x/nft/custody`: custody of NFTs to support trading functionality.\n* `x/nft/marketplace`: selling and buying NFTs using sdk.Coins.\n* `x/fractional`: a module to split an ownership of an asset (NFT or other assets) for multiple stakeholder. `x/group`  should work for most of the cases.\n\nOther networks in the Cosmos ecosystem could design and implement their own NFT modules for specific NFT applications and use cases.\n\n## References\n\n* Initial discussion: https://github.com/cosmos/cosmos-sdk/discussions/9065\n* x/nft: initialize module: https://github.com/cosmos/cosmos-sdk/pull/9174\n* [ADR 033](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-033-protobuf-inter-module-comm.md)"
  },
  {
    "number": 44,
    "filename": "adr-044-protobuf-updates-guidelines.md",
    "title": "ADR 044: Guidelines for Updating Protobuf Definitions",
    "content": "# ADR 044: Guidelines for Updating Protobuf Definitions\n\n## Changelog\n\n* 28.06.2021: Initial Draft\n* 02.12.2021: Add `Since:` comment for new fields\n* 21.07.2022: Remove the rule of no new `Msg` in the same proto version.\n\n## Status\n\nDraft\n\n## Abstract\n\nThis ADR provides guidelines and recommended practices when updating Protobuf definitions. These guidelines are targeting module developers.\n\n## Context\n\nThe Cosmos SDK maintains a set of [Protobuf definitions](https://github.com/cosmos/cosmos-sdk/tree/main/proto/cosmos). It is important to correctly design Protobuf definitions to avoid any breaking changes within the same version. The reasons are to not break tooling (including indexers and explorers), wallets and other third-party integrations.\n\nWhen making changes to these Protobuf definitions, the Cosmos SDK currently only follows [Buf's](https://docs.buf.build/) recommendations. We noticed however that Buf's recommendations might still result in breaking changes in the SDK in some cases. For example:\n\n* Adding fields to `Msg`s. Adding fields is not a Protobuf spec-breaking operation. However, when adding new fields to `Msg`s, the unknown field rejection will throw an error when sending the new `Msg` to an older node.\n* Marking fields as `reserved`. Protobuf proposes the `reserved` keyword for removing fields without the need to bump the package version. However, by doing so, client backwards compatibility is broken as Protobuf doesn't generate anything for `reserved` fields. See [#9446](https://github.com/cosmos/cosmos-sdk/issues/9446) for more details on this issue.\n\nMoreover, module developers often face other questions around Protobuf definitions such as \"Can I rename a field?\" or \"Can I deprecate a field?\" This ADR aims to answer all these questions by providing clear guidelines about allowed updates for Protobuf definitions.\n\n## Decision\n\nWe decide to keep [Buf's](https://docs.buf.build/) recommendations with the following exceptions:\n\n* `UNARY_RPC`: the Cosmos SDK currently does not support streaming RPCs.\n* `COMMENT_FIELD`: the Cosmos SDK allows fields with no comments.\n* `SERVICE_SUFFIX`: we use the `Query` and `Msg` service naming convention, which doesn't use the `-Service` suffix.\n* `PACKAGE_VERSION_SUFFIX`: some packages, such as `cosmos.crypto.ed25519`, don't use a version suffix.\n* `RPC_REQUEST_STANDARD_NAME`: Requests for the `Msg` service don't have the `-Request` suffix to keep backwards compatibility.\n\nOn top of Buf's recommendations we add the following guidelines that are specific to the Cosmos SDK.\n\n### Updating Protobuf Definition Without Bumping Version\n\n#### 1. Module developers MAY add new Protobuf definitions\n\nModule developers MAY add new `message`s, new `Service`s, new `rpc` endpoints, and new fields to existing messages. This recommendation follows the Protobuf specification, but is added in this document for clarity, as the SDK requires one additional change.\n\nThe SDK requires the Protobuf comment of the new addition to contain one line with the following format:\n\n```protobuf\n// Since: cosmos-sdk <version>{, <version>...}\n```\n\nWhere each `version` denotes a minor (\"0.45\") or patch (\"0.44.5\") version from which the field is available. This will greatly help client libraries, who can optionally use reflection or custom code generation to show/hide these fields depending on the targeted node version.\n\nAs examples, the following comments are valid:\n\n```protobuf\n// Since: cosmos-sdk 0.44\n\n// Since: cosmos-sdk 0.42.11, 0.44.5\n```\n\nand the following ones are NOT valid:\n\n```protobuf\n// Since cosmos-sdk v0.44\n\n// since: cosmos-sdk 0.44\n\n// Since: cosmos-sdk 0.42.11 0.44.5\n\n// Since: Cosmos SDK 0.42.11, 0.44.5\n```\n\n#### 2. Fields MAY be marked as `deprecated`, and nodes MAY implement a protocol-breaking change for handling these fields\n\nProtobuf supports the [`deprecated` field option](https://developers.google.com/protocol-buffers/docs/proto#options), and this option MAY be used on any field, including `Msg` fields. If a node handles a Protobuf message with a non-empty deprecated field, the node MAY change its behavior upon processing it, even in a protocol-breaking way. When possible, the node MUST handle backwards compatibility without breaking the consensus (unless we increment the proto version).\n\nAs an example, the Cosmos SDK v0.42 to v0.43 update contained two Protobuf-breaking changes, listed below. Instead of bumping the package versions from `v1beta1` to `v1`, the SDK team decided to follow this guideline, by reverting the breaking changes, marking those changes as deprecated, and modifying the node implementation when processing messages with deprecated fields. More specifically:\n\n* The Cosmos SDK recently removed support for [time-based software upgrades](https://github.com/cosmos/cosmos-sdk/pull/8849). As such, the `time` field has been marked as deprecated in `cosmos.upgrade.v1beta1.Plan`. Moreover, the node will reject any proposal containing an upgrade Plan whose `time` field is non-empty.\n* The Cosmos SDK now supports [governance split votes](./adr-037-gov-split-vote.md). When querying for votes, the returned `cosmos.gov.v1beta1.Vote` message has its `option` field (used for 1 vote option) deprecated in favor of its `options` field (allowing multiple vote options). Whenever possible, the SDK still populates the deprecated `option` field, that is, if and only if the `len(options) == 1` and `options[0].Weight == 1.0`.\n\n#### 3. Fields MUST NOT be renamed\n\nWhereas the official Protobuf recommendations do not prohibit renaming fields, as it does not break the Protobuf binary representation, the SDK explicitly forbids renaming fields in Protobuf structs. The main reason for this choice is to avoid introducing breaking changes for clients, which often rely on hard-coded fields from generated types. Moreover, renaming fields will lead to client-breaking JSON representations of Protobuf definitions, used in REST endpoints and in the CLI.\n\n### Incrementing Protobuf Package Version\n\nTODO, needs architecture review. Some topics:\n\n* Bumping versions frequency\n* When bumping versions, should the Cosmos SDK support both versions?\n    * i.e. v1beta1 -> v1, should we have two folders in the Cosmos SDK, and handlers for both versions?\n* mention ADR-023 Protobuf naming\n\n## Consequences\n\n> This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\n\n### Backwards Compatibility\n\n> All ADRs that introduce backwards incompatibilities must include a section describing these incompatibilities and their severity. The ADR must explain how the author proposes to deal with these incompatibilities. ADR submissions without a sufficient backwards compatibility treatise may be rejected outright.\n\n### Positive\n\n* less pain to tool developers\n* more compatibility in the ecosystem\n* ...\n\n### Negative\n\n{negative consequences}\n\n### Neutral\n\n* more rigor in Protobuf review\n\n## Further Discussions\n\nThis ADR is still in the DRAFT stage, and the \"Incrementing Protobuf Package Version\" will be filled in once we make a decision on how to correctly do it.\n\n## Test Cases [optional]\n\nTest cases for an implementation are mandatory for ADRs that are affecting consensus changes. Other ADRs can choose to include links to test cases if applicable.\n\n## References\n\n* [#9445](https://github.com/cosmos/cosmos-sdk/issues/9445) Release proto definitions v1\n* [#9446](https://github.com/cosmos/cosmos-sdk/issues/9446) Address v1beta1 proto breaking changes"
  },
  {
    "number": 45,
    "filename": "adr-045-check-delivertx-middlewares.md",
    "title": "ADR 045: BaseApp `{Check,Deliver}Tx` as Middlewares",
    "content": "# ADR 045: BaseApp `{Check,Deliver}Tx` as Middlewares\n\n## Changelog\n\n* 20.08.2021: Initial draft.\n* 07.12.2021: Update `tx.Handler` interface ([\\#10693](https://github.com/cosmos/cosmos-sdk/pull/10693)).\n* 17.05.2022: ADR is abandoned, as middlewares are deemed too hard to reason about.\n\n## Status\n\nABANDONED. Replacement is being discussed in [#11955](https://github.com/cosmos/cosmos-sdk/issues/11955).\n\n## Abstract\n\nThis ADR replaces the current BaseApp `runTx` and antehandlers design with a middleware-based design.\n\n## Context\n\nBaseApp's implementation of ABCI `{Check,Deliver}Tx()` and its own `Simulate()` method call the `runTx` method under the hood, which first runs antehandlers, then executes `Msg`s. However, the [transaction Tips](https://github.com/cosmos/cosmos-sdk/issues/9406) and [refunding unused gas](https://github.com/cosmos/cosmos-sdk/issues/2150) use cases require custom logic to be run after the `Msg`s execution. There is currently no way to achieve this.\n\nA naive solution would be to add post-`Msg` hooks to BaseApp. However, the Cosmos SDK team thinks in parallel about the bigger picture of making app wiring simpler ([#9181](https://github.com/cosmos/cosmos-sdk/discussions/9182)), which includes making BaseApp more lightweight and modular.\n\n## Decision\n\nWe decide to transform Baseapp's implementation of ABCI `{Check,Deliver}Tx` and its own `Simulate` methods to use a middleware-based design.\n\nThe two following interfaces are the base of the middleware design, and are defined in `types/tx`:\n\n```go\ntype Handler interface {\n    CheckTx(ctx context.Context, req Request, checkReq RequestCheckTx) (Response, ResponseCheckTx, error)\n    DeliverTx(ctx context.Context, req Request) (Response, error)\n    SimulateTx(ctx context.Context, req Request (Response, error)\n}\n\ntype Middleware func(Handler) Handler\n```\n\nwhere we define the following arguments and return types:\n\n```go\ntype Request struct {\n\tTx      sdk.Tx\n\tTxBytes []byte\n}\n\ntype Response struct {\n\tGasWanted uint64\n\tGasUsed   uint64\n\t// MsgResponses is an array containing each Msg service handler's response\n\t// type, packed in an Any. This will get proto-serialized into the `Data` field\n\t// in the ABCI Check/DeliverTx responses.\n\tMsgResponses []*codectypes.Any\n\tLog          string\n\tEvents       []abci.Event\n}\n\ntype RequestCheckTx struct {\n\tType abci.CheckTxType\n}\n\ntype ResponseCheckTx struct {\n\tPriority int64\n}\n```\n\nPlease note that because CheckTx handles separate logic related to mempool prioritization, its signature is different than DeliverTx and SimulateTx.\n\nBaseApp holds a reference to a `tx.Handler`:\n\n```go\ntype BaseApp  struct {\n    // other fields\n    txHandler tx.Handler\n}\n```\n\nBaseapp's ABCI `{Check,Deliver}Tx()` and `Simulate()` methods simply call `app.txHandler.{Check,Deliver,Simulate}Tx()` with the relevant arguments. For example, for `DeliverTx`:\n\n```go\nfunc (app *BaseApp) DeliverTx(req abci.RequestDeliverTx) abci.ResponseDeliverTx {\n    var abciRes abci.ResponseDeliverTx\n\tctx := app.getContextForTx(runTxModeDeliver, req.Tx)\n\tres, err := app.txHandler.DeliverTx(ctx, tx.Request{TxBytes: req.Tx})\n\tif err != nil {\n\t\tabciRes = sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\n\t\treturn abciRes\n\t}\n\n\tabciRes, err = convertTxResponseToDeliverTx(res)\n\tif err != nil {\n\t\treturn sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\n\t}\n\n\treturn abciRes\n}\n\n// convertTxResponseToDeliverTx converts a tx.Response into a abci.ResponseDeliverTx.\nfunc convertTxResponseToDeliverTx(txRes tx.Response) (abci.ResponseDeliverTx, error) {\n\tdata, err := makeABCIData(txRes)\n\tif err != nil {\n\t\treturn abci.ResponseDeliverTx{}, nil\n\t}\n\n\treturn abci.ResponseDeliverTx{\n\t\tData:   data,\n\t\tLog:    txRes.Log,\n\t\tEvents: txRes.Events,\n\t}, nil\n}\n\n// makeABCIData generates the Data field to be sent to ABCI Check/DeliverTx.\nfunc makeABCIData(txRes tx.Response) ([]byte, error) {\n\treturn proto.Marshal(&sdk.TxMsgData{MsgResponses: txRes.MsgResponses})\n}\n```\n\nThe implementations are similar for `BaseApp.CheckTx` and `BaseApp.Simulate`.\n\n`baseapp.txHandler`'s three methods' implementations can obviously be monolithic functions, but for modularity we propose a middleware composition design, where a middleware is simply a function that takes a `tx.Handler`, and returns another `tx.Handler` wrapped around the previous one.\n\n### Implementing a Middleware\n\nIn practice, middlewares are created by Go function that takes as arguments some parameters needed for the middleware, and returns a `tx.Middleware`.\n\nFor example, for creating an arbitrary `MyMiddleware`, we can implement:\n\n```go\n// myTxHandler is the tx.Handler of this middleware. Note that it holds a\n// reference to the next tx.Handler in the stack.\ntype myTxHandler struct {\n    // next is the next tx.Handler in the middleware stack.\n    next tx.Handler\n    // some other fields that are relevant to the middleware can be added here\n}\n\n// NewMyMiddleware returns a middleware that does this and that.\nfunc NewMyMiddleware(arg1, arg2) tx.Middleware {\n    return func (txh tx.Handler) tx.Handler {\n        return myTxHandler{\n            next: txh,\n            // optionally, set arg1, arg2... if they are needed in the middleware\n        }\n    }\n}\n\n// Assert myTxHandler is a tx.Handler.\nvar _ tx.Handler = myTxHandler{}\n\nfunc (h myTxHandler) CheckTx(ctx context.Context, req Request, checkReq RequestcheckTx) (Response, ResponseCheckTx, error) {\n    // CheckTx specific pre-processing logic\n\n    // run the next middleware\n    res, checkRes, err := txh.next.CheckTx(ctx, req, checkReq)\n\n    // CheckTx specific post-processing logic\n\n    return res, checkRes, err\n}\n\nfunc (h myTxHandler) DeliverTx(ctx context.Context, req Request) (Response, error) {\n    // DeliverTx specific pre-processing logic\n\n    // run the next middleware\n    res, err := txh.next.DeliverTx(ctx, tx, req)\n\n    // DeliverTx specific post-processing logic\n\n    return res, err\n}\n\nfunc (h myTxHandler) SimulateTx(ctx context.Context, req Request) (Response, error) {\n    // SimulateTx specific pre-processing logic\n\n    // run the next middleware\n    res, err := txh.next.SimulateTx(ctx, tx, req)\n\n    // SimulateTx specific post-processing logic\n\n    return res, err\n}\n```\n\n### Composing Middlewares\n\nWhile BaseApp simply holds a reference to a `tx.Handler`, this `tx.Handler` itself is defined using a middleware stack. The Cosmos SDK exposes a base (i.e. innermost) `tx.Handler` called `RunMsgsTxHandler`, which executes messages.\n\nThen, the app developer can compose multiple middlewares on top of the base `tx.Handler`. Each middleware can run pre-and-post-processing logic around its next middleware, as described in the section above. Conceptually, as an example, given the middlewares `A`, `B`, and `C` and the base `tx.Handler` `H` the stack looks like:\n\n```text\nA.pre\n    B.pre\n        C.pre\n            H # The base tx.handler, for example `RunMsgsTxHandler`\n        C.post\n    B.post\nA.post\n```\n\nWe define a `ComposeMiddlewares` function for composing middlewares. It takes the base handler as first argument, and middlewares in the \"outer to inner\" order. For the above stack, the final `tx.Handler` is:\n\n```go\ntxHandler := middleware.ComposeMiddlewares(H, A, B, C)\n```\n\nThe middleware is set in BaseApp via its `SetTxHandler` setter:\n\n```go\n// simapp/app.go\n\ntxHandler := middleware.ComposeMiddlewares(...)\napp.SetTxHandler(txHandler)\n```\n\nThe app developer can define their own middlewares, or use the Cosmos SDK's pre-defined middlewares from `middleware.NewDefaultTxHandler()`.\n\n### Middlewares Maintained by the Cosmos SDK\n\nWhile the app developer can define and compose the middlewares of their choice, the Cosmos SDK provides a set of middlewares that caters for the ecosystem's most common use cases. These middlewares are:\n\n| Middleware              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| RunMsgsTxHandler        | This is the base `tx.Handler`. It replaces the old baseapp's `runMsgs`, and executes a transaction's `Msg`s.                                                                                                                                                                                                                                                                                                                                                                             |\n| TxDecoderMiddleware     | This middleware takes in transaction raw bytes, and decodes them into a `sdk.Tx`. It replaces the `baseapp.txDecoder` field, so that BaseApp stays as thin as possible. Since most middlewares read the contents of the `sdk.Tx`, the TxDecoderMiddleware should be run first in the middleware stack.                                                                                                                                                                                   |\n| {Antehandlers}          | Each antehandler is converted to its own middleware. These middlewares perform signature verification, fee deductions and other validations on the incoming transaction.                                                                                                                                                                                                                                                                                                                 |\n| IndexEventsTxMiddleware | This is a simple middleware that chooses which events to index in Tendermint. Replaces `baseapp.indexEvents` (which unfortunately still exists in baseapp too, because it's used to index Begin/EndBlock events)                                                                                                                                                                                                                                                                         |\n| RecoveryTxMiddleware    | This index recovers from panics. It replaces baseapp.runTx's panic recovery described in [ADR-022](./adr-022-custom-panic-handling.md).                                                                                                                                                                                                                                                                                                                                                  |\n| GasTxMiddleware         | This replaces the [`Setup`](https://github.com/cosmos/cosmos-sdk/blob/v0.43.0/x/auth/ante/setup.go) Antehandler. It sets a GasMeter on sdk.Context. Note that before, GasMeter was set on sdk.Context inside the antehandlers, and there was some mess around the fact that antehandlers had their own panic recovery system so that the GasMeter could be read by baseapp's recovery system. Now, this mess is all removed: one middleware sets GasMeter, another one handles recovery. |\n\n### Similarities and Differences between Antehandlers and Middlewares\n\nThe middleware-based design builds upon the existing antehandlers design described in [ADR-010](./adr-010-modular-antehandler.md). Even though the final decision of ADR-010 was to go with the \"Simple Decorators\" approach, the middleware design is actually very similar to the other [Decorator Pattern](./adr-010-modular-antehandler.md#decorator-pattern) proposal, also used in [weave](https://github.com/iov-one/weave).\n\n#### Similarities with Antehandlers\n\n* Designed as chaining/composing small modular pieces.\n* Allow code reuse for `{Check,Deliver}Tx` and for `Simulate`.\n* Set up in `app.go`, and easily customizable by app developers.\n* Order is important.\n\n#### Differences with Antehandlers\n\n* The Antehandlers are run before `Msg` execution, whereas middlewares can run before and after.\n* The middleware approach uses separate methods for `{Check,Deliver,Simulate}Tx`, whereas the antehandlers pass a `simulate bool` flag and uses the `sdkCtx.Is{Check,Recheck}Tx()` flags to determine in which transaction mode we are.\n* The middleware design lets each middleware hold a reference to the next middleware, whereas the antehandlers pass a `next` argument in the `AnteHandle` method.\n* The middleware design use Go's standard `context.Context`, whereas the antehandlers use `sdk.Context`.\n\n## Consequences\n\n### Backwards Compatibility\n\nSince this refactor removes some logic away from BaseApp and into middlewares, it introduces API-breaking changes for app developers. Most notably, instead of creating an antehandler chain in `app.go`, app developers need to create a middleware stack:\n\n```diff\n- anteHandler, err := ante.NewAnteHandler(\n-    ante.HandlerOptions{\n-        AccountKeeper:   app.AccountKeeper,\n-        BankKeeper:      app.BankKeeper,\n-        SignModeHandler: encodingConfig.TxConfig.SignModeHandler(),\n-        FeegrantKeeper:  app.FeeGrantKeeper,\n-        SigGasConsumer:  ante.DefaultSigVerificationGasConsumer,\n-    },\n-)\n+txHandler, err := authmiddleware.NewDefaultTxHandler(authmiddleware.TxHandlerOptions{\n+    Debug:             app.Trace(),\n+    IndexEvents:       indexEvents,\n+    LegacyRouter:      app.legacyRouter,\n+    MsgServiceRouter:  app.msgSvcRouter,\n+    LegacyAnteHandler: anteHandler,\n+    TxDecoder:         encodingConfig.TxConfig.TxDecoder,\n+})\nif err != nil {\n    panic(err)\n}\n- app.SetAnteHandler(anteHandler)\n+ app.SetTxHandler(txHandler)\n```\n\nOther more minor API breaking changes will also be provided in the CHANGELOG. As usual, the Cosmos SDK will provide a release migration document for app developers.\n\nThis ADR does not introduce any state-machine-, client- or CLI-breaking changes.\n\n### Positive\n\n* Allow custom logic to be run before an after `Msg` execution. This enables the [tips](https://github.com/cosmos/cosmos-sdk/issues/9406) and [gas refund](https://github.com/cosmos/cosmos-sdk/issues/2150) uses cases, and possibly other ones.\n* Make BaseApp more lightweight, and defer complex logic to small modular components.\n* Separate paths for `{Check,Deliver,Simulate}Tx` with different returns types. This allows for improved readability (replace `if sdkCtx.IsRecheckTx() && !simulate {...}` with separate methods) and more flexibility (e.g. returning a `priority` in `ResponseCheckTx`).\n\n### Negative\n\n* It is hard to understand at first glance the state updates that would occur after a middleware runs given the `sdk.Context` and `tx`. A middleware can have an arbitrary number of nested middleware being called within its function body, each possibly doing some pre- and post-processing before calling the next middleware on the chain. Thus to understand what a middleware is doing, one must also understand what every other middleware further along the chain is also doing, and the order of middlewares matters. This can get quite complicated to understand.\n* API-breaking changes for app developers.\n\n### Neutral\n\nNo neutral consequences.\n\n## Further Discussions\n\n* [#9934](https://github.com/cosmos/cosmos-sdk/discussions/9934) Decomposing BaseApp's other ABCI methods into middlewares.\n* Replace `sdk.Tx` interface with the concrete protobuf Tx type in the `tx.Handler` methods signature.\n\n## Test Cases\n\nWe update the existing baseapp and antehandlers tests to use the new middleware API, but keep the same test cases and logic, to avoid introducing regressions. Existing CLI tests will also be left untouched.\n\nFor new middlewares, we introduce unit tests. Since middlewares are purposefully small, unit tests suit well.\n\n## References\n\n* Initial discussion: https://github.com/cosmos/cosmos-sdk/issues/9585\n* Implementation: [#9920 BaseApp refactor](https://github.com/cosmos/cosmos-sdk/pull/9920) and [#10028 Antehandlers migration](https://github.com/cosmos/cosmos-sdk/pull/10028)"
  },
  {
    "number": 46,
    "filename": "adr-046-module-params.md",
    "title": "ADR 046: Module Params",
    "content": "# ADR 046: Module Params\n\n## Changelog\n\n* Sep 22, 2021: Initial Draft\n\n## Status\n\nProposed\n\n## Abstract\n\nThis ADR describes an alternative approach to how Cosmos SDK modules use, interact,\nand store their respective parameters.\n\n## Context\n\nCurrently, in the Cosmos SDK, modules that require the use of parameters use the\n`x/params` module. The `x/params` works by having modules define parameters,\ntypically via a simple `Params` structure, and registering that structure in\nthe `x/params` module via a unique `Subspace` that belongs to the respective\nregistering module. The registering module then has unique access to its respective\n`Subspace`. Through this `Subspace`, the module can get and set its `Params`\nstructure.\n\nIn addition, the Cosmos SDK's `x/gov` module has direct support for changing\nparameters on-chain via a `ParamChangeProposal` governance proposal type, where\nstakeholders can vote on suggested parameter changes.\n\nThere are various tradeoffs to using the `x/params` module to manage individual\nmodule parameters. Namely, managing parameters essentially comes for \"free\" in\nthat developers only need to define the `Params` struct, the `Subspace`, and the\nvarious auxiliary functions, e.g. `ParamSetPairs`, on the `Params` type. However,\nthere are some notable drawbacks. These drawbacks include the fact that parameters\nare serialized in state via JSON which is extremely slow. In addition, parameter\nchanges via `ParamChangeProposal` governance proposals have no way of reading from\nor writing to state. In other words, it is currently not possible to have any\nstate transitions in the application during an attempt to change param(s).\n\n## Decision\n\nWe will build off of the alignment of `x/gov` and `x/authz` work per\n[#9810](https://github.com/cosmos/cosmos-sdk/pull/9810). Namely, module developers\nwill create one or more unique parameter data structures that must be serialized\nto state. The Param data structures must implement `sdk.Msg` interface with respective\nProtobuf Msg service method which will validate and update the parameters with all\nnecessary changes. The `x/gov` module via the work done in\n[#9810](https://github.com/cosmos/cosmos-sdk/pull/9810), will dispatch Param\nmessages, which will be handled by Protobuf Msg services.\n\nNote, it is up to developers to decide how to structure their parameters and\nthe respective `sdk.Msg` messages. Consider the parameters currently defined in\n`x/auth` using the `x/params` module for parameter management:\n\n```protobuf\nmessage Params {\n  uint64 max_memo_characters       = 1;\n  uint64 tx_sig_limit              = 2;\n  uint64 tx_size_cost_per_byte     = 3;\n  uint64 sig_verify_cost_ed25519   = 4;\n  uint64 sig_verify_cost_secp256k1 = 5;\n}\n```\n\nDevelopers can choose to either create a unique data structure for every field in\n`Params` or they can create a single `Params` structure as outlined above in the\ncase of `x/auth`.\n\nIn the former, `x/params`, approach, a `sdk.Msg` would need to be created for every single\nfield along with a handler. This can become burdensome if there are a lot of\nparameter fields. In the latter case, there is only a single data structure and\nthus only a single message handler, however, the message handler might have to be\nmore sophisticated in that it might need to understand what parameters are being\nchanged vs what parameters are untouched.\n\nParams change proposals are made using the `x/gov` module. Execution is done through\n`x/authz` authorization to the root `x/gov` module's account.\n\nContinuing to use `x/auth`, we demonstrate a more complete example:\n\n```go\ntype Params struct {\n\tMaxMemoCharacters      uint64\n\tTxSigLimit             uint64\n\tTxSizeCostPerByte      uint64\n\tSigVerifyCostED25519   uint64\n\tSigVerifyCostSecp256k1 uint64\n}\n\ntype MsgUpdateParams struct {\n\tMaxMemoCharacters      uint64\n\tTxSigLimit             uint64\n\tTxSizeCostPerByte      uint64\n\tSigVerifyCostED25519   uint64\n\tSigVerifyCostSecp256k1 uint64\n}\n\ntype MsgUpdateParamsResponse struct {}\n\nfunc (ms msgServer) UpdateParams(goCtx context.Context, msg *types.MsgUpdateParams) (*types.MsgUpdateParamsResponse, error) {\n  ctx := sdk.UnwrapSDKContext(goCtx)\n\n  // verification logic...\n\n  // persist params\n  params := ParamsFromMsg(msg)\n  ms.SaveParams(ctx, params)\n\n  return &types.MsgUpdateParamsResponse{}, nil\n}\n\nfunc ParamsFromMsg(msg *types.MsgUpdateParams) Params {\n  // ...\n}\n```\n\nA gRPC `Service` query should also be provided, for example:\n\n```protobuf\nservice Query {\n  // ...\n  \n  rpc Params(QueryParamsRequest) returns (QueryParamsResponse) {\n    option (google.api.http).get = \"/cosmos/<module>/v1beta1/params\";\n  }\n}\n\nmessage QueryParamsResponse {\n  Params params = 1 [(gogoproto.nullable) = false];\n}\n```\n\n## Consequences\n\nAs a result of implementing the module parameter methodology, we gain the ability\nfor module parameter changes to be stateful and extensible to fit nearly every\napplication's use case. We will be able to emit events (and trigger hooks registered\nto that events using the work proposed in [event hooks](https://github.com/cosmos/cosmos-sdk/discussions/9656)),\ncall other Msg service methods or perform migration.\nIn addition, there will be significant gains in performance when it comes to reading\nand writing parameters from and to state, especially if a specific set of parameters\nare read on a consistent basis.\n\nHowever, this methodology will require developers to implement more types and\nMsg service methods which can become burdensome if many parameters exist. In addition,\ndevelopers are required to implement persistence logics of module parameters.\nHowever, this should be trivial.\n\n### Backwards Compatibility\n\nThe new method for working with module parameters is naturally not backwards\ncompatible with the existing `x/params` module. However, the `x/params` will\nremain in the Cosmos SDK and will be marked as deprecated with no additional\nfunctionality being added apart from potential bug fixes. Note, the `x/params`\nmodule may be removed entirely in a future release.\n\n### Positive\n\n* Module parameters are serialized more efficiently\n* Modules are able to react on parameters changes and perform additional actions.\n* Special events can be emitted, allowing hooks to be triggered.\n\n### Negative\n\n* Module parameters become slightly more burdensome for module developers:\n    * Modules are now responsible for persisting and retrieving parameter state\n    * Modules are now required to have unique message handlers to handle parameter\n      changes per unique parameter data structure.\n\n### Neutral\n\n* Requires [#9810](https://github.com/cosmos/cosmos-sdk/pull/9810) to be reviewed\n  and merged.\n\n<!-- ## Further Discussions\n\nWhile an ADR is in the DRAFT or PROPOSED stage, this section should contain a summary of issues to be solved in future iterations (usually referencing comments from a pull-request discussion).\nLater, this section can optionally list ideas or improvements the author or reviewers found during the analysis of this ADR. -->\n\n## References\n\n* https://github.com/cosmos/cosmos-sdk/pull/9810\n* https://github.com/cosmos/cosmos-sdk/issues/9438\n* https://github.com/cosmos/cosmos-sdk/discussions/9913"
  },
  {
    "number": 47,
    "filename": "adr-047-extend-upgrade-plan.md",
    "title": "ADR 047: Extend Upgrade Plan",
    "content": "# ADR 047: Extend Upgrade Plan\n\n## Changelog\n\n* Nov, 23, 2021: Initial Draft\n* May, 16, 2023: Proposal ABANDONED. `pre_run` and `post_run` are not necessary anymore and adding the `artifacts` brings minor benefits.\n\n## Status\n\nABANDONED\n\n## Abstract\n\nThis ADR expands the existing x/upgrade `Plan` proto message to include new fields for defining pre-run and post-run processes within upgrade tooling.\nIt also defines a structure for providing downloadable artifacts involved in an upgrade.\n\n## Context\n\nThe `upgrade` module in conjunction with Cosmovisor are designed to facilitate and automate a blockchain's transition from one version to another.\n\nUsers submit a software upgrade governance proposal containing an upgrade `Plan`.\nThe [Plan](https://github.com/cosmos/cosmos-sdk/blob/v0.44.5/proto/cosmos/upgrade/v1beta1/upgrade.proto#L12) currently contains the following fields:\n\n* `name`: A short string identifying the new version.\n* `height`: The chain height at which the upgrade is to be performed.\n* `info`: A string containing information about the upgrade.\n\nThe `info` string can be anything.\nHowever, Cosmovisor will try to use the `info` field to automatically download a new version of the blockchain executable.\nFor the auto-download to work, Cosmovisor expects it to be either a stringified JSON object (with a specific structure defined through documentation), or a URL that will return such JSON.\nThe JSON object identifies URLs used to download the new blockchain executable for different platforms (OS and Architecture, e.g. \"linux/amd64\").\nSuch a URL can either return the executable file directly or can return an archive containing the executable and possibly other assets.\n\nIf the URL returns an archive, it is decompressed into `{DAEMON_HOME}/cosmovisor/{upgrade name}`.\nThen, if `{DAEMON_HOME}/cosmovisor/{upgrade name}/bin/{DAEMON_NAME}` does not exist, but `{DAEMON_HOME}/cosmovisor/{upgrade name}/{DAEMON_NAME}` does, the latter is copied to the former.\nIf the URL returns something other than an archive, it is downloaded to `{DAEMON_HOME}/cosmovisor/{upgrade name}/bin/{DAEMON_NAME}`.\n\nIf an upgrade height is reached and the new version of the executable version isn't available, Cosmovisor will stop running.\n\nBoth `DAEMON_HOME` and `DAEMON_NAME` are [environment variables used to configure Cosmovisor](https://github.com/cosmos/cosmos-sdk/blob/cosmovisor/v1.0.0/cosmovisor/README.md#command-line-arguments-and-environment-variables).\n\nCurrently, there is no mechanism that makes Cosmovisor run a command after the upgraded chain has been restarted.\n\nThe current upgrade process has this timeline:\n\n1. An upgrade governance proposal is submitted and approved.\n1. The upgrade height is reached.\n1. The `x/upgrade` module writes the `upgrade_info.json` file.\n1. The chain halts.\n1. Cosmovisor backs up the data directory (if set up to do so).\n1. Cosmovisor downloads the new executable (if not already in place).\n1. Cosmovisor executes the `${DAEMON_NAME} pre-upgrade`.\n1. Cosmovisor restarts the app using the new version and same args originally provided.\n\n## Decision\n\n### Protobuf Updates\n\nWe will update the `x/upgrade.Plan` message for providing upgrade instructions.\nThe upgrade instructions will contain a list of artifacts available for each platform.\nIt allows for the definition of a pre-run and post-run commands.\nThese commands are not consensus guaranteed; they will be executed by Cosmovisor (or other) during its upgrade handling.\n\n```protobuf\nmessage Plan {\n  // ... (existing fields)\n\n  UpgradeInstructions instructions = 6;\n}\n```\n\nThe new `UpgradeInstructions instructions` field MUST be optional.\n\n```protobuf\nmessage UpgradeInstructions {\n  string pre_run              = 1;\n  string post_run             = 2;\n  repeated Artifact artifacts = 3;\n  string description          = 4;\n}\n```\n\nAll fields in the `UpgradeInstructions` are optional.\n\n* `pre_run` is a command to run prior to the upgraded chain restarting.\n  If defined, it will be executed after halting and downloading the new artifact but before restarting the upgraded chain.\n  The working directory this command runs from MUST be `{DAEMON_HOME}/cosmovisor/{upgrade name}`.\n  This command MUST behave the same as the current [pre-upgrade](https://github.com/cosmos/cosmos-sdk/blob/v0.44.5/docs/migrations/pre-upgrade.md) command.\n  It does not take in any command-line arguments and is expected to terminate with the following exit codes:\n\n  | Exit status code | How it is handled in Cosmovisor                                                                                    |\n  |------------------|---------------------------------------------------------------------------------------------------------------------|\n  | `0`              | Assumes `pre-upgrade` command executed successfully and continues the upgrade.                                      |\n  | `1`              | Default exit code when `pre-upgrade` command has not been implemented.                                              |\n  | `30`             | `pre-upgrade` command was executed but failed. This fails the entire upgrade.                                       |\n  | `31`             | `pre-upgrade` command was executed but failed. But the command is retried until exit code `1` or `30` are returned. |\n  If defined, then the app supervisors (e.g. Cosmovisor) MUST NOT run `app pre-run`.\n\n* `post_run` is a command to run after the upgraded chain has been started. If defined, this command MUST be only executed at most once by an upgrading node.\n  The output and exit code SHOULD be logged but SHOULD NOT affect the running of the upgraded chain.\n  The working directory this command runs from MUST be `{DAEMON_HOME}/cosmovisor/{upgrade name}`.\n* `artifacts` define items to be downloaded.\n  It SHOULD have only one entry per platform.\n* `description` contains human-readable information about the upgrade and might contain references to external resources.\n  It SHOULD NOT be used for structured processing information.\n\n```protobuf\nmessage Artifact {\n  string platform      = 1;\n  string url           = 2;\n  string checksum      = 3;\n  string checksum_algo = 4;\n}\n```\n\n* `platform` is a required string that SHOULD be in the format `{OS}/{CPU}`, e.g. `\"linux/amd64\"`.\n  The string `\"any\"` SHOULD also be allowed.\n  An `Artifact` with a `platform` of `\"any\"` SHOULD be used as a fallback when a specific `{OS}/{CPU}` entry is not found.\n  That is, if an `Artifact` exists with a `platform` that matches the system's OS and CPU, that should be used;\n  otherwise, if an `Artifact` exists with a `platform` of `any`, that should be used;\n  otherwise no artifact should be downloaded.\n* `url` is a required URL string that MUST conform to [RFC 1738: Uniform Resource Locators](https://www.ietf.org/rfc/rfc1738.txt).\n  A request to this `url` MUST return either an executable file or an archive containing either `bin/{DAEMON_NAME}` or `{DAEMON_NAME}`.\n  The URL should not contain checksum - it should be specified by the `checksum` attribute.\n* `checksum` is a checksum of the expected result of a request to the `url`.\n  It is not required, but is recommended.\n  If provided, it MUST be a hex encoded checksum string.\n  Tools utilizing these `UpgradeInstructions` MUST fail if a `checksum` is provided but is different from the checksum of the result returned by the `url`.\n* `checksum_algo` is a string identifying the algorithm used to generate the `checksum`.\n  Recommended algorithms: `sha256`, `sha512`.\n  Algorithms also supported (but not recommended): `sha1`, `md5`.\n  If a `checksum` is provided, a `checksum_algo` MUST also be provided.\n\nA `url` is not required to contain a `checksum` query parameter.\nIf the `url` does contain a `checksum` query parameter, the `checksum` and `checksum_algo` fields MUST also be populated, and their values MUST match the value of the query parameter.\nFor example, if the `url` is `\"https://example.com?checksum=md5:d41d8cd98f00b204e9800998ecf8427e\"`, then the `checksum` field must be `\"d41d8cd98f00b204e9800998ecf8427e\"` and the `checksum_algo` field must be `\"md5\"`.\n\n### Upgrade Module Updates\n\nIf an upgrade `Plan` does not use the new `UpgradeInstructions` field, existing functionality will be maintained.\nThe parsing of the `info` field as either a URL or `binaries` JSON will be deprecated.\nDuring validation, if the `info` field is used as such, a warning will be issued, but not an error.\n\nWe will update the creation of the `upgrade-info.json` file to include the `UpgradeInstructions`.\n\nWe will update the optional validation available via CLI to account for the new `Plan` structure.\nWe will add the following validation:\n\n1.  If `UpgradeInstructions` are provided:\n    1.  There MUST be at least one entry in `artifacts`.\n    1.  All of the `artifacts` MUST have a unique `platform`.\n    1.  For each `Artifact`, if the `url` contains a `checksum` query parameter:\n        1. The `checksum` query parameter value MUST be in the format of `{checksum_algo}:{checksum}`.\n        1. The `{checksum}` from the query parameter MUST equal the `checksum` provided in the `Artifact`.\n        1. The `{checksum_algo}` from the query parameter MUST equal the `checksum_algo` provided in the `Artifact`.\n1.  The following validation is currently done using the `info` field. We will apply similar validation to the `UpgradeInstructions`.\n    For each `Artifact`:\n    1.  The `platform` MUST have the format `{OS}/{CPU}` or be `\"any\"`.\n    1.  The `url` field MUST NOT be empty.\n    1.  The `url` field MUST be a proper URL.\n    1.  A `checksum` MUST be provided either in the `checksum` field or as a query parameter in the `url`.\n    1.  If the `checksum` field has a value and the `url` also has a `checksum` query parameter, the two values MUST be equal.\n    1.  The `url` MUST return either a file or an archive containing either `bin/{DAEMON_NAME}` or `{DAEMON_NAME}`.\n    1.  If a `checksum` is provided (in the field or as a query param), the checksum of the result of the `url` MUST equal the provided checksum.\n\nDownloading of an `Artifact` will happen the same way that URLs from `info` are currently downloaded.\n\n### Cosmovisor Updates\n\nIf the `upgrade-info.json` file does not contain any `UpgradeInstructions`, existing functionality will be maintained.\n\nWe will update Cosmovisor to look for and handle the new `UpgradeInstructions` in `upgrade-info.json`.\nIf the `UpgradeInstructions` are provided, we will do the following:\n\n1.  The `info` field will be ignored.\n1.  The `artifacts` field will be used to identify the artifact to download based on the `platform` that Cosmovisor is running in.\n1.  If a `checksum` is provided (either in the field or as a query param in the `url`), and the downloaded artifact has a different checksum, the upgrade process will be interrupted and Cosmovisor will exit with an error.\n1.  If a `pre_run` command is defined, it will be executed at the same point in the process where the `app pre-upgrade` command would have been executed.\n    It will be executed using the same environment as other commands run by Cosmovisor.\n1.  If a `post_run` command is defined, it will be executed after executing the command that restarts the chain.\n    It will be executed in a background process using the same environment as the other commands.\n    Any output generated by the command will be logged.\n    Once complete, the exit code will be logged.\n\nWe will deprecate the use of the `info` field for anything other than human readable information.\nA warning will be logged if the `info` field is used to define the assets (either by URL or JSON).\n\nThe new upgrade timeline is very similar to the current one. Changes are in bold:\n\n1. An upgrade governance proposal is submitted and approved.\n1. The upgrade height is reached.\n1. The `x/upgrade` module writes the `upgrade_info.json` file **(now possibly with `UpgradeInstructions`)**.\n1. The chain halts.\n1. Cosmovisor backs up the data directory (if set up to do so).\n1. Cosmovisor downloads the new executable (if not already in place).\n1. Cosmovisor executes **the `pre_run` command if provided**, or else the `${DAEMON_NAME} pre-upgrade` command.\n1. Cosmovisor restarts the app using the new version and same args originally provided.\n1. **Cosmovisor immediately runs the `post_run` command in a detached process.**\n\n## Consequences\n\n### Backwards Compatibility\n\nSince the only change to existing definitions is the addition of the `instructions` field to the `Plan` message, and that field is optional, there are no backwards incompatibilities with respects to the proto messages.\nAdditionally, current behavior will be maintained when no `UpgradeInstructions` are provided, so there are no backwards incompatibilities with respects to either the upgrade module or Cosmovisor.\n\n### Forwards Compatibility\n\nIn order to utilize the `UpgradeInstructions` as part of a software upgrade, both of the following must be true:\n\n1.  The chain must already be using a sufficiently advanced version of the Cosmos SDK.\n1.  The chain's nodes must be using a sufficiently advanced version of Cosmovisor.\n\n### Positive\n\n1.  The structure for defining artifacts is clearer since it is now defined in the proto instead of in documentation.\n1.  Availability of a pre-run command becomes more obvious.\n1.  A post-run command becomes possible.\n\n### Negative\n\n1.  The `Plan` message becomes larger. This is negligible because A) the `x/upgrades` module only stores at most one upgrade plan, and B) upgrades are rare enough that the increased gas cost isn't a concern.\n1.  There is no option for providing a URL that will return the `UpgradeInstructions`.\n1.  The only way to provide multiple assets (executables and other files) for a platform is to use an archive as the platform's artifact.\n\n### Neutral\n\n1. Existing functionality of the `info` field is maintained when the `UpgradeInstructions` aren't provided.\n\n## Further Discussions\n\n1.  [Draft PR #10032 Comment](https://github.com/cosmos/cosmos-sdk/pull/10032/files?authenticity_token=pLtzpnXJJB%2Fif2UWiTp9Td3MvRrBF04DvjSuEjf1azoWdLF%2BSNymVYw9Ic7VkqHgNLhNj6iq9bHQYnVLzMXd4g%3D%3D&file-filters%5B%5D=.go&file-filters%5B%5D=.proto#r698708349):\n    Consider different names for `UpgradeInstructions instructions` (either the message type or field name).\n1.  [Draft PR #10032 Comment](https://github.com/cosmos/cosmos-sdk/pull/10032/files?authenticity_token=pLtzpnXJJB%2Fif2UWiTp9Td3MvRrBF04DvjSuEjf1azoWdLF%2BSNymVYw9Ic7VkqHgNLhNj6iq9bHQYnVLzMXd4g%3D%3D&file-filters%5B%5D=.go&file-filters%5B%5D=.proto#r754655072):\n    1.  Consider putting the `string platform` field inside `UpgradeInstructions` and make `UpgradeInstructions` a repeated field in `Plan`.\n    1.  Consider using a `oneof` field in the `Plan` which could either be `UpgradeInstructions` or else a URL that should return the `UpgradeInstructions`.\n    1.  Consider allowing `info` to either be a JSON serialized version of `UpgradeInstructions` or else a URL that returns that.\n1.  [Draft PR #10032 Comment](https://github.com/cosmos/cosmos-sdk/pull/10032/files?authenticity_token=pLtzpnXJJB%2Fif2UWiTp9Td3MvRrBF04DvjSuEjf1azoWdLF%2BSNymVYw9Ic7VkqHgNLhNj6iq9bHQYnVLzMXd4g%3D%3D&file-filters%5B%5D=.go&file-filters%5B%5D=.proto#r755462876):\n    Consider not including the `UpgradeInstructions.description` field, using the `info` field for that purpose instead.\n1.  [Draft PR #10032 Comment](https://github.com/cosmos/cosmos-sdk/pull/10032/files?authenticity_token=pLtzpnXJJB%2Fif2UWiTp9Td3MvRrBF04DvjSuEjf1azoWdLF%2BSNymVYw9Ic7VkqHgNLhNj6iq9bHQYnVLzMXd4g%3D%3D&file-filters%5B%5D=.go&file-filters%5B%5D=.proto#r754643691):\n    Consider allowing multiple artifacts to be downloaded for any given `platform` by adding a `name` field to the `Artifact` message.\n1.  [PR #10502 Comment](https://github.com/cosmos/cosmos-sdk/pull/10602#discussion_r781438288)\n    Allow the new `UpgradeInstructions` to be provided via URL.\n1.  [PR #10502 Comment](https://github.com/cosmos/cosmos-sdk/pull/10602#discussion_r781438288)\n    Allow definition of a `signer` for assets (as an alternative to using a `checksum`).\n\n## References\n\n* [Current upgrade.proto](https://github.com/cosmos/cosmos-sdk/blob/v0.44.5/proto/cosmos/upgrade/v1beta1/upgrade.proto)\n* [Upgrade Module README](https://github.com/cosmos/cosmos-sdk/blob/v0.44.5/x/upgrade/spec/README.md)\n* [Cosmovisor README](https://github.com/cosmos/cosmos-sdk/blob/cosmovisor/v1.0.0/cosmovisor/README.md)\n* [Pre-upgrade README](https://github.com/cosmos/cosmos-sdk/blob/v0.44.5/docs/migrations/pre-upgrade.md)\n* [Draft/POC PR #10032](https://github.com/cosmos/cosmos-sdk/pull/10032)\n* [RFC 1738: Uniform Resource Locators](https://www.ietf.org/rfc/rfc1738.txt)"
  },
  {
    "number": 48,
    "filename": "adr-048-consensus-fees.md",
    "title": "ADR 048: Multi Tier Gas Price System",
    "content": "# ADR 048: Multi Tier Gas Price System\n\n## Changelog\n\n* Dec 1, 2021: Initial Draft\n\n## Status\n\nRejected\n\n## Abstract\n\nThis ADR describes a flexible mechanism to maintain a consensus level gas prices, in which one can choose a multi-tier gas price system or EIP-1559 like one through configuration.\n\n## Context\n\nCurrently, each validator configures it's own `minimal-gas-prices` in `app.yaml`. But setting a proper minimal gas price is critical to protect network from dos attack, and it's hard for all the validators to pick a sensible value, so we propose to maintain a gas price in consensus level.\n\nSince tendermint 0.34.20 has supported mempool prioritization, we can take advantage of that to implement more sophisticated gas fee system.\n\n## Multi-Tier Price System\n\nWe propose a multi-tier price system on consensus to provide maximum flexibility:\n\n* Tier 1: a constant gas price, which could only be modified occasionally through governance proposal.\n* Tier 2: a dynamic gas price which is adjusted according to previous block load.\n* Tier 3: a dynamic gas price which is adjusted according to previous block load at a higher speed.\n\nThe gas price of higher tier should be bigger than the lower tier.\n\nThe transaction fees are charged with the exact gas price calculated on consensus.\n\nThe parameter schema is like this:\n\n```protobuf\nmessage TierParams {\n  uint32 priority = 1           // priority in tendermint mempool\n  Coin initial_gas_price = 2    //\n  uint32 parent_gas_target = 3  // the target saturation of block\n  uint32 change_denominator = 4 // decides the change speed\n  Coin min_gas_price = 5        // optional lower bound of the price adjustment\n  Coin max_gas_price = 6        // optional upper bound of the price adjustment\n}\n\nmessage Params {\n  repeated TierParams tiers = 1;\n}\n```\n\n### Extension Options\n\nWe need to allow user to specify the tier of service for the transaction, to support it in an extensible way, we add an extension option in `AuthInfo`:\n\n```protobuf\nmessage ExtensionOptionsTieredTx {\n  uint32 fee_tier = 1\n}\n```\n\nThe value of `fee_tier` is just the index to the `tiers` parameter list.\n\nWe also change the semantic of existing `fee` field of `Tx`, instead of charging user the exact `fee` amount, we treat it as a fee cap, while the actual amount of fee charged is decided dynamically. If the `fee` is smaller than dynamic one, the transaction won't be included in current block and ideally should stay in the mempool until the consensus gas price drop. The mempool can eventually prune old transactions.\n\n### Tx Prioritization\n\nTransactions are prioritized based on the tier, the higher the tier, the higher the priority.\n\nWithin the same tier, follow the default Tendermint order (currently FIFO). Be aware of that the mempool tx ordering logic is not part of consensus and can be modified by malicious validator.\n\nThis mechanism can be easily composed with prioritization mechanisms:\n\n* we can add extra tiers out of a user control:\n    * Example 1: user can set tier 0, 10 or 20, but the protocol will create tiers 0, 1, 2 ... 29. For example IBC transactions will go to tier `user_tier + 5`: if user selected tier 1, then the transaction will go to tier 15.\n    * Example 2: we can reserve tier 4, 5, ... only for special transaction types. For example, tier 5 is reserved for evidence tx. So if submits a bank.Send transaction and set tier 5, it will be delegated to tier 3 (the max tier level available for any transaction). \n    * Example 3: we can enforce that all transactions of a specific type will go to specific tier. For example, tier 100 will be reserved for evidence transactions and all evidence transactions will always go to that tier.\n\n### `min-gas-prices`\n\nDeprecate the current per-validator `min-gas-prices` configuration, since it would confusing for it to work together with the consensus gas price.\n\n### Adjust For Block Load\n\nFor tier 2 and tier 3 transactions, the gas price is adjusted according to previous block load, the logic could be similar to EIP-1559:\n\n```python\ndef adjust_gas_price(gas_price, parent_gas_used, tier):\n  if parent_gas_used == tier.parent_gas_target:\n    return gas_price\n  elif parent_gas_used > tier.parent_gas_target:\n    gas_used_delta = parent_gas_used - tier.parent_gas_target\n    gas_price_delta = max(gas_price * gas_used_delta // tier.parent_gas_target // tier.change_speed, 1)\n    return gas_price + gas_price_delta\n  else:\n    gas_used_delta = parent_gas_target - parent_gas_used\n    gas_price_delta = gas_price * gas_used_delta // parent_gas_target // tier.change_speed\n    return gas_price - gas_price_delta\n```\n\n### Block Segment Reservation\n\nIdeally we should reserve block segments for each tier, so the lower tiered transactions won't be completely squeezed out by higher tier transactions, which will force user to use higher tier, and the system degraded to a single tier.\n\nWe need help from tendermint to implement this.\n\n## Implementation\n\nWe can make each tier's gas price strategy fully configurable in protocol parameters, while providing a sensible default one.\n\nPseudocode in python-like syntax:\n\n```python\ninterface TieredTx:\n  def tier(self) -> int:\n    pass\n\ndef tx_tier(tx):\n    if isinstance(tx, TieredTx):\n      return tx.tier()\n    else:\n      # default tier for custom transactions\n      return 0\n    # NOTE: we can add more rules here per \"Tx Prioritization\" section \n\nclass TierParams:\n  'gas price strategy parameters of one tier'\n  priority: int           # priority in tendermint mempool\n  initial_gas_price: Coin\n  parent_gas_target: int\n  change_speed: Decimal   # 0 means don't adjust for block load.\n\nclass Params:\n    'protocol parameters'\n    tiers: List[TierParams]\n\nclass State:\n    'consensus state'\n    # total gas used in last block, None when it's the first block\n    parent_gas_used: Optional[int]\n    # gas prices of last block for all tiers\n    gas_prices: List[Coin]\n\ndef begin_block():\n    'Adjust gas prices'\n    for i, tier in enumerate(Params.tiers):\n        if State.parent_gas_used is None:\n            # initialized gas price for the first block\n\t          State.gas_prices[i] = tier.initial_gas_price\n        else:\n            # adjust gas price according to gas used in previous block\n            State.gas_prices[i] = adjust_gas_price(State.gas_prices[i], State.parent_gas_used, tier)\n\ndef mempoolFeeTxHandler_checkTx(ctx, tx):\n    # the minimal-gas-price configured by validator, zero in deliver_tx context\n    validator_price = ctx.MinGasPrice()\n    consensus_price = State.gas_prices[tx_tier(tx)]\n    min_price = max(validator_price, consensus_price)\n\n    # zero means infinity for gas price cap\n    if tx.gas_price() > 0 and tx.gas_price() < min_price:\n        return 'insufficient fees'\n    return next_CheckTx(ctx, tx)\n\ndef txPriorityHandler_checkTx(ctx, tx):\n    res, err := next_CheckTx(ctx, tx)\n    # pass priority to tendermint\n    res.Priority = Params.tiers[tx_tier(tx)].priority\n    return res, err\n\ndef end_block():\n    'Update block gas used'\n    State.parent_gas_used = block_gas_meter.consumed()\n```\n\n### Dos attack protection\n\nTo fully saturate the blocks and prevent other transactions from executing, attacker need to use transactions of highest tier, the cost would be significantly higher than the default tier.\n\nIf attacker spam with lower tier transactions, user can mitigate by sending higher tier transactions.\n\n## Consequences\n\n### Backwards Compatibility\n\n* New protocol parameters.\n* New consensus states.\n* New/changed fields in transaction body.\n\n### Positive\n\n* The default tier keeps the same predictable gas price experience for client.\n* The higher tier's gas price can adapt to block load.\n* No priority conflict with custom priority based on transaction types, since this proposal only occupy three priority levels.\n* Possibility to compose different priority rules with tiers\n\n### Negative\n\n* Wallets & tools need to update to support the new `tier` parameter, and semantic of `fee` field is changed.\n\n### Neutral\n\n## References\n\n* https://eips.ethereum.org/EIPS/eip-1559\n* https://iohk.io/en/blog/posts/2021/11/26/network-traffic-and-tiered-pricing/"
  },
  {
    "number": 49,
    "filename": "adr-049-state-sync-hooks.md",
    "title": "ADR 049: State Sync Hooks",
    "content": "# ADR 049: State Sync Hooks\n\n## Changelog\n\n* Jan 19, 2022: Initial Draft\n* Apr 29, 2022: Safer extension snapshotter interface\n\n## Status\n\nImplemented\n\n## Abstract\n\nThis ADR outlines a hooks-based mechanism for application modules to provide additional state (outside of the IAVL tree) to be used \nduring state sync.\n\n## Context\n\nNew clients use state-sync to download snapshots of module state from peers. Currently, the snapshot consists of a\nstream of `SnapshotStoreItem` and `SnapshotIAVLItem`, which means that application modules that define their state outside of the IAVL \ntree cannot include their state as part of the state-sync process.\n\nNote, Even though the module state data is outside of the tree, for determinism we require that the hash of the external data should \nbe posted in the IAVL tree.\n\n## Decision\n\nA simple proposal based on our existing implementation is that, we can add two new message types: `SnapshotExtensionMeta` \nand `SnapshotExtensionPayload`, and they are appended to the existing multi-store stream with `SnapshotExtensionMeta` \nacting as a delimiter between extensions. As the chunk hashes should be able to ensure data integrity, we don't need \na delimiter to mark the end of the snapshot stream.\n\nBesides, we provide `Snapshotter` and `ExtensionSnapshotter` interface for modules to implement snapshotters, which will handle both taking \nsnapshot and the restoration. Each module could have multiple snapshotters, and for modules with additional state, they should\nimplement `ExtensionSnapshotter` as extension snapshotters. When setting up the application, the snapshot `Manager` should call \n`RegisterExtensions([]ExtensionSnapshotter…)` to register all the extension snapshotters.\n\n```protobuf\n// SnapshotItem is an item contained in a rootmulti.Store snapshot.\n// On top of the existing SnapshotStoreItem and SnapshotIAVLItem, we add two new options for the item.\nmessage SnapshotItem {\n  // item is the specific type of snapshot item.\n  oneof item {\n    SnapshotStoreItem        store             = 1;\n    SnapshotIAVLItem         iavl              = 2 [(gogoproto.customname) = \"IAVL\"];\n    SnapshotExtensionMeta    extension         = 3;\n    SnapshotExtensionPayload extension_payload = 4;\n  }\n}\n\n// SnapshotExtensionMeta contains metadata about an external snapshotter.\n// One module may need multiple snapshotters, so each module may have multiple SnapshotExtensionMeta.\nmessage SnapshotExtensionMeta {\n  // the name of the ExtensionSnapshotter, and it is registered to snapshotter manager when setting up the application\n  // name should be unique for each ExtensionSnapshotter as we need to alphabetically order their snapshots to get\n  // deterministic snapshot stream.\n  string name   = 1;\n  // this is used by each ExtensionSnapshotter to decide the format of payloads included in SnapshotExtensionPayload message\n  // it is used within the snapshotter/namespace, not global one for all modules\n  uint32 format = 2;\n}\n\n// SnapshotExtensionPayload contains payloads of an external snapshotter.\nmessage SnapshotExtensionPayload {\n  bytes payload = 1;\n}\n```\n\nWhen we create a snapshot stream, the `multistore` snapshot is always placed at the beginning of the binary stream, and other extension snapshots are alphabetically ordered by the name of the corresponding `ExtensionSnapshotter`. \n\nThe snapshot stream would look like as follows:\n\n```go\n// multi-store snapshot\n{SnapshotStoreItem | SnapshotIAVLItem, ...}\n// extension1 snapshot\nSnapshotExtensionMeta\n{SnapshotExtensionPayload, ...}\n// extension2 snapshot\nSnapshotExtensionMeta\n{SnapshotExtensionPayload, ...}\n```\n\nWe add an `extensions` field to snapshot `Manager` for extension snapshotters. The `multistore` snapshotter is a special one and it doesn't need a name because it is always placed at the beginning of the binary stream.\n\n```go\ntype Manager struct {\n\tstore      *Store\n\tmultistore types.Snapshotter\n\textensions map[string]types.ExtensionSnapshotter\n\tmtx                sync.Mutex\n\toperation          operation\n\tchRestore          chan<- io.ReadCloser\n\tchRestoreDone      <-chan restoreDone\n\trestoreChunkHashes [][]byte\n\trestoreChunkIndex  uint32\n}\n```\n\nFor extension snapshotters that implement the `ExtensionSnapshotter` interface, their names should be registered to the snapshot `Manager` by \ncalling `RegisterExtensions` when setting up the application. The snapshotters will handle both taking snapshot and restoration.\n\n```go\n// RegisterExtensions register extension snapshotters to manager\nfunc (m *Manager) RegisterExtensions(extensions ...types.ExtensionSnapshotter) error \n```\n\nOn top of the existing `Snapshotter` interface for the `multistore`, we add `ExtensionSnapshotter` interface for the extension snapshotters. Three more function signatures: `SnapshotFormat()`, `SupportedFormats()` and `SnapshotName()` are added to `ExtensionSnapshotter`.\n\n```go\n// ExtensionPayloadReader read extension payloads,\n// it returns io.EOF when reached either end of stream or the extension boundaries.\ntype ExtensionPayloadReader = func() ([]byte, error)\n\n// ExtensionPayloadWriter is a helper to write extension payloads to underlying stream.\ntype ExtensionPayloadWriter = func([]byte) error\n\n// ExtensionSnapshotter is an extension Snapshotter that is appended to the snapshot stream.\n// ExtensionSnapshotter has an unique name and manages it's own internal formats.\ntype ExtensionSnapshotter interface {\n\t// SnapshotName returns the name of snapshotter, it should be unique in the manager.\n\tSnapshotName() string\n\n\t// SnapshotFormat returns the default format used to take a snapshot.\n\tSnapshotFormat() uint32\n\n\t// SupportedFormats returns a list of formats it can restore from.\n\tSupportedFormats() []uint32\n\n\t// SnapshotExtension writes extension payloads into the underlying protobuf stream.\n\tSnapshotExtension(height uint64, payloadWriter ExtensionPayloadWriter) error\n\n\t// RestoreExtension restores an extension state snapshot,\n\t// the payload reader returns `io.EOF` when reached the extension boundaries.\n\tRestoreExtension(height uint64, format uint32, payloadReader ExtensionPayloadReader) error\n\n}\n```\n\n## Consequences\n\nAs a result of this implementation, we are able to create snapshots of binary chunk stream for the state that we maintain outside of the IAVL Tree, CosmWasm blobs for example. And new clients are able to fetch snapshots of state for all modules that have implemented the corresponding interface from peer nodes. \n\n\n### Backwards Compatibility\n\nThis ADR introduces new proto message types, adds an `extensions` field in snapshot `Manager`, and add new `ExtensionSnapshotter` interface, so this is not backwards compatible if we have extensions.\n\nBut for applications that do not have the state data outside of the IAVL tree for any module, the snapshot stream is backwards-compatible.\n\n### Positive\n\n* State maintained outside of IAVL tree like CosmWasm blobs can create snapshots by implementing extension snapshotters, and being fetched by new clients via state-sync.\n\n### Negative\n\n### Neutral\n\n* All modules that maintain state outside of IAVL tree need to implement `ExtensionSnapshotter` and the snapshot `Manager` need to call `RegisterExtensions` when setting up the application.\n\n## Further Discussions\n\nWhile an ADR is in the DRAFT or PROPOSED stage, this section should contain a summary of issues to be solved in future iterations (usually referencing comments from a pull-request discussion).\nLater, this section can optionally list ideas or improvements the author or reviewers found during the analysis of this ADR.\n\n## Test Cases [optional]\n\nTest cases for an implementation are mandatory for ADRs that are affecting consensus changes. Other ADRs can choose to include links to test cases if applicable.\n\n## References\n\n* https://github.com/cosmos/cosmos-sdk/pull/10961\n* https://github.com/cosmos/cosmos-sdk/issues/7340\n* https://hackmd.io/gJoyev6DSmqqkO667WQlGw"
  },
  {
    "number": 50,
    "filename": "adr-050-sign-mode-textual.md",
    "title": "ADR 050: SIGN_MODE_TEXTUAL",
    "content": "# ADR 050: SIGN_MODE_TEXTUAL\n\n## Changelog\n\n* Dec 06, 2021: Initial Draft.\n* Feb 07, 2022: Draft read and concept-ACKed by the Ledger team.\n* May 16, 2022: Change status to Accepted.\n* Aug 11, 2022: Require signing over tx raw bytes.\n* Sep 07, 2022: Add custom `Msg`-renderers.\n* Sep 18, 2022: Structured format instead of lines of text\n* Nov 23, 2022: Specify CBOR encoding.\n* Dec 01, 2022: Link to examples in separate JSON file.\n* Dec 06, 2022: Re-ordering of envelope screens.\n* Dec 14, 2022: Mention exceptions for invertibility.\n* Jan 23, 2023: Switch Screen.Text to Title+Content.\n* Mar 07, 2023: Change SignDoc from array to struct containing array.\n* Mar 20, 2023: Introduce a spec version initialized to 0.\n\n## Status\n\nAccepted. Implementation started. Small value renderers details still need to be polished.\n\nSpec version: 0.\n\n## Abstract\n\nThis ADR specifies SIGN_MODE_TEXTUAL, a new string-based sign mode that is targeted at signing with hardware devices.\n\n## Context\n\nProtobuf-based SIGN_MODE_DIRECT was introduced in [ADR-020](./adr-020-protobuf-transaction-encoding.md) and is intended to replace SIGN_MODE_LEGACY_AMINO_JSON in most situations, such as mobile wallets and CLI keyrings. However, the [Ledger](https://www.ledger.com/) hardware wallet is still using SIGN_MODE_LEGACY_AMINO_JSON for displaying the sign bytes to the user. Hardware wallets cannot transition to SIGN_MODE_DIRECT as:\n\n* SIGN_MODE_DIRECT is binary-based and thus not suitable for display to end-users. Technically, hardware wallets could simply display the sign bytes to the user. But this would be considered as blind signing, and is a security concern.\n* hardware cannot decode the protobuf sign bytes due to memory constraints, as the Protobuf definitions would need to be embedded on the hardware device.\n\nIn an effort to remove Amino from the SDK, a new sign mode needs to be created for hardware devices. [Initial discussions](https://github.com/cosmos/cosmos-sdk/issues/6513) propose a text-based sign mode, which this ADR formally specifies.\n\n## Decision\n\nIn SIGN_MODE_TEXTUAL, a transaction is rendered into a textual representation,\nwhich is then sent to a secure device or subsystem for the user to review and sign.\nUnlike `SIGN_MODE_DIRECT`, the transmitted data can be simply decoded into legible text\neven on devices with limited processing and display.\n\nThe textual representation is a sequence of _screens_.\nEach screen is meant to be displayed in its entirety (if possible) even on a small device like a Ledger.\nA screen is roughly equivalent to a short line of text.\nLarge screens can be displayed in several pieces,\nmuch as long lines of text are wrapped,\nso no hard guidance is given, though 40 characters is a good target.\nA screen is used to display a single key/value pair for scalar values\n(or composite values with a compact notation, such as `Coins`)\nor to introduce or conclude a larger grouping.\n\nThe text can contain the full range of Unicode code points, including control characters and nul.\nThe device is responsible for deciding how to display characters it cannot render natively.\nSee [annex 2](./adr-050-sign-mode-textual-annex2.md) for guidance.\n\nScreens have a non-negative indentation level to signal composite or nested structures.\nIndentation level zero is the top level.\nIndentation is displayed via some device-specific mechanism.\nMessage quotation notation is an appropriate model, such as\nleading `>` characters or vertical bars on more capable displays.\n\nSome screens are marked as _expert_ screens,\nmeant to be displayed only if the viewer chooses to opt in for the extra detail.\nExpert screens are meant for information that is rarely useful,\nor needs to be present only for signature integrity (see below).\n\n### Invertible Rendering\n\nWe require that the rendering of the transaction be invertible:\nthere must be a parsing function such that for every transaction,\nwhen rendered to the textual representation,\nparsing that representation yields a proto message equivalent\nto the original under proto equality.\n\nNote that this inverse function does not need to perform correct\nparsing or error signaling for the whole domain of textual data.\nMerely that the range of valid transactions be invertible under\nthe composition of rendering and parsing.\n\nNote that the existence of an inverse function ensures that the\nrendered text contains the full information of the original transaction,\nnot a hash or subset.\n\nWe make an exception for invertibility for data which are too large to\nmeaningfully display, such as byte strings longer than 32 bytes. We may then\nselectively render them with a cryptographically-strong hash. In these cases,\nit is still computationally infeasible to find a different transaction which\nhas the same rendering. However, we must ensure that the hash computation is\nsimple enough to be reliably executed independently, so at least the hash is\nitself reasonably verifiable when the raw byte string is not.\n\n### Chain State\n\nThe rendering function (and parsing function) may depend on the current chain state.\nThis is useful for reading parameters, such as coin display metadata,\nor for reading user-specific preferences such as language or address aliases.\nNote that if the observed state changes between signature generation\nand the transaction's inclusion in a block, the delivery-time rendering\nmight differ. If so, the signature will be invalid and the transaction\nwill be rejected.\n\n### Signature and Security\n\nFor security, transaction signatures should have three properties:\n\n1. Given the transaction, signatures, and chain state, it must be possible to validate that the signatures matches the transaction,\nto verify that the signers must have known their respective secret keys.\n\n2. It must be computationally infeasible to find a substantially different transaction for which the given signatures are valid, given the same chain state.\n\n3. The user should be able to give informed consent to the signed data via a simple, secure device with limited display capabilities.\n\nThe correctness and security of `SIGN_MODE_TEXTUAL` is guaranteed by demonstrating an inverse function from the rendering to transaction protos.\nThis means that it is impossible for a different protocol buffer message to render to the same text.\n\n### Transaction Hash Malleability\n\nWhen client software forms a transaction, the \"raw\" transaction (`TxRaw`) is serialized as a proto\nand a hash of the resulting byte sequence is computed.\nThis is the `TxHash`, and is used by various services to track the submitted transaction through its lifecycle.\nVarious misbehavior is possible if one can generate a modified transaction with a different TxHash\nbut for which the signature still checks out.\n\nSIGN_MODE_TEXTUAL prevents this transaction malleability by including the TxHash as an expert screen\nin the rendering.\n\n### SignDoc\n\nThe SignDoc for `SIGN_MODE_TEXTUAL` is formed from a data structure like:\n\n```go\ntype Screen struct {\n  Title string   // possibly size limited to, advised to 64 characters\n  Content string // possibly size limited to, advised to 255 characters\n  Indent uint8   // size limited to something small like 16 or 32\n  Expert bool\n}\n\ntype SignDocTextual struct {\n  Screens []Screen\n}\n```\n\nWe do not plan to use protobuf serialization to form the sequence of bytes\nthat will be transmitted and signed, in order to keep the decoder simple.\nWe will use [CBOR](https://cbor.io) ([RFC 8949](https://www.rfc-editor.org/rfc/rfc8949.html)) instead.\nThe encoding is defined by the following CDDL ([RFC 8610](https://www.rfc-editor.org/rfc/rfc8610)):\n\n```\n;;; CDDL (RFC 8610) Specification of SignDoc for SIGN_MODE_TEXTUAL.\n;;; Must be encoded using CBOR deterministic encoding (RFC 8949, section 4.2.1).\n\n;; A Textual document is a struct containing one field: an array of screens.\nsign_doc = {\n  screens_key: [* screen],\n}\n\n;; The key is an integer to keep the encoding small.\nscreens_key = 1\n\n;; A screen consists of a text string, an indentation, and the expert flag,\n;; represented as an integer-keyed map. All entries are optional\n;; and MUST be omitted from the encoding if empty, zero, or false.\n;; Text defaults to the empty string, indent defaults to zero,\n;; and expert defaults to false.\nscreen = {\n  ? title_key: tstr,\n  ? content_key: tstr,\n  ? indent_key: uint,\n  ? expert_key: bool,\n}\n\n;; Keys are small integers to keep the encoding small.\ntitle_key = 1\ncontent_key = 2\nindent_key = 3\nexpert_key = 4\n```\n\nDefining the sign_doc as directly an array of screens has also been considered. However, given the possibility of future iterations of this specification, using a single-keyed struct has been chosen over the former proposal, as structs allow for easier backwards-compatibility.\n\n## Details\n\nIn the examples that follow, screens will be shown as lines of text,\nindentation is indicated with a leading '>',\nand expert screens are marked with a leading `*`.\n\n### Encoding of the Transaction Envelope\n\nWe define \"transaction envelope\" as all data in a transaction that is not in the `TxBody.Messages` field. Transaction envelope includes fee, signer infos and memo, but don't include `Msg`s. `//` denotes comments and are not shown on the Ledger device.\n\n```\nChain ID: <string>\nAccount number: <uint64>\nSequence: <uint64>\nAddress: <string>\n*Public Key: <Any>\nThis transaction has <int> Message(s)                       // Pluralize \"Message\" only when int>1\n> Message (<int>/<int>): <Any>                              // See value renderers for Any rendering.\nEnd of Message\nMemo: <string>                                              // Skipped if no memo set.\nFee: <coins>                                                // See value renderers for coins rendering.\n*Fee payer: <string>                                        // Skipped if no fee_payer set.\n*Fee granter: <string>                                      // Skipped if no fee_granter set.\nTip: <coins>                                                // Skipped if no tip.\nTipper: <string>\n*Gas Limit: <uint64>\n*Timeout Height: <uint64>                                   // Skipped if no timeout_height set.\n*Other signer: <int> SignerInfo                             // Skipped if the transaction only has 1 signer.\n*> Other signer (<int>/<int>): <SignerInfo>\n*End of other signers\n*Extension options: <int> Any:                              // Skipped if no body extension options\n*> Extension options (<int>/<int>): <Any>\n*End of extension options\n*Non critical extension options: <int> Any:                 // Skipped if no body non critical extension options\n*> Non critical extension options (<int>/<int>): <Any>\n*End of Non critical extension options\n*Hash of raw bytes: <hex_string>                            // Hex encoding of bytes defined, to prevent tx hash malleability.\n```\n\n### Encoding of the Transaction Body\n\nTransaction Body is the `Tx.TxBody.Messages` field, which is an array of `Any`s, where each `Any` packs a `sdk.Msg`. Since `sdk.Msg`s are widely used, they have a slightly different encoding than usual array of `Any`s (Protobuf: `repeated google.protobuf.Any`) described in Annex 1.\n\n```\nThis transaction has <int> message:   // Optional 's' for \"message\" if there's  >1 sdk.Msgs.\n// For each Msg, print the following 2 lines:\nMsg (<int>/<int>): <string>           // E.g. Msg (1/2): bank v1beta1 send coins\n<value rendering of Msg struct>\nEnd of transaction messages\n```\n\n#### Example\n\nGiven the following Protobuf message:\n\n```protobuf\nmessage Grant {\n  google.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\n  google.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\n}\n\nmessage MsgGrant {\n  option (cosmos.msg.v1.signer) = \"granter\";\n\n  string granter = 1 [(cosmos_proto.scalar) = \"cosmos.AddressString\"];\n  string grantee = 2 [(cosmos_proto.scalar) = \"cosmos.AddressString\"];\n}\n```\n\nand a transaction containing 1 such `sdk.Msg`, we get the following encoding:\n\n```\nThis transaction has 1 message:\nMsg (1/1): authz v1beta1 grant\nGranter: cosmos1abc...def\nGrantee: cosmos1ghi...jkl\nEnd of transaction messages\n```\n\n### Custom `Msg` Renderers\n\nApplication developers may choose to not follow default renderer value output for their own `Msg`s. In this case, they can implement their own custom `Msg` renderer. This is similar to [EIP4430](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-4430.md), where the smart contract developer chooses the description string to be shown to the end user.\n\nThis is done by setting the `cosmos.msg.textual.v1.expert_custom_renderer` Protobuf option to a non-empty string. This option CAN ONLY be set on a Protobuf message representing transaction message object (implementing `sdk.Msg` interface).\n\n```protobuf\nmessage MsgFooBar {\n  // Optional comments to describe in human-readable language the formatting\n  // rules of the custom renderer.\n  option (cosmos.msg.textual.v1.expert_custom_renderer) = \"<unique algorithm identifier>\";\n\n  // proto fields\n}\n```\n\nWhen this option is set on a `Msg`, a registered function will transform the `Msg` into an array of one or more strings, which MAY use the key/value format (described in point #3) with the expert field prefix (described in point #5) and arbitrary indentation (point #6). These strings MAY be rendered from a `Msg` field using a default value renderer, or they may be generated from several fields using custom logic.\n\nThe `<unique algorithm identifier>` is a string convention chosen by the application developer and is used to identify the custom `Msg` renderer. For example, the documentation or specification of this custom algorithm can reference this identifier. This identifier CAN have a versioned suffix (e.g. `_v1`) to adapt for future changes (which would be consensus-breaking). We also recommend adding Protobuf comments to describe in human language the custom logic used.\n\nMoreover, the renderer must provide 2 functions: one for formatting from Protobuf to string, and one for parsing string to Protobuf. These 2 functions are provided by the application developer. To satisfy point #1, the parse function MUST be the inverse of the formatting function. This property will not be checked by the SDK at runtime. However, we strongly recommend the application developer to include a comprehensive suite in their app repo to test invertibility, as to not introduce security bugs.\n\n### Require signing over the `TxBody` and `AuthInfo` raw bytes\n\nRecall that the transaction bytes merkleized on chain are the Protobuf binary serialization of [TxRaw](https://buf.build/cosmos/cosmos-sdk/docs/main:cosmos.tx.v1beta1#cosmos.tx.v1beta1.TxRaw), which contains the `body_bytes` and `auth_info_bytes`. Moreover, the transaction hash is defined as the SHA256 hash of the `TxRaw` bytes. We require that the user signs over these bytes in SIGN_MODE_TEXTUAL, more specifically over the following string:\n\n```\n*Hash of raw bytes: <HEX(sha256(len(body_bytes) ++ body_bytes ++ len(auth_info_bytes) ++ auth_info_bytes))>\n```\n\nwhere:\n\n* `++` denotes concatenation,\n* `HEX` is the hexadecimal representation of the bytes, all in capital letters, no `0x` prefix,\n* and `len()` is encoded as a Big-Endian uint64.\n\nThis is to prevent transaction hash malleability. The point #1 about invertibility assures that transaction `body` and `auth_info` values are not malleable, but the transaction hash still might be malleable with point #1 only, because the SIGN_MODE_TEXTUAL strings don't follow the byte ordering defined in `body_bytes` and `auth_info_bytes`. Without this hash, a malicious validator or exchange could intercept a transaction, modify its transaction hash _after_ the user signed it using SIGN_MODE_TEXTUAL (by tweaking the byte ordering inside `body_bytes` or `auth_info_bytes`), and then submit it to Tendermint.\n\nBy including this hash in the SIGN_MODE_TEXTUAL signing payload, we keep the same level of guarantees as [SIGN_MODE_DIRECT](./adr-020-protobuf-transaction-encoding.md).\n\nThese bytes are only shown in expert mode, hence the leading `*`.\n\n## Updates to the current specification\n\nThe current specification is not set in stone, and future iterations are to be expected. We distinguish two categories of updates to this specification:\n\n1. Updates that require changes of the hardware device embedded application.\n2. Updates that only modify the envelope and the value renderers.\n\nUpdates in the 1st category include changes of the `Screen` struct or its corresponding CBOR encoding. This type of updates require a modification of the hardware signer application, to be able to decode and parse the new types. Backwards-compatibility must also be guaranteed, so that the new hardware application works with existing versions of the SDK. These updates require the coordination of multiple parties: SDK developers, hardware application developers (currently: Zondax), and client-side developers (e.g. CosmJS). Furthermore, a new submission of the hardware device application may be necessary, which, depending on the vendor, can take some time. As such, we recommend to avoid this type of updates as much as possible.\n\nUpdates in the 2nd category include changes to any of the value renderers or to the transaction envelope. For example, the ordering of fields in the envelope can be swapped, or the timestamp formatting can be modified. Since SIGN_MODE_TEXTUAL sends `Screen`s to the hardware device, this type of change does not need a hardware wallet application update. They are however state-machine-breaking, and must be documented as such. They require the coordination of SDK developers with client-side developers (e.g. CosmJS), so that the updates are released on both sides close to each other in time.\n\nWe define a spec version, which is an integer that must be incremented on each update of either category. This spec version will be exposed by the SDK's implementation, and can be communicated to clients. For example, SDK v0.50 might use the spec version 1, and SDK v0.51 might use 2; thanks to this versioning, clients can know how to craft SIGN_MODE_TEXTUAL transactions based on the target SDK version.\n\nThe current spec version is defined in the \"Status\" section, on the top of this document. It is initialized to `0` to allow flexibility in choosing how to define future versions, as it would allow adding a field either in the SignDoc Go struct or in Protobuf in a backwards-compatible way.\n\n## Additional Formatting by the Hardware Device\n\nSee [annex 2](./adr-050-sign-mode-textual-annex2.md).\n\n## Examples\n\n1. A minimal MsgSend: [see transaction](https://github.com/cosmos/cosmos-sdk/blob/094abcd393379acbbd043996024d66cd65246fb1/tx/textual/internal/testdata/e2e.json#L2-L70).\n2. A transaction with a bit of everything: [see transaction](https://github.com/cosmos/cosmos-sdk/blob/094abcd393379acbbd043996024d66cd65246fb1/tx/textual/internal/testdata/e2e.json#L71-L270).\n\nThe examples below are stored in a JSON file with the following fields:\n\n* `proto`: the representation of the transaction in ProtoJSON,\n* `screens`: the transaction rendered into SIGN_MODE_TEXTUAL screens,\n* `cbor`: the sign bytes of the transaction, which is the CBOR encoding of the screens.\n\n## Consequences\n\n### Backwards Compatibility\n\nSIGN_MODE_TEXTUAL is purely additive, and doesn't break any backwards compatibility with other sign modes.\n\n### Positive\n\n* Human-friendly way of signing in hardware devices.\n* Once SIGN_MODE_TEXTUAL is shipped, SIGN_MODE_LEGACY_AMINO_JSON can be deprecated and removed. On the longer term, once the ecosystem has totally migrated, Amino can be totally removed.\n\n### Negative\n\n* Some fields are still encoded in non-human-readable ways, such as public keys in hexadecimal.\n* New ledger app needs to be released, still unclear\n\n### Neutral\n\n* If the transaction is complex, the string array can be arbitrarily long, and some users might just skip some screens and blind sign.\n\n## Further Discussions\n\n* Some details on value renderers need to be polished, see [Annex 1](./adr-050-sign-mode-textual-annex1.md).\n* Are ledger apps able to support both SIGN_MODE_LEGACY_AMINO_JSON and SIGN_MODE_TEXTUAL at the same time?\n* Open question: should we add a Protobuf field option to allow app developers to overwrite the textual representation of certain Protobuf fields and message? This would be similar to Ethereum's [EIP4430](https://github.com/ethereum/EIPs/pull/4430), where the contract developer decides on the textual representation.\n* Internationalization.\n\n## References\n\n* [Annex 1](./adr-050-sign-mode-textual-annex1.md)\n\n* Initial discussion: https://github.com/cosmos/cosmos-sdk/issues/6513\n* Living document used in the working group: https://hackmd.io/fsZAO-TfT0CKmLDtfMcKeA?both\n* Working group meeting notes: https://hackmd.io/7RkGfv_rQAaZzEigUYhcXw\n* Ethereum's \"Described Transactions\" https://github.com/ethereum/EIPs/pull/4430"
  },
  {
    "number": 50,
    "filename": "adr-050-sign-mode-textual-annex1.md",
    "title": "ADR 050: SIGN_MODE_TEXTUAL: Annex 1 Value Renderers",
    "content": "# ADR 050: SIGN_MODE_TEXTUAL: Annex 1 Value Renderers\n\n## Changelog\n\n* Dec 06, 2021: Initial Draft\n* Feb 07, 2022: Draft read and concept-ACKed by the Ledger team.\n* Dec 01, 2022: Remove `Object: ` prefix on Any header screen.\n* Dec 13, 2022: Sign over bytes hash when bytes length > 32.\n* Mar 27, 2023: Update `Any` value renderer to omit message header screen.\n\n## Status\n\nAccepted. Implementation started. Small value renderers details still need to be polished.\n\n## Abstract\n\nThis Annex describes value renderers, which are used for displaying Protobuf values in a human-friendly way using a string array.\n\n## Value Renderers\n\nValue Renderers describe how values of different Protobuf types should be encoded as a string array. Value renderers can be formalized as a set of bijective functions `func renderT(value T) []string`, where `T` is one of the below Protobuf types for which this spec is defined.\n\n### Protobuf `number`\n\n* Applies to:\n    * protobuf numeric integer types (`int{32,64}`, `uint{32,64}`, `sint{32,64}`, `fixed{32,64}`, `sfixed{32,64}`)\n    * strings whose `customtype` is `github.com/cosmos/cosmos-sdk/types.Int` or `github.com/cosmos/cosmos-sdk/types.Dec`\n    * bytes whose `customtype` is `github.com/cosmos/cosmos-sdk/types.Int` or `github.com/cosmos/cosmos-sdk/types.Dec`\n* Trailing decimal zeroes are always removed\n* Formatting with `'`s for every three integral digits.\n* Usage of `.` to denote the decimal delimiter.\n\n#### Examples\n\n* `1000` (uint64) -> `1'000`\n* `\"1000000.00\"` (string representing a Dec) -> `1'000'000`\n* `\"1000000.10\"` (string representing a Dec) -> `1'000'000.1`\n\n### `coin`\n\n* Applies to `cosmos.base.v1beta1.Coin`.\n* Denoms are converted to `display` denoms using `Metadata` (if available). **This requires a state query**. The definition of `Metadata` can be found in the [bank protobuf definition](https://buf.build/cosmos/cosmos-sdk/docs/main:cosmos.bank.v1beta1#cosmos.bank.v1beta1.Metadata). If the `display` field is empty or nil, then we do not perform any denom conversion.\n* Amounts are converted to `display` denom amounts and rendered as `number`s above\n    * We do not change the capitalization of the denom. In practice, `display` denoms are stored in lowercase in state (e.g. `10 atom`), however they are often showed in UPPERCASE in everyday life (e.g. `10 ATOM`). Value renderers keep the case used in state, but we may recommend chains changing the denom metadata to be uppercase for better user display.\n* One space between the denom and amount (e.g. `10 atom`).\n* In the future, IBC denoms could maybe be converted to DID/IIDs, if we can find a robust way for doing this (ex. `cosmos:cosmos:hub:bank:denom:atom`)\n\n#### Examples\n\n* `1000000000uatom` -> `[\"1'000 atom\"]`, because atom is the metadata's display denom.\n\n### `coins`\n\n* an array of `coin` is display as the concatenation of each `coin` encoded as the specification above, then joined together with the delimiter `\", \"` (a comma and a space, no quotes around).\n* the list of coins is ordered by unicode code point of the display denom: `A-Z` < `a-z`. For example, the string `aAbBcC` would be sorted `ABCabc`.\n    * if the coins list had 0 items in it then it'll be rendered as `zero`\n\n### Example\n\n* `[\"3cosm\", \"2000000uatom\"]` -> `2 atom, 3 COSM` (assuming the display denoms are `atom` and `COSM`)\n* `[\"10atom\", \"20Acoin\"]` -> `20 Acoin, 10 atom` (assuming the display denoms are `atom` and `Acoin`)\n* `[]` -> `zero` \n\n### `repeated`\n\n* Applies to all `repeated` fields, except `cosmos.tx.v1beta1.TxBody#Messages`, which has a particular encoding (see [ADR-050](./adr-050-sign-mode-textual.md)).\n* A repeated type has the following template:\n\n```\n<field_name>: <int> <field_kind>\n<field_name> (<index>/<int>): <value rendered 1st line>\n<optional value rendered in the next lines>\n<field_name> (<index>/<int>): <value rendered 1st line>\n<optional value rendered in the next lines>\nEnd of <field_name>.\n```\n\nwhere:\n\n* `field_name` is the Protobuf field name of the repeated field\n* `field_kind`:\n    * if the type of the repeated field is a message, `field_kind` is the message name\n    * if the type of the repeated field is an enum, `field_kind` is the enum name\n    * in any other case, `field_kind` is the protobuf primitive type (e.g. \"string\" or \"bytes\")\n* `int` is the length of the array\n* `index` is one based index of the repeated field\n\n#### Examples\n\nGiven the proto definition:\n\n```protobuf\nmessage AllowedMsgAllowance {\n  repeated string allowed_messages = 1;\n}\n```\n\nand initializing with:\n\n```go\nx := []AllowedMsgAllowance{\"cosmos.bank.v1beta1.MsgSend\", \"cosmos.gov.v1.MsgVote\"}\n```\n\nwe have the following value-rendered encoding:\n\n```\nAllowed messages: 2 strings\nAllowed messages (1/2): cosmos.bank.v1beta1.MsgSend\nAllowed messages (2/2): cosmos.gov.v1.MsgVote\nEnd of Allowed messages\n```\n\n### `message`\n\n* Applies to all Protobuf messages that do not have a custom encoding.\n* Field names follow [sentence case](https://en.wiktionary.org/wiki/sentence_case)\n    * replace each `_` with a space\n    * capitalize first letter of the sentence\n* Field names are ordered by their Protobuf field number\n* Screen title is the field name, and screen content is the value.\n* Nesting:\n    * if a field contains a nested message, we value-render the underlying message using the template:\n\n  ```\n  <field_name>: <1st line of value-rendered message>\n  > <lines 2-n of value-rendered message>             // Notice the `>` prefix.\n  ```\n\n    * `>` character is used to denote nesting. For each additional level of nesting, add `>`.\n\n#### Examples\n\nGiven the following Protobuf messages:\n\n```protobuf\nenum VoteOption {\n  VOTE_OPTION_UNSPECIFIED = 0;\n  VOTE_OPTION_YES = 1;\n  VOTE_OPTION_ABSTAIN = 2;\n  VOTE_OPTION_NO = 3;\n  VOTE_OPTION_NO_WITH_VETO = 4;\n}\n\nmessage WeightedVoteOption {\n  VoteOption option = 1;\n  string     weight = 2 [(cosmos_proto.scalar) = \"cosmos.Dec\"];\n}\n\nmessage Vote {\n  uint64 proposal_id = 1;\n  string voter       = 2 [(cosmos_proto.scalar) = \"cosmos.AddressString\"];\n  reserved 3;\n  repeated WeightedVoteOption options = 4;\n}\n```\n\nwe get the following encoding for the `Vote` message:\n\n```\nVote object\n> Proposal id: 4\n> Voter: cosmos1abc...def\n> Options: 2 WeightedVoteOptions\n> Options (1/2): WeightedVoteOption object\n>> Option: VOTE_OPTION_YES\n>> Weight: 0.7\n> Options (2/2): WeightedVoteOption object\n>> Option: VOTE_OPTION_NO\n>> Weight: 0.3\n> End of Options\n```\n\n### Enums\n\n* Show the enum variant name as string.\n\n#### Examples\n\nSee example above with `message Vote{}`.\n\n### `google.protobuf.Any`\n\n* Applies to `google.protobuf.Any`\n* Rendered as:\n\n```\n<type_url>\n> <value rendered underlying message>\n```\n\nThere is however one exception: when the underlying message is a Protobuf message that does not have a custom encoding, then the message header screen is omitted, and one level of indentation is removed.\n\nMessages that have a custom encoding, including `google.protobuf.Timestamp`, `google.protobuf.Duration`, `google.protobuf.Any`, `cosmos.base.v1beta1.Coin`, and messages that have an app-defined custom encoding, will preserve their header and indentation level.\n\n#### Examples\n\nMessage header screen is stripped, one-level of indentation removed:\n\n```\n/cosmos.gov.v1.Vote\n> Proposal id: 4\n> Vote: cosmos1abc...def\n> Options: 2 WeightedVoteOptions\n> Options (1/2): WeightedVoteOption object\n>> Option: Yes\n>> Weight: 0.7\n> Options (2/2): WeightedVoteOption object\n>> Option: No\n>> Weight: 0.3\n> End of Options\n```\n\nMessage with custom encoding:\n\n```\n/cosmos.base.v1beta1.Coin\n> 10uatom\n```\n\n### `google.protobuf.Timestamp`\n\nRendered using [RFC 3339](https://www.rfc-editor.org/rfc/rfc3339) (a\nsimplification of ISO 8601), which is the current recommendation for portable\ntime values. The rendering always uses \"Z\" (UTC) as the timezone. It uses only\nthe necessary fractional digits of a second, omitting the fractional part\nentirely if the timestamp has no fractional seconds. (The resulting timestamps\nare not automatically sortable by standard lexicographic order, but we favor\nthe legibility of the shorter string.)\n\n#### Examples\n\nThe timestamp with 1136214245 seconds and 700000000 nanoseconds is rendered\nas `2006-01-02T15:04:05.7Z`.\nThe timestamp with 1136214245 seconds and zero nanoseconds is rendered\nas `2006-01-02T15:04:05Z`.\n\n### `google.protobuf.Duration`\n\nThe duration proto expresses a raw number of seconds and nanoseconds.\nThis will be rendered as longer time units of days, hours, and minutes,\nplus any remaining seconds, in that order.\nLeading and trailing zero-quantity units will be omitted, but all\nunits in between nonzero units will be shown, e.g. ` 3 days, 0 hours, 0 minutes, 5 seconds`.\n\nEven longer time units such as months or years are imprecise.\nWeeks are precise, but not commonly used - `91 days` is more immediately\nlegible than `13 weeks`.  Although `days` can be problematic,\ne.g. noon to noon on subsequent days can be 23 or 25 hours depending on\ndaylight savings transitions, there is significant advantage in using\nstrict 24-hour days over using only hours (e.g. `91 days` vs `2184 hours`).\n\nWhen nanoseconds are nonzero, they will be shown as fractional seconds,\nwith only the minimum number of digits, e.g `0.5 seconds`.\n\nA duration of exactly zero is shown as `0 seconds`.\n\nUnits will be given as singular (no trailing `s`) when the quantity is exactly one,\nand will be shown in plural otherwise.\n\nNegative durations will be indicated with a leading minus sign (`-`).\n\nExamples:\n\n* `1 day`\n* `30 days`\n* `-1 day, 12 hours`\n* `3 hours, 0 minutes, 53.025 seconds`\n\n### bytes\n\n* Bytes of length shorter or equal to 35 are rendered in hexadecimal, all capital letters, without the `0x` prefix.\n* Bytes of length greater than 35 are hashed using SHA256. The rendered text is `SHA-256=`, followed by the 32-byte hash, in hexadecimal, all capital letters, without the `0x` prefix.\n* The hexadecimal string is finally separated into groups of 4 digits, with a space `' '` as separator. If the bytes length is odd, the 2 remaining hexadecimal characters are at the end.\n\nThe number 35 was chosen because it is the longest length where the hashed-and-prefixed representation is longer than the original data directly formatted, using the 3 rules above. More specifically:\n\n* a 35-byte array will have 70 hex characters, plus 17 space characters, resulting in 87 characters.\n* byte arrays starting from length 36 will be hashed to 32 bytes, which is 64 hex characters plus 15 spaces, and with the `SHA-256=` prefix, it takes 87 characters.\nAlso, secp256k1 public keys have length 33, so their Textual representation is not their hashed value, which we would like to avoid.\n\nNote: Data longer than 35 bytes are not rendered in a way that can be inverted. See ADR-050's [section about invertibility](./adr-050-sign-mode-textual.md#invertible-rendering) for a discussion.\n\n#### Examples\n\nInputs are displayed as byte arrays.\n\n* `[0]`: `00`\n* `[0,1,2]`: `0001 02`\n* `[0,1,2,..,34]`: `0001 0203 0405 0607 0809 0A0B 0C0D 0E0F 1011 1213 1415 1617 1819 1A1B 1C1D 1E1F 2021 22`\n* `[0,1,2,..,35]`: `SHA-256=5D7E 2D9B 1DCB C85E 7C89 0036 A2CF 2F9F E7B6 6554 F2DF 08CE C6AA 9C0A 25C9 9C21`\n\n### address bytes\n\nWe currently use `string` types in protobuf for addresses so this may not be needed, but if any address bytes are used in sign mode textual they should be rendered with bech32 formatting\n\n### strings\n\nStrings are rendered as-is.\n\n### Default Values\n\n* Default Protobuf values for each field are skipped.\n\n#### Example\n\n```protobuf\nmessage TestData {\n  string signer = 1;\n  string metadata = 2;\n}\n```\n\n```go\nmyTestData := TestData{\n  Signer: \"cosmos1abc\"\n}\n```\n\nWe get the following encoding for the `TestData` message:\n\n```\nTestData object\n> Signer: cosmos1abc\n```\n\n### bool\n\nBoolean values are rendered as `True` or `False`.\n\n### [ABANDONED] Custom `msg_title` instead of Msg `type_url`\n\n_This paragraph is in the Annex for informational purposes only, and will be removed in a next update of the ADR._\n\n<details>\n  <summary>Click to see abandoned idea.</summary>\n\n* all protobuf messages to be used with `SIGN_MODE_TEXTUAL` CAN have a short title associated with them that can be used in format strings whenever the type URL is explicitly referenced via the `cosmos.msg.v1.textual.msg_title` Protobuf message option.\n* if this option is not specified for a Msg, then the Protobuf fully qualified name will be used.\n\n```protobuf\nmessage MsgSend {\n  option (cosmos.msg.v1.textual.msg_title) = \"bank send coins\";\n}\n```\n\n* they MUST be unique per message, per chain\n\n#### Examples\n\n* `cosmos.gov.v1.MsgVote` -> `governance v1 vote`\n\n#### Best Practices\n\nWe recommend to use this option only for `Msg`s whose Protobuf fully qualified name can be hard to understand. As such, the two examples above (`MsgSend` and `MsgVote`) are not good examples to be used with `msg_title`. We still allow `msg_title` for chains who might have `Msg`s with complex or non-obvious names.\n\nIn those cases, we recommend to drop the version (e.g. `v1`) in the string if there's only one version of the module on chain. This way, the bijective mapping can figure out which message each string corresponds to. If multiple Protobuf versions of the same module exist on the same chain, we recommend keeping the first `msg_title` with version, and the second `msg_title` with version (e.g. `v2`):\n\n* `mychain.mymodule.v1.MsgDo` -> `mymodule do something`\n* `mychain.mymodule.v2.MsgDo` -> `mymodule v2 do something`\n\n</details>"
  },
  {
    "number": 50,
    "filename": "adr-050-sign-mode-textual-annex2.md",
    "title": "ADR 050: SIGN_MODE_TEXTUAL: Annex 2 XXX",
    "content": "# ADR 050: SIGN_MODE_TEXTUAL: Annex 2 XXX\n\n## Changelog\n\n* Oct 3, 2022: Initial Draft\n\n## Status\n\nDRAFT\n\n## Abstract\n\nThis annex provides normative guidance on how devices should render a\n`SIGN_MODE_TEXTUAL` document.\n\n## Context\n\n`SIGN_MODE_TEXTUAL` allows a legible version of a transaction to be signed\non a hardware security device, such as a Ledger. Early versions of the\ndesign rendered transactions directly to lines of ASCII text, but this\nproved awkward from its in-band signaling, and for the need to display\nUnicode text within the transaction.\n\n## Decision\n\n`SIGN_MODE_TEXTUAL` renders to an abstract representation, leaving it\nup to device-specific software how to present this representation given the\ncapabilities, limitations, and conventions of the device.\n\nWe offer the following normative guidance:\n\n1. The presentation should be as legible as possible to the user, given\nthe capabilities of the device. If legibility could be sacrificed for other\nproperties, we would recommend just using some other signing mode.\nLegibility should focus on the common case - it is okay for unusual cases\nto be less legible.\n\n2. The presentation should be invertible if possible without substantial\nsacrifice of legibility.  Any change to the rendered data should result\nin a visible change to the presentation. This extends the integrity of the\nsigning to user-visible presentation.\n\n3. The presentation should follow normal conventions of the device,\nwithout sacrificing legibility or invertibility.\n\nAs an illustration of these principles, here is an example algorithm\nfor presentation on a device which can display a single 80-character\nline of printable ASCII characters:\n\n* The presentation is broken into lines, and each line is presented in\nsequence, with user controls for going forward or backward a line.\n\n* Expert mode screens are only presented if the device is in expert mode.\n\n* Each line of the screen starts with a number of `>` characters equal\nto the screen's indentation level, followed by a `+` character if this\nisn't the first line of the screen, followed by a space if either a\n`>` or a `+` has been emitted,\nor if this header is followed by a `>`, `+`, or space.\n\n* If the line ends with whitespace or an `@` character, an additional `@`\ncharacter is appended to the line.\n\n* The following ASCII control characters or backslash (`\\`) are converted\nto a backslash followed by a letter code, in the manner of string literals\nin many languages:\n\n    * a: U+0007 alert or bell\n    * b: U+0008 backspace\n    * f: U+000C form feed\n    * n: U+000A line feed\n    * r: U+000D carriage return\n    * t: U+0009 horizontal tab\n    * v: U+000B vertical tab\n    * `\\`: U+005C backslash\n\n* All other ASCII control characters, plus non-ASCII Unicode code points,\nare shown as either:\n\n    * `\\u` followed by 4 uppercase hex characters for code points\n    in the basic multilingual plane (BMP).\n\n    * `\\U` followed by 8 uppercase hex characters for other code points.\n\n* The screen will be broken into multiple lines to fit the 80-character\nlimit, considering the above transformations in a way that attempts to\nminimize the number of lines generated. Expanded control or Unicode characters\nare never split across lines.\n\nExample output:\n\n```\nAn introductory line.\nkey1: 123456\nkey2: a string that ends in whitespace   @\nkey3: a string that ends in  a single ampersand - @@\n >tricky key4<: note the leading space in the presentation\nintroducing an aggregate\n> key5: false\n> key6: a very long line of text, please co\\u00F6perate and break into\n>+  multiple lines.\n> Can we do further nesting?\n>> You bet we can!\n```\n\nThe inverse mapping gives us the only input which could have\ngenerated this output (JSON notation for string data):\n\n```\nIndent  Text\n0       \"An introductory line.\"\n0       \"key1: 123456\"\n0       \"key2: a string that ends in whitespace   \"\n0       \"key3: a string that ends in  a single ampersand - @\"\n0       \">tricky key4<: note the leading space in the presentation\"\n0       \"introducing an aggregate\"\n1       \"key5: false\"\n1       \"key6: a very long line of text, please coöperate and break into multiple lines.\"\n1       \"Can we do further nesting?\"\n2       \"You bet we can!\"\n```"
  },
  {
    "number": 53,
    "filename": "adr-053-go-module-refactoring.md",
    "title": "ADR 053: Go Module Refactoring",
    "content": "# ADR 053: Go Module Refactoring\n\n## Changelog\n\n* 2022-04-27: First Draft\n\n## Status\n\nPROPOSED\n\n## Abstract\n\nThe current SDK is built as a single monolithic go module. This ADR describes\nhow we refactor the SDK into smaller independently versioned go modules\nfor ease of maintenance.\n\n## Context\n\nGo modules impose certain requirements on software projects with respect to\nstable version numbers (anything above 0.x) in that [any API breaking changes\nnecessitate a major version](https://go.dev/doc/modules/release-workflow#breaking)\nincrease which technically creates a new go module\n(with a v2, v3, etc. suffix).\n\n[Keeping modules API compatible](https://go.dev/blog/module-compatibility) in\nthis way requires a fair amount of thought and discipline.\n\nThe Cosmos SDK is a fairly large project which originated before go modules\ncame into existence and has always been under a v0.x release even though\nit has been used in production for years now, not because it isn't production\nquality software, but rather because the API compatibility guarantees required\nby go modules are fairly complex to adhere to with such a large project.\nUp to now, it has generally been deemed more important to be able to break the\nAPI if needed rather than require all users update all package import paths\nto accommodate breaking changes causing v2, v3, etc. releases. This is in\naddition to the other complexities related to protobuf generated code that will\nbe addressed in a separate ADR.\n\nNevertheless, the desire for semantic versioning has been [strong in the\ncommunity](https://github.com/cosmos/cosmos-sdk/discussions/10162) and the\nsingle go module release process has made it very hard to\nrelease small changes to isolated features in a timely manner. Release cycles\noften exceed six months which means small improvements done in a day or\ntwo get bottle-necked by everything else in the monolithic release cycle.\n\n## Decision\n\nTo improve the current situation, the SDK is being refactored into multiple\ngo modules within the current repository. There has been a [fair amount of\ndebate](https://github.com/cosmos/cosmos-sdk/discussions/10582#discussioncomment-1813377)\nas to how to do this, with some developers arguing for larger vs smaller\nmodule scopes. There are pros and cons to both approaches (which will be\ndiscussed below in the [Consequences](#consequences) section), but the\napproach being adopted is the following:\n\n* a go module should generally be scoped to a specific coherent set of\nfunctionality (such as math, errors, store, etc.)\n* when code is removed from the core SDK and moved to a new module path, every \neffort should be made to avoid API breaking changes in the existing code using\naliases and wrapper types (as done in https://github.com/cosmos/cosmos-sdk/pull/10779\nand https://github.com/cosmos/cosmos-sdk/pull/11788)\n* new go modules should be moved to a standalone domain (`cosmossdk.io`) before\nbeing tagged as `v1.0.0` to accommodate the possibility that they may be\nbetter served by a standalone repository in the future\n* all go modules should follow the guidelines in https://go.dev/blog/module-compatibility\nbefore `v1.0.0` is tagged and should make use of `internal` packages to limit\nthe exposed API surface\n* the new go module's API may deviate from the existing code where there are\nclear improvements to be made or to remove legacy dependencies (for instance on\namino or gogo proto), as long the old package attempts\nto avoid API breakage with aliases and wrappers\n* care should be taken when simply trying to turn an existing package into a\nnew go module: https://github.com/golang/go/wiki/Modules#is-it-possible-to-add-a-module-to-a-multi-module-repository.\nIn general, it seems safer to just create a new module path (appending v2, v3, etc.\nif necessary), rather than trying to make an old package a new module.\n\n## Consequences\n\n### Backwards Compatibility\n\nIf the above guidelines are followed to use aliases or wrapper types pointing\nin existing APIs that point back to the new go modules, there should be no or\nvery limited breaking changes to existing APIs.\n\n### Positive\n\n* standalone pieces of software will reach `v1.0.0` sooner\n* new features to specific functionality will be released sooner \n\n### Negative\n\n* there will be more go module versions to update in the SDK itself and\nper-project, although most of these will hopefully be indirect\n\n### Neutral\n\n## Further Discussions\n\nFurther discussions are occurring primarily in\nhttps://github.com/cosmos/cosmos-sdk/discussions/10582 and within\nthe Cosmos SDK Framework Working Group.\n\n## References\n\n* https://go.dev/doc/modules/release-workflow\n* https://go.dev/blog/module-compatibility\n* https://github.com/cosmos/cosmos-sdk/discussions/10162\n* https://github.com/cosmos/cosmos-sdk/discussions/10582\n* https://github.com/cosmos/cosmos-sdk/pull/10779\n* https://github.com/cosmos/cosmos-sdk/pull/11788"
  },
  {
    "number": 54,
    "filename": "adr-054-semver-compatible-modules.md",
    "title": "ADR 054: Semver Compatible SDK Modules",
    "content": "# ADR 054: Semver Compatible SDK Modules\n\n## Changelog\n\n* 2022-04-27: First draft\n\n## Status\n\nDRAFT\n\n## Abstract\n\nIn order to move the Cosmos SDK to a system of decoupled semantically versioned\nmodules which can be composed in different combinations (ex. staking v3 with\nbank v1 and distribution v2), we need to reassess how we organize the API surface\nof modules to avoid problems with go semantic import versioning and\ncircular dependencies. This ADR explores various approaches we can take to\naddressing these issues.\n\n## Context\n\nThere has been [a fair amount of desire](https://github.com/cosmos/cosmos-sdk/discussions/10162)\nin the community for semantic versioning in the SDK and there has been significant\nmovement to splitting SDK modules into [standalone go modules](https://github.com/cosmos/cosmos-sdk/issues/11899).\nBoth of these will ideally allow the ecosystem to move faster because we won't\nbe waiting for all dependencies to update synchronously. For instance, we could\nhave 3 versions of the core SDK compatible with the latest 2 releases of\nCosmWasm as well as 4 different versions of staking . This sort of setup would\nallow early adopters to aggressively integrate new versions, while allowing\nmore conservative users to be selective about which versions they're ready for.\n\nIn order to achieve this, we need to solve the following problems:\n\n1. because of the way [go semantic import versioning](https://research.swtch.com/vgo-import) (SIV)\n   works, moving to SIV naively will actually make it harder to achieve these goals\n2. circular dependencies between modules need to be broken to actually release\n   many modules in the SDK independently\n3. pernicious minor version incompatibilities introduced through correctly\n   [evolving protobuf schemas](https://developers.google.com/protocol-buffers/docs/proto3#updating)\n   without correct [unknown field filtering](./adr-020-protobuf-transaction-encoding.md#unknown-field-filtering)\n\nNote that all the following discussion assumes that the proto file versioning and state machine versioning of a module\nare distinct in that:\n\n* proto files are maintained in a non-breaking way (using something\n  like [buf breaking](https://docs.buf.build/breaking/overview)\n  to ensure all changes are backwards compatible)\n* proto file versions get bumped much less frequently, i.e. we might maintain `cosmos.bank.v1` through many versions\n  of the bank module state machine\n* state machine breaking changes are more common and ideally this is what we'd want to semantically version with\n  go modules, ex. `x/bank/v2`, `x/bank/v3`, etc.\n\n### Problem 1: Semantic Import Versioning Compatibility\n\nConsider we have a module `foo` which defines the following `MsgDoSomething` and that we've released its state\nmachine in go module `example.com/foo`:\n\n```protobuf\npackage foo.v1;\n\nmessage MsgDoSomething {\n  string sender = 1;\n  uint64 amount = 2;\n}\n\nservice Msg {\n  DoSomething(MsgDoSomething) returns (MsgDoSomethingResponse);\n}\n```\n\nNow consider that we make a revision to this module and add a new `condition` field to `MsgDoSomething` and also\nadd a new validation rule on `amount` requiring it to be non-zero, and that following go semantic versioning we\nrelease the next state machine version of `foo` as `example.com/foo/v2`.\n\n```protobuf\n// Revision 1\npackage foo.v1;\n\nmessage MsgDoSomething {\n  string sender = 1;\n  \n  // amount must be a non-zero integer.\n  uint64 amount = 2;\n  \n  // condition is an optional condition on doing the thing.\n  //\n  // Since: Revision 1\n  Condition condition = 3;\n}\n```\n\nApproaching this naively, we would generate the protobuf types for the initial\nversion of `foo` in `example.com/foo/types` and we would generate the protobuf\ntypes for the second version in `example.com/foo/v2/types`.\n\nNow let's say we have a module `bar` which talks to `foo` using this keeper\ninterface which `foo` provides:\n\n```go\ntype FooKeeper interface {\n\tDoSomething(MsgDoSomething) error\n}\n```\n\n#### Scenario A: Backward Compatibility: Newer Foo, Older Bar\n\nImagine we have a chain which uses both `foo` and `bar` and wants to upgrade to\n`foo/v2`, but the `bar` module has not upgraded to `foo/v2`.\n\nIn this case, the chain will not be able to upgrade to `foo/v2` until `bar`\nhas upgraded its references to `example.com/foo/types.MsgDoSomething` to\n`example.com/foo/v2/types.MsgDoSomething`.\n\nEven if `bar`'s usage of `MsgDoSomething` has not changed at all, the upgrade\nwill be impossible without this change because `example.com/foo/types.MsgDoSomething`\nand `example.com/foo/v2/types.MsgDoSomething` are fundamentally different\nincompatible structs in the go type system.\n\n#### Scenario B: Forward Compatibility: Older Foo, Newer Bar\n\nNow let's consider the reverse scenario, where `bar` upgrades to `foo/v2`\nby changing the `MsgDoSomething` reference to `example.com/foo/v2/types.MsgDoSomething`\nand releases that as `bar/v2` with some other changes that a chain wants.\nThe chain, however, has decided that it thinks the changes in `foo/v2` are too\nrisky and that it'd prefer to stay on the initial version of `foo`.\n\nIn this scenario, it is impossible to upgrade to `bar/v2` without upgrading\nto `foo/v2` even if `bar/v2` would have worked 100% fine with `foo` other\nthan changing the import path to `MsgDoSomething` (meaning that `bar/v2`\ndoesn't actually use any new features of `foo/v2`).\n\nNow because of the way go semantic import versioning works, we are locked\ninto either using `foo` and `bar` OR `foo/v2` and `bar/v2`. We cannot have\n`foo` + `bar/v2` OR `foo/v2` + `bar`. The go type system doesn't allow this\neven if both versions of these modules are otherwise compatible with each\nother.\n\n#### Naive Mitigation\n\nA naive approach to fixing this would be to not regenerate the protobuf types\nin `example.com/foo/v2/types` but instead just update `example.com/foo/types`\nto reflect the changes needed for `v2` (adding `condition` and requiring\n`amount` to be non-zero). Then we could release a patch of `example.com/foo/types`\nwith this update and use that for `foo/v2`. But this change is state machine\nbreaking for `v1`. It requires changing the `ValidateBasic` method to reject\nthe case where `amount` is zero, and it adds the `condition` field which\nshould be rejected based\non [ADR 020 unknown field filtering](./adr-020-protobuf-transaction-encoding.md#unknown-field-filtering).\nSo adding these changes as a patch on `v1` is actually incorrect based on semantic\nversioning. Chains that want to stay on `v1` of `foo` should not\nbe importing these changes because they are incorrect for `v1.`\n\n### Problem 2: Circular dependencies\n\nNone of the above approaches allow `foo` and `bar` to be separate modules\nif for some reason `foo` and `bar` depend on each other in different ways.\nFor instance, we can't have `foo` import `bar/types` while `bar` imports\n`foo/types`.\n\nWe have several cases of circular module dependencies in the SDK\n(ex. staking, distribution and slashing) that are legitimate from a state machine\nperspective. Without separating the API types out somehow, there would be\nno way to independently semantically version these modules without some other\nmitigation.\n\n### Problem 3: Handling Minor Version Incompatibilities\n\nImagine that we solve the first two problems but now have a scenario where\n`bar/v2` wants the option to use `MsgDoSomething.condition` which only `foo/v2`\nsupports. If `bar/v2` works with `foo` `v1` and sets `condition` to some non-nil\nvalue, then `foo` will silently ignore this field resulting in a silent logic\npossibly dangerous logic error. If `bar/v2` were able to check whether `foo` was\non `v1` or `v2` and dynamically, it could choose to only use `condition` when\n`foo/v2` is available. Even if `bar/v2` were able to perform this check, however,\nhow do we know that it is always performing the check properly. Without\nsome sort of\nframework-level [unknown field filtering](./adr-020-protobuf-transaction-encoding.md#unknown-field-filtering),\nit is hard to know whether these pernicious hard to detect bugs are getting into\nour app and a client-server layer such as [ADR 033: Inter-Module Communication](./adr-033-protobuf-inter-module-comm.md)\nmay be needed to do this.\n\n## Solutions\n\n### Approach A) Separate API and State Machine Modules\n\nOne solution (first proposed in https://github.com/cosmos/cosmos-sdk/discussions/10582) is to isolate all protobuf\ngenerated code into a separate module\nfrom the state machine module. This would mean that we could have state machine\ngo modules `foo` and `foo/v2` which could use a types or API go module say\n`foo/api`. This `foo/api` go module would be perpetually on `v1.x` and only\naccept non-breaking changes. This would then allow other modules to be\ncompatible with either `foo` or `foo/v2` as long as the inter-module API only\ndepends on the types in `foo/api`. It would also allow modules `foo` and `bar`\nto depend on each other in that both of them could depend on `foo/api` and\n`bar/api` without `foo` directly depending on `bar` and vice versa.\n\nThis is similar to the naive mitigation described above except that it separates\nthe types into separate go modules which in and of itself could be used to\nbreak circular module dependencies. It has the same problems as the naive solution,\notherwise, which we could rectify by:\n\n1. removing all state machine breaking code from the API module (ex. `ValidateBasic` and any other interface methods)\n2. embedding the correct file descriptors for unknown field filtering in the binary\n\n#### Migrate all interface methods on API types to handlers\n\nTo solve 1), we need to remove all interface implementations from generated\ntypes and instead use a handler approach which essentially means that given\na type `X`, we have some sort of resolver which allows us to resolve interface\nimplementations for that type (ex. `sdk.Msg` or `authz.Authorization`). For\nexample:\n\n```go\nfunc (k Keeper) DoSomething(msg MsgDoSomething) error {\n\tvar validateBasicHandler ValidateBasicHandler\n\terr := k.resolver.Resolve(&validateBasic, msg)\n\tif err != nil {\n\t\treturn err\n\t}   \n\t\n\terr = validateBasicHandler.ValidateBasic()\n\t...\n}\n```\n\nIn the case of some methods on `sdk.Msg`, we could replace them with declarative\nannotations. For instance, `GetSigners` can already be replaced by the protobuf\nannotation `cosmos.msg.v1.signer`. In the future, we may consider some sort\nof protobuf validation framework (like https://github.com/bufbuild/protoc-gen-validate\nbut more Cosmos-specific) to replace `ValidateBasic`.\n\n#### Pinned FileDescriptor's\n\nTo solve 2), state machine modules must be able to specify what the version of\nthe protobuf files was that they were built against. For instance if the API\nmodule for `foo` upgrades to `foo/v2`, the original `foo` module still needs\na copy of the original protobuf files it was built with so that ADR 020\nunknown field filtering will reject `MsgDoSomething` when `condition` is\nset.\n\nThe simplest way to do this may be to embed the protobuf `FileDescriptor`s into\nthe module itself so that these `FileDescriptor`s are used at runtime rather\nthan the ones that are built into the `foo/api` which may be different. Using\n[buf build](https://docs.buf.build/build/usage#output-format), [go embed](https://pkg.go.dev/embed),\nand a build script we can probably come up with a solution for embedding\n`FileDescriptor`s into modules that is fairly straightforward.\n\n#### Potential limitations to generated code\n\nOne challenge with this approach is that it places heavy restrictions on what\ncan go in API modules and requires that most of this is state machine breaking.\nAll or most of the code in the API module would be generated from protobuf\nfiles, so we can probably control this with how code generation is done, but\nit is a risk to be aware of.\n\nFor instance, we do code generation for the ORM that in the future could\ncontain optimizations that are state machine breaking. We\nwould either need to ensure very carefully that the optimizations aren't\nactually state machine breaking in generated code or separate this generated code\nout from the API module into the state machine module. Both of these mitigations\nare potentially viable but the API module approach does require an extra level\nof care to avoid these sorts of issues.\n\n#### Minor Version Incompatibilities\n\nThis approach in and of itself does little to address any potential minor\nversion incompatibilities and the\nrequisite [unknown field filtering](./adr-020-protobuf-transaction-encoding.md#unknown-field-filtering).\nLikely some sort of client-server routing layer which does this check such as\n[ADR 033: Inter-Module communication](./adr-033-protobuf-inter-module-comm.md)\nis required to make sure that this is done properly. We could then allow\nmodules to perform a runtime check given a `MsgClient`, ex:\n\n```go\nfunc (k Keeper) CallFoo() error {\n\tif k.interModuleClient.MinorRevision(k.fooMsgClient) >= 2 {\n\t\tk.fooMsgClient.DoSomething(&MsgDoSomething{Condition: ...})\n    } else {\n        ...\n    }\n}\n```\n\nTo do the unknown field filtering itself, the ADR 033 router would need to use\nthe [protoreflect API](https://pkg.go.dev/google.golang.org/protobuf/reflect/protoreflect)\nto ensure that no fields unknown to the receiving module are set. This could\nresult in an undesirable performance hit depending on how complex this logic is.\n\n### Approach B) Changes to Generated Code\n\nAn alternate approach to solving the versioning problem is to change how protobuf code is generated and move modules\nmostly or completely in the direction of inter-module communication as described\nin [ADR 033](./adr-033-protobuf-inter-module-comm.md).\nIn this paradigm, a module could generate all the types it needs internally - including the API types of other modules -\nand talk to other modules via a client-server boundary. For instance, if `bar` needs to talk to `foo`, it could\ngenerate its own version of `MsgDoSomething` as `bar/internal/foo/v1.MsgDoSomething` and just pass this to the\ninter-module router which would somehow convert it to the version which foo needs (ex. `foo/internal.MsgDoSomething`).\n\nCurrently, two generated structs for the same protobuf type cannot exist in the same go binary without special\nbuild flags (see https://developers.google.com/protocol-buffers/docs/reference/go/faq#fix-namespace-conflict).\nA relatively simple mitigation to this issue would be to set up the protobuf code to not register protobuf types\nglobally if they are generated in an `internal/` package. This will require modules to register their types manually\nwith the app-level level protobuf registry, this is similar to what modules already do with the `InterfaceRegistry`\nand amino codec.\n\nIf modules _only_ do ADR 033 message passing then a naive and non-performant solution for\nconverting `bar/internal/foo/v1.MsgDoSomething`\nto `foo/internal.MsgDoSomething` would be marshaling and unmarshaling in the ADR 033 router. This would break down if\nwe needed to expose protobuf types in `Keeper` interfaces because the whole point is to try to keep these types\n`internal/` so that we don't end up with all the import version incompatibilities we've described above. However,\nbecause of the issue with minor version incompatibilities and the need\nfor [unknown field filtering](./adr-020-protobuf-transaction-encoding.md#unknown-field-filtering),\nsticking with the `Keeper` paradigm instead of ADR 033 may be unviable to begin with.\n\nA more performant solution (that could maybe be adapted to work with `Keeper` interfaces) would be to only expose\ngetters and setters for generated types and internally store data in memory buffers which could be passed from\none implementation to another in a zero-copy way.\n\nFor example, imagine this protobuf API with only getters and setters is exposed for `MsgSend`:\n\n```go\ntype MsgSend interface {\n\tproto.Message\n\tGetFromAddress() string\n\tGetToAddress() string\n\tGetAmount() []v1beta1.Coin\n    SetFromAddress(string)\n    SetToAddress(string)\n    SetAmount([]v1beta1.Coin)\n}\n\nfunc NewMsgSend() MsgSend { return &msgSendImpl{memoryBuffers: ...} }\n```\n\nUnder the hood, `MsgSend` could be implemented based on some raw memory buffer in the same way\nthat [Cap'n Proto](https://capnproto.org)\nand [FlatBuffers](https://google.github.io/flatbuffers/) so that we could convert between one version of `MsgSend`\nand another without serialization (i.e. zero-copy). This approach would have the added benefits of allowing zero-copy\nmessage passing to modules written in other languages such as Rust and accessed through a VM or FFI. It could also make\nunknown field filtering in inter-module communication simpler if we require that all new fields are added in sequential\norder, ex. just checking that no field `> 5` is set.\n\nAlso, we wouldn't have any issues with state machine breaking code on generated types because all the generated\ncode used in the state machine would actually live in the state machine module itself. Depending on how interface\ntypes and protobuf `Any`s are used in other languages, however, it may still be desirable to take the handler\napproach described in approach A. Either way, types implementing interfaces would still need to be registered\nwith an `InterfaceRegistry` as they are now because there would be no way to retrieve them via the global registry.\n\nIn order to simplify access to other modules using ADR 033, a public API module (maybe even one\n[remotely generated by Buf](https://buf.build/docs/bsr/generated-sdks/go/)) could be used by client modules instead\nof requiring to generate all client types internally.\n\nThe big downsides of this approach are that it requires big changes to how people use protobuf types and would be a\nsubstantial rewrite of the protobuf code generator. This new generated code, however, could still be made compatible\nwith\nthe [`google.golang.org/protobuf/reflect/protoreflect`](https://pkg.go.dev/google.golang.org/protobuf/reflect/protoreflect)\nAPI in order to work with all standard golang protobuf tooling.\n\nIt is possible that the naive approach of marshaling/unmarshaling in the ADR 033 router is an acceptable intermediate\nsolution if the changes to the code generator are seen as too complex. However, since all modules would likely need\nto migrate to ADR 033 anyway with this approach, it might be better to do this all at once.\n\n### Approach C) Don't address these issues\n\nIf the above solutions are seen as too complex, we can also decide not to do anything explicit to enable better module\nversion compatibility, and break circular dependencies.\n\nIn this case, when developers are confronted with the issues described above they can require dependencies to update in\nsync (what we do now) or attempt some ad-hoc potentially hacky solution.\n\nOne approach is to ditch go semantic import versioning (SIV) altogether. Some people have commented that go's SIV\n(i.e. changing the import path to `foo/v2`, `foo/v3`, etc.) is too restrictive and that it should be optional. The\ngolang maintainers disagree and only officially support semantic import versioning. We could, however, take the\ncontrarian perspective and get more flexibility by using 0.x-based versioning basically forever.\n\nModule version compatibility could then be achieved using go.mod replace directives to pin dependencies to specific\ncompatible 0.x versions. For instance if we knew `foo` 0.2 and 0.3 were both compatible with `bar` 0.3 and 0.4, we\ncould use replace directives in our go.mod to stick to the versions of `foo` and `bar` we want. This would work as\nlong as the authors of `foo` and `bar` avoid incompatible breaking changes between these modules.\n\nOr, if developers choose to use semantic import versioning, they can attempt the naive solution described above\nand would also need to use special tags and replace directives to make sure that modules are pinned to the correct\nversions.\n\nNote, however, that all of these ad-hoc approaches, would be vulnerable to the minor version compatibility issues\ndescribed above unless [unknown field filtering](./adr-020-protobuf-transaction-encoding.md#unknown-field-filtering)\nis properly addressed.\n\n### Approach D) Avoid protobuf generated code in public APIs\n\nAn alternative approach would be to avoid protobuf generated code in public module APIs. This would help avoid the\ndiscrepancy between state machine versions and client API versions at the module to module boundaries. It would mean\nthat we wouldn't do inter-module message passing based on ADR 033, but rather stick to the existing keeper approach\nand take it one step further by avoiding any protobuf generated code in the keeper interface methods.\n\nUsing this approach, our `foo.Keeper.DoSomething` method wouldn't have the generated `MsgDoSomething` struct (which\ncomes from the protobuf API), but instead positional parameters. Then in order for `foo/v2` to support the `foo/v1`\nkeeper it would simply need to implement both the v1 and v2 keeper APIs. The `DoSomething` method in v2 could have the\nadditional `condition` parameter, but this wouldn't be present in v1 at all so there would be no danger of a client\naccidentally setting this when it isn't available. \n\nSo this approach would avoid the challenge around minor version incompatibilities because the existing module keeper\nAPI would not get new fields when they are added to protobuf files.\n\nTaking this approach, however, would likely require making all protobuf generated code internal in order to prevent\nit from leaking into the keeper API. This means we would still need to modify the protobuf code generator to not\nregister `internal/` code with the global registry, and we would still need to manually register protobuf\n`FileDescriptor`s (this is probably true in all scenarios). It may, however, be possible to avoid needing to refactor\ninterface methods on generated types to handlers.\n\nAlso, this approach doesn't address what would be done in scenarios where modules still want to use the message router.\nEither way, we probably still want a way to pass messages from one module to another router safely even if it's just for\nuse cases like `x/gov`, `x/authz`, CosmWasm, etc. That would still require most of the things outlined in approach (B),\nalthough we could advise modules to prefer keepers for communicating with other modules.\n\nThe biggest downside of this approach is probably that it requires a strict refactoring of keeper interfaces to avoid\ngenerated code leaking into the API. This may result in cases where we need to duplicate types that are already defined\nin proto files and then write methods for converting between the golang and protobuf version. This may end up in a lot\nof unnecessary boilerplate and that may discourage modules from actually adopting it and achieving effective version\ncompatibility. Approaches (A) and (B), although heavy handed initially, aim to provide a system which once adopted\nmore or less gives the developer version compatibility for free with minimal boilerplate. Approach (D) may not be able\nto provide such a straightforward system since it requires a golang API to be defined alongside a protobuf API in a\nway that requires duplication and differing sets of design principles (protobuf APIs encourage additive changes\nwhile golang APIs would forbid it).\n\nOther downsides to this approach are:\n\n* no clear roadmap to supporting modules in other languages like Rust\n* doesn't get us any closer to proper object capability security (one of the goals of ADR 033)\n* ADR 033 needs to be done properly anyway for the set of use cases which do need it\n\n## Decision\n\nThe latest **DRAFT** proposal is:\n\n1. we are alignment on adopting [ADR 033](./adr-033-protobuf-inter-module-comm.md) not just as an addition to the\n   framework, but as a core replacement to the keeper paradigm entirely.\n2. the ADR 033 inter-module router will accommodate any variation of approach (A) or (B) given the following rules:\n   a. if the client type is the same as the server type then pass it directly through,\n   b. if both client and server use the zero-copy generated code wrappers (which still need to be defined), then pass\n   the memory buffers from one wrapper to the other, or\n   c. marshal/unmarshal types between client and server.\n\nThis approach will allow for both maximal correctness and enable a clear path to enabling modules within in other\nlanguages, possibly executed within a WASM VM.\n\n### Minor API Revisions\n\nTo declare minor API revisions of proto files, we propose the following guidelines (which were already documented\nin [cosmos.app.v1alpha module options](../proto/cosmos/app/v1alpha1/module.proto)):\n\n* proto packages which are revised from their initial version (considered revision `0`) should include a `package`\n* comment in some .proto file containing the test `Revision N` at the start of a comment line where `N` is the current\nrevision number.\n* all fields, messages, etc. added in a version beyond the initial revision should add a comment at the start of a\ncomment line of the form `Since: Revision N` where `N` is the non-zero revision it was added.\n\nIt is advised that there is a 1:1 correspondence between a state machine module and versioned set of proto files\nwhich are versioned either as a buf module a go API module or both. If the buf schema registry is used, the version of\nthis buf module should always be `1.N` where `N` corresponds to the package revision. Patch releases should be used when\nonly documentation comments are updated. It is okay to include proto packages named `v2`, `v3`, etc. in this same\n`1.N` versioned buf module (ex. `cosmos.bank.v2`) as long as all these proto packages consist of a single API intended\nto be served by a single SDK module.\n\n### Introspecting Minor API Revisions\n\nIn order for modules to introspect the minor API revision of peer modules, we propose adding the following method\nto `cosmossdk.io/core/intermodule.Client`:\n\n```go\nServiceRevision(ctx context.Context, serviceName string) uint64\n```\n\nModules could call this using the service name statically generated by the go grpc code generator:\n\n```go\nintermoduleClient.ServiceRevision(ctx, bankv1beta1.Msg_ServiceDesc.ServiceName)\n```\n\nIn the future, we may decide to extend the code generator used for protobuf services to add a field\nto client types which does this check more concisely, ex:\n\n```go\npackage bankv1beta1\n\ntype MsgClient interface {\n\tSend(context.Context, MsgSend) (MsgSendResponse, error)\n\tServiceRevision(context.Context) uint64\n}\n```\n\n### Unknown Field Filtering\n\nTo correctly perform [unknown field filtering](./adr-020-protobuf-transaction-encoding.md#unknown-field-filtering),\nthe inter-module router can do one of the following:\n\n* use the `protoreflect` API for messages which support that\n* for gogo proto messages, marshal and use the existing `codec/unknownproto` code\n* for zero-copy messages, do a simple check on the highest set field number (assuming we can require that fields are\n  adding consecutively in increasing order)\n\n### `FileDescriptor` Registration\n\nBecause a single go binary may contain different versions of the same generated protobuf code, we cannot rely on the\nglobal protobuf registry to contain the correct `FileDescriptor`s. Because `appconfig` module configuration is itself\nwritten in protobuf, we would like to load the `FileDescriptor`s for a module before loading a module itself. So we\nwill provide ways to register `FileDescriptor`s at module registration time before instantiation. We propose the\nfollowing `cosmossdk.io/core/appmodule.Option` constructors for the various cases of how `FileDescriptor`s may be\npackaged:\n\n```go\npackage appmodule\n\n// this can be used when we are using google.golang.org/protobuf compatible generated code\n// Ex:\n//   ProtoFiles(bankv1beta1.File_cosmos_bank_v1beta1_module_proto)\nfunc ProtoFiles(file []protoreflect.FileDescriptor) Option {}\n\n// this can be used when we are using gogo proto generated code.\nfunc GzippedProtoFiles(file [][]byte) Option {}\n\n// this can be used when we are using buf build to generated a pinned file descriptor\nfunc ProtoImage(protoImage []byte) Option {}\n```\n\nThis approach allows us to support several ways protobuf files might be generated:\n\n* proto files generated internally to a module (use `ProtoFiles`)\n* the API module approach with pinned file descriptors (use `ProtoImage`)\n* gogo proto (use `GzippedProtoFiles`)\n\n### Module Dependency Declaration\n\nOne risk of ADR 033 is that dependencies are called at runtime which are not present in the loaded set of SDK modules.  \nAlso we want modules to have a way to define a minimum dependency API revision that they require. Therefore, all\nmodules should declare their set of dependencies upfront. These dependencies could be defined when a module is\ninstantiated, but ideally we know what the dependencies are before instantiation and can statically look at an app\nconfig and determine whether the set of modules. For example, if `bar` requires `foo` revision `>= 1`, then we\nshould be able to know this when creating an app config with two versions of `bar` and `foo`.\n\nWe propose defining these dependencies in the proto options of the module config object itself.\n\n### Interface Registration\n\nWe will also need to define how interface methods are defined on types that are serialized as `google.protobuf.Any`'s.\nIn light of the desire to support modules in other languages, we may want to think of solutions that will accommodate\nother languages such as plugins described briefly in [ADR 033](./adr-033-protobuf-inter-module-comm.md#internal-methods).\n\n### Testing\n\nIn order to ensure that modules are indeed with multiple versions of their dependencies, we plan to provide specialized\nunit and integration testing infrastructure that automatically tests multiple versions of dependencies.\n\n#### Unit Testing\n\nUnit tests should be conducted inside SDK modules by mocking their dependencies. In a full ADR 033 scenario,\nthis means that all interaction with other modules is done via the inter-module router, so mocking of dependencies\nmeans mocking their msg and query server implementations. We will provide both a test runner and fixture to make this\nstreamlined. The key thing that the test runner should do to test compatibility is to test all combinations of\ndependency API revisions. This can be done by taking the file descriptors for the dependencies, parsing their comments\nto determine the revisions various elements were added, and then created synthetic file descriptors for each revision\nby subtracting elements that were added later.\n\nHere is a proposed API for the unit test runner and fixture:\n\n```go\npackage moduletesting\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"cosmossdk.io/core/intermodule\"\n\t\"cosmossdk.io/depinject\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"google.golang.org/protobuf/reflect/protodesc\"\n)\n\ntype TestFixture interface {\n\tcontext.Context\n\tintermodule.Client // for making calls to the module we're testing\n\tBeginBlock()\n\tEndBlock()\n}\n\ntype UnitTestFixture interface {\n\tTestFixture\n\tgrpc.ServiceRegistrar // for registering mock service implementations\n}\n\ntype UnitTestConfig struct {\n\tModuleConfig              proto.Message    // the module's config object\n\tDepinjectConfig           depinject.Config // optional additional depinject config options\n\tDependencyFileDescriptors []protodesc.FileDescriptorProto // optional dependency file descriptors to use instead of the global registry\n}\n\n// Run runs the test function for all combinations of dependency API revisions.\nfunc (cfg UnitTestConfig) Run(t *testing.T, f func(t *testing.T, f UnitTestFixture)) {\n\t// ...\n}\n```\n\nHere is an example for testing bar calling foo which takes advantage of conditional service revisions in the expected\nmock arguments:\n\n```go\nfunc TestBar(t *testing.T) {\n    UnitTestConfig{ModuleConfig: &foomodulev1.Module{}}.Run(t, func (t *testing.T, f moduletesting.UnitTestFixture) {\n        ctrl := gomock.NewController(t)\n        mockFooMsgServer := footestutil.NewMockMsgServer()\n        foov1.RegisterMsgServer(f, mockFooMsgServer)\n        barMsgClient := barv1.NewMsgClient(f)\n\t\tif f.ServiceRevision(foov1.Msg_ServiceDesc.ServiceName) >= 1 {\n            mockFooMsgServer.EXPECT().DoSomething(gomock.Any(), &foov1.MsgDoSomething{\n\t\t\t\t...,\n\t\t\t\tCondition: ..., // condition is expected in revision >= 1\n            }).Return(&foov1.MsgDoSomethingResponse{}, nil)\n        } else {\n            mockFooMsgServer.EXPECT().DoSomething(gomock.Any(), &foov1.MsgDoSomething{...}).Return(&foov1.MsgDoSomethingResponse{}, nil)\n        }\n        res, err := barMsgClient.CallFoo(f, &MsgCallFoo{})\n        ...\n    })\n}\n```\n\nThe unit test runner would make sure that no dependency mocks return arguments which are invalid for the service\nrevision being tested to ensure that modules don't incorrectly depend on functionality not present in a given revision.\n\n#### Integration Testing\n\nAn integration test runner and fixture would also be provided which instead of using mocks would test actual module\ndependencies in various combinations. Here is the proposed API:\n\n```go\ntype IntegrationTestFixture interface {\n    TestFixture\n}\n\ntype IntegrationTestConfig struct {\n    ModuleConfig     proto.Message    // the module's config object\n    DependencyMatrix map[string][]proto.Message // all the dependent module configs\n}\n\n// Run runs the test function for all combinations of dependency modules.\nfunc (cfg IntegrationTestConfig) Run(t *testing.T, f func (t *testing.T, f IntegrationTestFixture)) {\n    // ...\n}\n```\n\nAnd here is an example with foo and bar:\n\n```go\nfunc TestBarIntegration(t *testing.T) {\n    IntegrationTestConfig{\n        ModuleConfig: &barmodulev1.Module{},\n        DependencyMatrix: map[string][]proto.Message{\n            \"runtime\": []proto.Message{ // test against two versions of runtime\n                &runtimev1.Module{},\n                &runtimev2.Module{},\n            },\n            \"foo\": []proto.Message{ // test against three versions of foo\n                &foomodulev1.Module{},\n                &foomodulev2.Module{},\n                &foomodulev3.Module{},\n            }\n        }   \n    }.Run(t, func (t *testing.T, f moduletesting.IntegrationTestFixture) {\n        barMsgClient := barv1.NewMsgClient(f)\n        res, err := barMsgClient.CallFoo(f, &MsgCallFoo{})\n        ...\n    })\n}\n```\n\nUnlike unit tests, integration tests actually pull in other module dependencies. So that modules can be written\nwithout direct dependencies on other modules and because golang has no concept of development dependencies, integration\ntests should be written in separate go modules, ex. `example.com/bar/v2/test`. Because this paradigm uses go semantic\nversioning, it is possible to build a single go module which imports 3 versions of bar and 2 versions of runtime and\ncan test these all together in the six various combinations of dependencies.\n\n## Consequences\n\n### Backwards Compatibility\n\nModules which migrate fully to ADR 033 will not be compatible with existing modules which use the keeper paradigm.\nAs a temporary workaround we may create some wrapper types that emulate the current keeper interface to minimize\nthe migration overhead.\n\n### Positive\n\n* we will be able to deliver interoperable semantically versioned modules which should dramatically increase the\n  ability of the Cosmos SDK ecosystem to iterate on new features\n* it will be possible to write Cosmos SDK modules in other languages in the near future\n\n### Negative\n\n* all modules will need to be refactored somewhat dramatically\n\n### Neutral\n\n* the `cosmossdk.io/core/appconfig` framework will play a more central role in terms of how modules are defined, this\n  is likely generally a good thing but does mean additional changes for users wanting to stick to the pre-depinject way\n  of wiring up modules\n* `depinject` is somewhat less needed or maybe even obviated because of the full ADR 033 approach. If we adopt the\n  core API proposed in https://github.com/cosmos/cosmos-sdk/pull/12239, then a module would probably always instantiate\n  itself with a method `ProvideModule(appmodule.Service) (appmodule.AppModule, error)`. There is no complex wiring of\n  keeper dependencies in this scenario and dependency injection may not have as much of (or any) use case.\n\n## Further Discussions\n\nThe decision described above is considered in draft mode and is pending final buy-in from the team and key stakeholders.\nKey outstanding discussions if we do adopt that direction are:\n\n* how do module clients introspect dependency module API revisions\n* how do modules determine a minor dependency module API revision requirement\n* how do modules appropriately test compatibility with different dependency versions\n* how to register and resolve interface implementations\n* how do modules register their protobuf file descriptors depending on the approach they take to generated code (the\n  API module approach may still be viable as a supported strategy and would need pinned file descriptors)\n\n## References\n\n* https://github.com/cosmos/cosmos-sdk/discussions/10162\n* https://github.com/cosmos/cosmos-sdk/discussions/10582\n* https://github.com/cosmos/cosmos-sdk/discussions/10368\n* https://github.com/cosmos/cosmos-sdk/pull/11340\n* https://github.com/cosmos/cosmos-sdk/issues/11899\n* [ADR 020](./adr-020-protobuf-transaction-encoding.md)\n* [ADR 033](./adr-033-protobuf-inter-module-comm.md)"
  },
  {
    "number": 55,
    "filename": "adr-055-orm.md",
    "title": "ADR 055: ORM",
    "content": "# ADR 055: ORM\n\n## Changelog\n\n* 2022-04-27: First draft\n\n## Status\n\nACCEPTED Implemented\n\n## Abstract\n\nIn order to make it easier for developers to build Cosmos SDK modules and for clients to query, index and verify proofs\nagainst state data, we have implemented an ORM (object-relational mapping) layer for the Cosmos SDK.\n\n## Context\n\nHistorically modules in the Cosmos SDK have always used the key-value store directly and created various handwritten\nfunctions for managing key format as well as constructing secondary indexes. This consumes a significant amount of\ntime when building a module and is error-prone. Because key formats are non-standard, sometimes poorly documented,\nand subject to change, it is hard for clients to generically index, query and verify merkle proofs against state data.\n\nThe known first instance of an \"ORM\" in the Cosmos ecosystem was in [weave](https://github.com/iov-one/weave/tree/master/orm).\nA later version was built for [regen-ledger](https://github.com/regen-network/regen-ledger/tree/157181f955823149e1825263a317ad8e16096da4/orm) for\nuse in the group module and later [ported to the SDK](https://github.com/cosmos/cosmos-sdk/tree/35d3312c3be306591fcba39892223f1244c8d108/x/group/internal/orm)\njust for that purpose.\n\nWhile these earlier designs made it significantly easier to write state machines, they still required a lot of manual\nconfiguration, didn't expose state format directly to clients, and were limited in their support of different types\nof index keys, composite keys, and range queries.\n\nDiscussions about the design continued in https://github.com/cosmos/cosmos-sdk/discussions/9156 and more\nsophisticated proofs of concept were created in https://github.com/allinbits/cosmos-sdk-poc/tree/master/runtime/orm\nand https://github.com/cosmos/cosmos-sdk/pull/10454.\n\n## Decision\n\nThese prior efforts culminated in the creation of the Cosmos SDK `orm` go module which uses protobuf annotations\nfor specifying ORM table definitions. This ORM is based on the new `google.golang.org/protobuf/reflect/protoreflect`\nAPI and supports:\n\n* sorted indexes for all simple protobuf types (except `bytes`, `enum`, `float`, `double`) as well as `Timestamp` and `Duration`\n* unsorted `bytes` and `enum` indexes\n* composite primary and secondary keys\n* unique indexes\n* auto-incrementing `uint64` primary keys\n* complex prefix and range queries\n* paginated queries\n* complete logical decoding of KV-store data\n\nAlmost all the information needed to decode state directly is specified in .proto files. Each table definition specifies\nan ID which is unique per .proto file and each index within a table is unique within that table. Clients then only need\nto know the name of a module and the prefix ORM data for a specific .proto file within that module in order to decode\nstate data directly. This additional information will be exposed directly through app configs which will be explained\nin a future ADR related to app wiring.\n\nThe ORM makes optimizations around storage space by not repeating values in the primary key in the key value\nwhen storing primary key records. For example, if the object `{\"a\":0,\"b\":1}` has the primary key `a`, it will\nbe stored in the key value store as `Key: '0', Value: {\"b\":1}` (with more efficient protobuf binary encoding).\nAlso, the generated code from https://github.com/cosmos/cosmos-proto does optimizations around the\n`google.golang.org/protobuf/reflect/protoreflect` API to improve performance.\n\nA code generator is included with the ORM which creates type safe wrappers around the ORM's dynamic `Table`\nimplementation and is the recommended way for modules to use the ORM.\n\nThe ORM tests provide a simplified bank module demonstration which illustrates:\n\n* [ORM proto options](https://github.com/cosmos/cosmos-sdk/blob/0d846ae2f0424b2eb640f6679a703b52d407813d/orm/internal/testpb/bank.proto)\n* [Generated Code](https://github.com/cosmos/cosmos-sdk/blob/0d846ae2f0424b2eb640f6679a703b52d407813d/orm/internal/testpb/bank.cosmos_orm.go)\n* [Example Usage in a Module Keeper](https://github.com/cosmos/cosmos-sdk/blob/0d846ae2f0424b2eb640f6679a703b52d407813d/orm/model/ormdb/module_test.go)\n\n## Consequences\n\n### Backwards Compatibility\n\nState machine code that adopts the ORM will need migrations as the state layout is generally backwards incompatible.\nThese state machines will also need to migrate to https://github.com/cosmos/cosmos-proto at least for state data.\n\n### Positive\n\n* easier to build modules\n* easier to add secondary indexes to state\n* possible to write a generic indexer for ORM state\n* easier to write clients that do state proofs\n* possible to automatically write query layers rather than needing to manually implement gRPC queries\n\n### Negative\n\n* worse performance than handwritten keys (for now). See [Further Discussions](#further-discussions)\nfor potential improvements\n\n### Neutral\n\n## Further Discussions\n\nFurther discussions will happen within the Cosmos SDK Framework Working Group. Current planned and ongoing work includes:\n\n* automatically generate client-facing query layer\n* client-side query libraries that transparently verify light client proofs\n* index ORM data to SQL databases\n* improve performance by:\n    * optimizing existing reflection based code to avoid unnecessary gets when doing deletes & updates of simple tables\n    * more sophisticated code generation such as making fast path reflection even faster (avoiding `switch` statements),\n  or even fully generating code that equals handwritten performance\n\n\n## References\n\n* https://github.com/iov-one/weave/tree/master/orm).\n* https://github.com/regen-network/regen-ledger/tree/157181f955823149e1825263a317ad8e16096da4/orm\n* https://github.com/cosmos/cosmos-sdk/tree/35d3312c3be306591fcba39892223f1244c8d108/x/group/internal/orm\n* https://github.com/cosmos/cosmos-sdk/discussions/9156\n* https://github.com/allinbits/cosmos-sdk-poc/tree/master/runtime/orm\n* https://github.com/cosmos/cosmos-sdk/pull/10454"
  },
  {
    "number": 57,
    "filename": "adr-057-app-wiring.md",
    "title": "ADR 057: App Wiring",
    "content": "# ADR 057: App Wiring\n\n## Changelog\n\n* 2022-05-04: Initial Draft\n* 2022-08-19: Updates\n\n## Status\n\nPROPOSED Implemented\n\n## Abstract\n\nIn order to make it easier to build Cosmos SDK modules and apps, we propose a new app wiring system based on\ndependency injection and declarative app configurations to replace the current `app.go` code.\n\n## Context\n\nA number of factors have made the SDK and SDK apps in their current state hard to maintain. A symptom of the current\nstate of complexity is [`simapp/app.go`](https://github.com/cosmos/cosmos-sdk/blob/c3edbb22cab8678c35e21fe0253919996b780c01/simapp/app.go)\nwhich contains almost 100 lines of imports and is otherwise over 600 lines of mostly boilerplate code that is\ngenerally copied to each new project. (Not to mention the additional boilerplate which gets copied in `simapp/simd`.)\n\nThe large amount of boilerplate needed to bootstrap an app has made it hard to release independently versioned go\nmodules for Cosmos SDK modules as described in [ADR 053: Go Module Refactoring](./adr-053-go-module-refactoring.md).\n\nIn addition to being very verbose and repetitive, `app.go` also exposes a large surface area for breaking changes\nas most modules instantiate themselves with positional parameters which forces breaking changes anytime a new parameter\n(even an optional one) is needed.\n\nSeveral attempts were made to improve the current situation including [ADR 033: Internal-Module Communication](./adr-033-protobuf-inter-module-comm.md)\nand [a proof-of-concept of a new SDK](https://github.com/allinbits/cosmos-sdk-poc). The discussions around these\ndesigns led to the current solution described here.\n\n## Decision\n\nIn order to improve the current situation, a new \"app wiring\" paradigm has been designed to replace `app.go` which\ninvolves:\n\n* declaration configuration of the modules in an app which can be serialized to JSON or YAML\n* a dependency-injection (DI) framework for instantiating apps from the configuration\n\n### Dependency Injection\n\nWhen examining the code in `app.go` most of the code simply instantiates modules with dependencies provided either\nby the framework (such as store keys) or by other modules (such as keepers). It is generally pretty obvious given\nthe context what the correct dependencies actually should be, so dependency-injection is an obvious solution. Rather\nthan making developers manually resolve dependencies, a module will tell the DI container what dependency it needs\nand the container will figure out how to provide it.\n\nWe explored several existing DI solutions in golang and felt that the reflection-based approach in [uber/dig](https://github.com/uber-go/dig)\nwas closest to what we needed but not quite there. Assessing what we needed for the SDK, we designed and built\nthe Cosmos SDK [depinject module](https://pkg.go.dev/github.com/cosmos/cosmos-sdk/depinject), which has the following\nfeatures:\n\n* dependency resolution and provision through functional constructors, ex: `func(need SomeDep) (AnotherDep, error)`\n* dependency injection `In` and `Out` structs which support `optional` dependencies\n* grouped-dependencies (many-per-container) through the `ManyPerContainerType` tag interface\n* module-scoped dependencies via `ModuleKey`s (where each module gets a unique dependency)\n* one-per-module dependencies through the `OnePerModuleType` tag interface\n* sophisticated debugging information and container visualization via GraphViz\n\nHere are some examples of how these would be used in an SDK module:\n\n* `StoreKey` could be a module-scoped dependency which is unique per module\n* a module's `AppModule` instance (or the equivalent) could be a `OnePerModuleType`\n* CLI commands could be provided with `ManyPerContainerType`s\n\nNote that even though dependency resolution is dynamic and based on reflection, which could be considered a pitfall\nof this approach, the entire dependency graph should be resolved immediately on app startup and only gets resolved\nonce (except in the case of dynamic config reloading which is a separate topic). This means that if there are any\nerrors in the dependency graph, they will get reported immediately on startup so this approach is only slightly worse\nthan fully static resolution in terms of error reporting and much better in terms of code complexity.\n\n### Declarative App Config\n\nIn order to compose modules into an app, a declarative app configuration will be used. This configuration is based off\nof protobuf and its basic structure is very simple:\n\n```protobuf\npackage cosmos.app.v1;\n\nmessage Config {\n  repeated ModuleConfig modules = 1;\n}\n\nmessage ModuleConfig {\n  string name = 1;\n  google.protobuf.Any config = 2;\n}\n```\n\n(See also https://github.com/cosmos/cosmos-sdk/blob/6e18f582bf69e3926a1e22a6de3c35ea327aadce/proto/cosmos/app/v1alpha1/config.proto)\n\nThe configuration for every module is itself a protobuf message and modules will be identified and loaded based\non the protobuf type URL of their config object (ex. `cosmos.bank.module.v1.Module`). Modules are given a unique short `name`\nto share resources across different versions of the same module which might have a different protobuf package\nversions (ex. `cosmos.bank.module.v2.Module`). All module config objects should define the `cosmos.app.v1alpha1.module`\ndescriptor option which will provide additional useful metadata for the framework and which can also be indexed\nin module registries.\n\nAn example app config in YAML might look like this:\n\n```yaml\nmodules:\n  - name: baseapp\n    config:\n      \"@type\": cosmos.baseapp.module.v1.Module\n      begin_blockers: [staking, auth, bank]\n      end_blockers: [bank, auth, staking]\n      init_genesis: [bank, auth, staking]\n  - name: auth\n    config:\n      \"@type\": cosmos.auth.module.v1.Module\n      bech32_prefix: \"foo\"\n  - name: bank\n    config:\n      \"@type\": cosmos.bank.module.v1.Module\n  - name: staking\n    config:\n      \"@type\": cosmos.staking.module.v1.Module\n```\n\nIn the above example, there is a hypothetical `baseapp` module which contains the information around ordering of\nbegin blockers, end blockers, and init genesis. Rather than lifting these concerns up to the module config layer,\nthey are themselves handled by a module which could allow a convenient way of swapping out different versions of\nbaseapp (for instance to target different versions of tendermint), without needing to change the rest of the config.\nThe `baseapp` module would then provide to the server framework (which sort of sits outside the ABCI app) an instance\nof `abci.Application`.\n\nIn this model, an app is *modules all the way down* and the dependency injection/app config layer is very much\nprotocol-agnostic and can adapt to even major breaking changes at the protocol layer.\n\n### Module & Protobuf Registration\n\nIn order for the two components of dependency injection and declarative configuration to work together as described,\nwe need a way for modules to actually register themselves and provide dependencies to the container.\n\nOne additional complexity that needs to be handled at this layer is protobuf registry initialization. Recall that\nin both the current SDK `codec` and the proposed [ADR 054: Protobuf Semver Compatible Codegen](https://github.com/cosmos/cosmos-sdk/pull/11802),\nprotobuf types need to be explicitly registered. Given that the app config itself is based on protobuf and\nuses protobuf `Any` types, protobuf registration needs to happen before the app config itself can be decoded. Because\nwe don't know which protobuf `Any` types will be needed a priori and modules themselves define those types, we need\nto decode the app config in separate phases:\n\n1. parse app config JSON/YAML as raw JSON and collect required module type URLs (without doing proto JSON decoding)\n2. build a [protobuf type registry](https://pkg.go.dev/google.golang.org/protobuf@v1.28.0/reflect/protoregistry) based\n   on file descriptors and types provided by each required module\n3. decode the app config as proto JSON using the protobuf type registry\n\nBecause in [ADR 054: Protobuf Semver Compatible Codegen](https://github.com/cosmos/cosmos-sdk/pull/11802), each module\nmight use `internal` generated code which is not registered with the global protobuf registry, this code should provide\nan alternate way to register protobuf types with a type registry. In the same way that `.pb.go` files currently have a\n`var File_foo_proto protoreflect.FileDescriptor` for the file `foo.proto`, generated code should have a new member\n`var Types_foo_proto TypeInfo` where `TypeInfo` is an interface or struct with all the necessary info to register both\nthe protobuf generated types and file descriptor.\n\nSo a module must provide dependency injection providers and protobuf types, and takes as input its module\nconfig object which uniquely identifies the module based on its type URL.\n\nWith this in mind, we define a global module register which allows module implementations to register themselves\nwith the following API:\n\n```go\n// Register registers a module with the provided type name (ex. cosmos.bank.module.v1.Module)\n// and the provided options.\nfunc Register(configTypeName protoreflect.FullName, option ...Option) { ... }\n\ntype Option { /* private methods */ }\n\n// Provide registers dependency injection provider functions which work with the\n// cosmos-sdk container module. These functions can also accept an additional\n// parameter for the module's config object.\nfunc Provide(providers ...interface{}) Option { ... }\n\n// Types registers protobuf TypeInfo's with the protobuf registry.\nfunc Types(types ...TypeInfo) Option { ... }\n```\n\nEx:\n\n```go\nfunc init() {\n\tappmodule.Register(\"cosmos.bank.module.v1.Module\",\n\t\tappmodule.Types(\n\t\t\ttypes.Types_tx_proto,\n            types.Types_query_proto,\n            types.Types_types_proto,\n\t    ),\n\t    appmodule.Provide(\n\t\t\tprovideBankModule,\n\t    )\n\t)\n}\n\ntype Inputs struct {\n\tcontainer.In\n\t\n\tAuthKeeper auth.Keeper\n\tDB ormdb.ModuleDB\n}\n\ntype Outputs struct {\n\tKeeper bank.Keeper\n\tAppModule appmodule.AppModule\n}\n\nfunc ProvideBankModule(config *bankmodulev1.Module, Inputs) (Outputs, error) { ... }\n```\n\nNote that in this module, a module configuration object *cannot* register different dependency providers at runtime\nbased on the configuration. This is intentional because it allows us to know globally which modules provide which\ndependencies, and it will also allow us to do code generation of the whole app initialization. This\ncan help us figure out issues with missing dependencies in an app config if the needed modules are loaded at runtime.\nIn cases where required modules are not loaded at runtime, it may be possible to guide users to the correct module if\nthrough a global Cosmos SDK module registry.\n\nThe `*appmodule.Handler` type referenced above is a replacement for the legacy `AppModule` framework, and\ndescribed in [ADR 063: Core Module API](./adr-063-core-module-api.md).\n\n### New `app.go`\n\nWith this setup, `app.go` might now look something like this:\n\n```go\npackage main\n\nimport (\n\t// Each go package which registers a module must be imported just for side-effects\n\t// so that module implementations are registered.\n\t_ \"github.com/cosmos/cosmos-sdk/x/auth/module\"\n\t_ \"github.com/cosmos/cosmos-sdk/x/bank/module\"\n\t_ \"github.com/cosmos/cosmos-sdk/x/staking/module\"\n\t\"github.com/cosmos/cosmos-sdk/core/app\"\n)\n\n// go:embed app.yaml\nvar appConfigYAML []byte\n\nfunc main() {\n\tapp.Run(app.LoadYAML(appConfigYAML))\n}\n```\n\n### Application to existing SDK modules\n\nSo far we have described a system which is largely agnostic to the specifics of the SDK such as store keys, `AppModule`,\n`BaseApp`, etc. Improvements to these parts of the framework that integrate with the general app wiring framework\ndefined here are described in [ADR 063: Core Module API](./adr-063-core-module-api.md).\n\n### Registration of Inter-Module Hooks\n\nSome modules define a hooks interface (ex. `StakingHooks`) which allows one module to call back into another module\nwhen certain events happen.\n\nWith the app wiring framework, these hooks interfaces can be defined as a `OnePerModuleType`s and then the module\nwhich consumes these hooks can collect these hooks as a map of module name to hook type (ex. `map[string]FooHooks`). Ex:\n\n```go\nfunc init() {\n    appmodule.Register(\n        &foomodulev1.Module{},\n        appmodule.Invoke(InvokeSetFooHooks),\n\t    ...\n    )\n}\nfunc InvokeSetFooHooks(\n    keeper *keeper.Keeper,\n    fooHooks map[string]FooHooks,\n) error {\n\tfor k in sort.Strings(maps.Keys(fooHooks)) {\n\t\tkeeper.AddFooHooks(fooHooks[k])\n    }\n}\n```\n\nOptionally, the module consuming hooks can allow app's to define an order for calling these hooks based on module name\nin its config object.\n\nAn alternative way for registering hooks via reflection was considered where all keeper types are inspected to see if\nthey implement the hook interface by the modules exposing hooks. This has the downsides of:\n\n* needing to expose all the keepers of all modules to the module providing hooks,\n* not allowing for encapsulating hooks on a different type which doesn't expose all keeper methods,\n* harder to know statically which module expose hooks or are checking for them.\n\nWith the approach proposed here, hooks registration will be obviously observable in `app.go` if `depinject` codegen\n(described below) is used.\n\n### Code Generation\n\nThe `depinject` framework will optionally allow the app configuration and dependency injection wiring to be code\ngenerated. This will allow:\n\n* dependency injection wiring to be inspected as regular go code just like the existing `app.go`,\n* dependency injection to be opt-in with manual wiring 100% still possible.\n\nCode generation requires that all providers and invokers and their parameters are exported and in non-internal packages.\n\n### Module Semantic Versioning\n\nWhen we start creating semantically versioned SDK modules that are in standalone go modules, a state machine breaking\nchange to a module should be handled as follows:\n\n* the semantic major version should be incremented, and\n* a new semantically versioned module config protobuf type should be created.\n\nFor instance, if we have the SDK module for bank in the go module `github.com/cosmos/cosmos-sdk/x/bank` with the module config type\n`cosmos.bank.module.v1.Module`, and we want to make a state machine breaking change to the module, we would:\n\n* create a new go module `github.com/cosmos/cosmos-sdk/x/bank/v2`,\n* with the module config protobuf type `cosmos.bank.module.v2.Module`.\n\nThis *does not* mean that we need to increment the protobuf API version for bank. Both modules can support\n`cosmos.bank.v1`, but `github.com/cosmos/cosmos-sdk/x/bank/v2` will be a separate go module with a separate module config type.\n\nThis practice will eventually allow us to use appconfig to load new versions of a module via a configuration change.\n\nEffectively, there should be a 1:1 correspondence between a semantically versioned go module and a \nversioned module config protobuf type, and major versioning bumps should occur whenever state machine breaking changes\nare made to a module.\n\nNOTE: SDK modules that are standalone go modules *should not* adopt semantic versioning until the concerns described in\n[ADR 054: Module Semantic Versioning](./adr-054-semver-compatible-modules.md) are\naddressed. The short-term solution for this issue was left somewhat unresolved. However, the easiest tactic is\nlikely to use a standalone API go module and follow the guidelines described in this comment: https://github.com/cosmos/cosmos-sdk/pull/11802#issuecomment-1406815181. For the time-being, it is recommended that\nCosmos SDK modules continue to follow tried and true [0-based versioning](https://0ver.org) until an officially\nrecommended solution is provided. This section of the ADR will be updated when that happens and for now, this section\nshould be considered as a design recommendation for future adoption of semantic versioning.\n\n## Consequences\n\n### Backwards Compatibility\n\nModules which work with the new app wiring system do not need to drop their existing `AppModule` and `NewKeeper`\nregistration paradigms. These two methods can live side-by-side for as long as is needed.\n\n### Positive\n\n* wiring up new apps will be simpler, more succinct and less error-prone\n* it will be easier to develop and test standalone SDK modules without needing to replicate all of simapp\n* it may be possible to dynamically load modules and upgrade chains without needing to do a coordinated stop and binary\n  upgrade using this mechanism\n* easier plugin integration\n* dependency injection framework provides more automated reasoning about dependencies in the project, with a graph visualization.\n\n### Negative\n\n* it may be confusing when a dependency is missing although error messages, the GraphViz visualization, and global\n  module registration may help with that\n\n### Neutral\n\n* it will require work and education\n\n## Further Discussions\n\nThe protobuf type registration system described in this ADR has not been implemented and may need to be reconsidered in\nlight of code generation. It may be better to do this type registration with a DI provider.\n\n## References\n\n* https://github.com/cosmos/cosmos-sdk/blob/c3edbb22cab8678c35e21fe0253919996b780c01/simapp/app.go\n* https://github.com/allinbits/cosmos-sdk-poc\n* https://github.com/uber-go/dig\n* https://github.com/google/wire\n* https://pkg.go.dev/github.com/cosmos/cosmos-sdk/container\n* https://github.com/cosmos/cosmos-sdk/pull/11802\n* [ADR 063: Core Module API](./adr-063-core-module-api.md)"
  },
  {
    "number": 58,
    "filename": "adr-058-auto-generated-cli.md",
    "title": "ADR 058: Auto-Generated CLI",
    "content": "# ADR 058: Auto-Generated CLI\n\n## Changelog\n\n* 2022-05-04: Initial Draft\n\n## Status\n\nACCEPTED Partially Implemented\n\n## Abstract\n\nIn order to make it easier for developers to write Cosmos SDK modules, we provide infrastructure which automatically\ngenerates CLI commands based on protobuf definitions.\n\n## Context\n\nCurrent Cosmos SDK modules generally implement a CLI command for every transaction and every query supported by the\nmodule. These are handwritten for each command and essentially amount to providing some CLI flags or positional\narguments for specific fields in protobuf messages.\n\nIn order to make sure CLI commands are correctly implemented as well as to make sure that the application works\nin end-to-end scenarios, we do integration tests using CLI commands. While these tests are valuable on some-level,\nthey can be hard to write and maintain, and run slowly. [Some teams have contemplated](https://github.com/regen-network/regen-ledger/issues/1041)\nmoving away from CLI-style integration tests (which are really end-to-end tests) towards narrower integration tests\nwhich exercise `MsgClient` and `QueryClient` directly. This might involve replacing the current end-to-end CLI\ntests with unit tests as there still needs to be some way to test these CLI commands for full quality assurance.\n\n## Decision\n\nTo make module development simpler, we provide infrastructure - in the new [`client/v2`](https://github.com/cosmos/cosmos-sdk/tree/main/client/v2)\ngo module - for automatically generating CLI commands based on protobuf definitions to either replace or complement\nhandwritten CLI commands. This will mean that when developing a module, it will be possible to skip both writing and\ntesting CLI commands as that can all be taken care of by the framework.\n\nThe basic design for automatically generating CLI commands is to:\n\n* create one CLI command for each `rpc` method in a protobuf `Query` or `Msg` service\n* create a CLI flag for each field in the `rpc` request type\n* for `query` commands call gRPC and print the response as protobuf JSON or YAML (via the `-o`/`--output` flag)\n* for `tx` commands, create a transaction and apply common transaction flags\n\nIn order to make the auto-generated CLI as easy to use (or easier) than handwritten CLI, we need to do custom handling\nof specific protobuf field types so that the input format is easy for humans:\n\n* `Coin`, `Coins`, `DecCoin`, and `DecCoins` should be input using the existing format (i.e. `1000uatom`)\n* it should be possible to specify an address using either the bech32 address string or a named key in the keyring\n* `Timestamp` and `Duration` should accept strings like `2001-01-01T00:00:00Z` and `1h3m` respectively\n* pagination should be handled with flags like `--page-limit`, `--page-offset`, etc.\n* it should be possible to customize any other protobuf type either via its message name or a `cosmos_proto.scalar` annotation\n\nAt a basic level it should be possible to generate a command for a single `rpc` method as well as all the commands for\na whole protobuf `service` definition. It should be possible to mix and match auto-generated and handwritten commands.\n\n## Consequences\n\n### Backwards Compatibility\n\nExisting modules can mix and match auto-generated and handwritten CLI commands so it is up to them as to whether they\nmake breaking changes by replacing handwritten commands with slightly different auto-generated ones.\n\nFor now the SDK will maintain the existing set of CLI commands for backwards compatibility but new commands will use\nthis functionality.\n\n### Positive\n\n* module developers will not need to write CLI commands\n* module developers will not need to test CLI commands\n* [lens](https://github.com/strangelove-ventures/lens) may benefit from this\n\n### Negative\n\n### Neutral\n\n## Further Discussions\n\nWe would like to be able to customize:\n\n* short and long usage strings for commands\n* aliases for flags (ex. `-a` for `--amount`)\n* which fields are positional parameters rather than flags\n\nIt is an [open discussion](https://github.com/cosmos/cosmos-sdk/pull/11725#issuecomment-1108676129)\nas to whether these customizations options should lie in:\n\n* the .proto files themselves,\n* separate config files (ex. YAML), or\n* directly in code\n\nProviding the options in .proto files would allow a dynamic client to automatically generate\nCLI commands on the fly. However, that may pollute the .proto files themselves with information that is only relevant\nfor a small subset of users.\n\n## References\n\n* https://github.com/regen-network/regen-ledger/issues/1041\n* https://github.com/cosmos/cosmos-sdk/tree/main/client/v2\n* https://github.com/cosmos/cosmos-sdk/pull/11725#issuecomment-1108676129"
  },
  {
    "number": 59,
    "filename": "adr-059-test-scopes.md",
    "title": "ADR 059: Test Scopes",
    "content": "# ADR 059: Test Scopes\n\n## Changelog\n\n* 2022-08-02: Initial Draft\n* 2023-03-02: Add precision for integration tests\n* 2023-03-23: Add precision for E2E tests\n\n## Status\n\nPROPOSED Partially Implemented\n\n## Abstract\n\nRecent work in the SDK aimed at breaking apart the monolithic root go module has highlighted\nshortcomings and inconsistencies in our testing paradigm. This ADR clarifies a common\nlanguage for talking about test scopes and proposes an ideal state of tests at each scope.\n\n## Context\n\n[ADR-053: Go Module Refactoring](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-053-go-module-refactoring.md) expresses our desire for an SDK composed of many\nindependently versioned Go modules, and [ADR-057: App Wiring](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-057-app-wiring.md) offers a methodology\nfor breaking apart inter-module dependencies through the use of dependency injection. As\ndescribed in [EPIC: Separate all SDK modules into standalone go modules](https://github.com/cosmos/cosmos-sdk/issues/11899), module\ndependencies are particularly complected in the test phase, where simapp is used as\nthe key test fixture in setting up and running tests. It is clear that the successful\ncompletion of Phases 3 and 4 in that EPIC require the resolution of this dependency problem.\n\nIn [EPIC: Unit Testing of Modules via Mocks](https://github.com/cosmos/cosmos-sdk/issues/12398) it was thought this Gordian knot could be\nunwound by mocking all dependencies in the test phase for each module, but seeing how these\nrefactors were complete rewrites of test suites discussions began around the fate of the\nexisting integration tests. One perspective is that they ought to be thrown out, another is\nthat integration tests have some utility of their own and a place in the SDK's testing story.\n\nAnother point of confusion has been the current state of CLI test suites, [x/auth](https://github.com/cosmos/cosmos-sdk/blob/0f7e56c6f9102cda0ca9aba5b6f091dbca976b5a/x/auth/client/testutil/suite.go#L44-L49) for\nexample. In code these are called integration tests, but in reality function as end to end\ntests by starting up a tendermint node and full application. [EPIC: Rewrite and simplify\nCLI tests](https://github.com/cosmos/cosmos-sdk/issues/12696) identifies the ideal state of CLI tests using mocks, but does not address the\nplace end to end tests may have in the SDK.\n\nFrom here we identify three scopes of testing, **unit**, **integration**, **e2e** (end to\nend), seek to define the boundaries of each, their shortcomings (real and imposed), and their\nideal state in the SDK.\n\n### Unit tests\n\nUnit tests exercise the code contained in a single module (e.g. `/x/bank`) or package\n(e.g. `/client`) in isolation from the rest of the code base. Within this we identify two\nlevels of unit tests, *illustrative* and *journey*. The definitions below lean heavily on\n[The BDD Books - Formulation](https://leanpub.com/bddbooks-formulation) section 1.3.\n\n*Illustrative* tests exercise an atomic part of a module in isolation - in this case we\nmight do fixture setup/mocking of other parts of the module.\n\nTests which exercise a whole module's function with dependencies mocked, are *journeys*.\nThese are almost like integration tests in that they exercise many things together but still\nuse mocks.\n\nExample 1 journey vs illustrative tests - [depinject's BDD style tests](https://github.com/cosmos/cosmos-sdk/blob/main/depinject/binding_test.go), show how we can\nrapidly build up many illustrative cases demonstrating behavioral rules without [very much code](https://github.com/cosmos/cosmos-sdk/blob/main/depinject/binding_test.go) while maintaining high level readability.\n\nExample 2 [depinject table driven tests](https://github.com/cosmos/cosmos-sdk/blob/main/depinject/provider_desc_test.go)\n\nExample 3 [Bank keeper tests](https://github.com/cosmos/cosmos-sdk/blob/2bec9d2021918650d3938c3ab242f84289daef80/x/bank/keeper/keeper_test.go#L94-L105) - A mock implementation of `AccountKeeper` is supplied to the keeper constructor.\n\n#### Limitations\n\nCertain modules are tightly coupled beyond the test phase. A recent dependency report for\n`bank -> auth` found 274 total usages of `auth` in `bank`, 50 of which are in\nproduction code and 224 in test. This tight coupling may suggest that either the modules\nshould be merged, or refactoring is required to abstract references to the core types tying\nthe modules together. It could also indicate that these modules should be tested together\nin integration tests beyond mocked unit tests.\n\nIn some cases setting up a test case for a module with many mocked dependencies can be quite\ncumbersome and the resulting test may only show that the mocking framework works as expected\nrather than working as a functional test of interdependent module behavior.\n\n### Integration tests\n\nIntegration tests define and exercise relationships between an arbitrary number of modules\nand/or application subsystems.\n\nWiring for integration tests is provided by `depinject` and some [helper code](https://github.com/cosmos/cosmos-sdk/blob/2bec9d2021918650d3938c3ab242f84289daef80/testutil/sims/app_helpers.go#L95) starts up\na running application. A section of the running application may then be tested. Certain\ninputs during different phases of the application life cycle are expected to produce\ninvariant outputs without too much concern for component internals. This type of black box\ntesting has a larger scope than unit testing.\n\nExample 1 [client/grpc_query_test/TestGRPCQuery](https://github.com/cosmos/cosmos-sdk/blob/2bec9d2021918650d3938c3ab242f84289daef80/client/grpc_query_test.go#L111-L129) - This test is misplaced in `/client`,\nbut tests the life cycle of (at least) `runtime` and `bank` as they progress through\nstartup, genesis and query time. It also exercises the fitness of the client and query\nserver without putting bytes on the wire through the use of [QueryServiceTestHelper](https://github.com/cosmos/cosmos-sdk/blob/2bec9d2021918650d3938c3ab242f84289daef80/baseapp/grpcrouter_helpers.go#L31).\n\nExample 2 `x/evidence` Keeper integration tests - Starts up an application composed of [8\nmodules](https://github.com/cosmos/cosmos-sdk/blob/2bec9d2021918650d3938c3ab242f84289daef80/x/evidence/testutil/app.yaml#L1) with [5 keepers](https://github.com/cosmos/cosmos-sdk/blob/2bec9d2021918650d3938c3ab242f84289daef80/x/evidence/keeper/keeper_test.go#L101-L106) used in the integration test suite. One test in the suite\nexercises [HandleEquivocationEvidence](https://github.com/cosmos/cosmos-sdk/blob/2bec9d2021918650d3938c3ab242f84289daef80/x/evidence/keeper/infraction_test.go#L42) which contains many interactions with the staking\nkeeper.\n\nExample 3 - Integration suite app configurations may also be specified via golang (not\nYAML as above) [statically](https://github.com/cosmos/cosmos-sdk/blob/main/x/nft/testutil/app_config.go) or [dynamically](https://github.com/cosmos/cosmos-sdk/blob/8c23f6f957d1c0bedd314806d1ac65bea59b084c/tests/integration/bank/keeper/keeper_test.go#L129-L134).\n\n#### Limitations\n\nSetting up a particular input state may be more challenging since the application is\nstarting from a zero state. Some of this may be addressed by good test fixture\nabstractions with testing of their own. Tests may also be more brittle, and larger\nrefactors could impact application initialization in unexpected ways with harder to\nunderstand errors. This could also be seen as a benefit, and indeed the SDK's current\nintegration tests were helpful in tracking down logic errors during earlier stages\nof app-wiring refactors.\n\n### Simulations\n\nSimulations (also called generative testing) are a special case of integration tests where\ndeterministically random module operations are executed against a running simapp, building\nblocks on the chain until a specified height is reached. No *specific* assertions are\nmade for the state transitions resulting from module operations but any error will halt and\nfail the simulation. Since `crisis` is included in simapp and the simulation runs\nEndBlockers at the end of each block any module invariant violations will also fail\nthe simulation.\n\nModules must implement [AppModuleSimulation.WeightedOperations](https://github.com/cosmos/cosmos-sdk/blob/2bec9d2021918650d3938c3ab242f84289daef80/types/module/simulation.go#L31) to define their\nsimulation operations. Note that not all modules implement this which may indicate a\ngap in current simulation test coverage.\n\nModules not returning simulation operations:\n\n* `auth`\n* `evidence`\n* `mint`\n* `params`\n\nA separate binary, [runsim](https://github.com/cosmos/tools/tree/master/cmd/runsim), is responsible for kicking off some of these tests and\nmanaging their life cycle.\n\n#### Limitations\n\n* [A success](https://github.com/cosmos/cosmos-sdk/runs/7606931983?check_suite_focus=true) may take a long time to run, 7-10 minutes per simulation in CI.\n* [Timeouts](https://github.com/cosmos/cosmos-sdk/runs/7606932295?check_suite_focus=true) sometimes occur on apparent successes without any indication why.\n* Useful error messages not provided on [failure](https://github.com/cosmos/cosmos-sdk/runs/7606932548?check_suite_focus=true) from CI, requiring a developer to run\n  the simulation locally to reproduce.\n\n### E2E tests\n\nEnd to end tests exercise the entire system as we understand it in as close an approximation\nto a production environment as is practical. Presently these tests are located at\n[tests/e2e](https://github.com/cosmos/cosmos-sdk/tree/main/tests/e2e) and rely on [testutil/network](https://github.com/cosmos/cosmos-sdk/tree/main/testutil/network) to start up an in-process Tendermint node.\n\nAn application should be built as minimally as possible to exercise the desired functionality.\nThe SDK uses an application will only the required modules for the tests. The application developer is advised to use its own application for e2e tests.\n\n#### Limitations\n\nIn general the limitations of end to end tests are orchestration and compute cost.\nScaffolding is required to start up and run a prod-like environment and this\nprocess takes much longer to start and run than unit or integration tests.\n\nGlobal locks present in Tendermint code cause stateful starting/stopping to sometimes hang\nor fail intermittently when run in a CI environment.\n\nThe scope of e2e tests has been complected with command line interface testing.\n\n## Decision\n\nWe accept these test scopes and identify the following decisions points for each.\n\n| Scope       | App Type            | Mocks? |\n| ----------- | ------------------- | ------ |\n| Unit        | None                | Yes    |\n| Integration | integration helpers | Some   |\n| Simulation  | minimal app         | No     |\n| E2E         | minimal app         | No     |\n\nThe decision above is valid for the SDK. An application developer should test their application with their full application instead of the minimal app.\n\n### Unit Tests\n\nAll modules must have mocked unit test coverage.\n\nIllustrative tests should outnumber journeys in unit tests.\n\nUnit tests should outnumber integration tests.\n\nUnit tests must not introduce additional dependencies beyond those already present in\nproduction code.\n\nWhen module unit test introduction as per [EPIC: Unit testing of modules via mocks](https://github.com/cosmos/cosmos-sdk/issues/12398)\nresults in a near complete rewrite of an integration test suite the test suite should be\nretained and moved to `/tests/integration`. We accept the resulting test logic\nduplication but recommend improving the unit test suite through the addition of\nillustrative tests.\n\n### Integration Tests\n\nAll integration tests shall be located in `/tests/integration`, even those which do not\nintroduce extra module dependencies.\n\nTo help limit scope and complexity, it is recommended to use the smallest possible number of\nmodules in application startup, i.e. don't depend on simapp.\n\nIntegration tests should outnumber e2e tests.\n\n### Simulations\n\nSimulations shall use a minimal application (usually via app wiring). They are located under `/x/{moduleName}/simulation`.\n\n### E2E Tests\n\nExisting e2e tests shall be migrated to integration tests by removing the dependency on the\ntest network and in-process Tendermint node to ensure we do not lose test coverage.\n\nThe e2e rest runner shall transition from in process Tendermint to a runner powered by\nDocker via [dockertest](https://github.com/ory/dockertest).\n\nE2E tests exercising a full network upgrade shall be written.\n\nThe CLI testing aspect of existing e2e tests shall be rewritten using the network mocking\ndemonstrated in [PR#12706](https://github.com/cosmos/cosmos-sdk/pull/12706).\n\n## Consequences\n\n### Positive\n\n* test coverage is increased\n* test organization is improved\n* reduced dependency graph size in modules\n* simapp removed as a dependency from modules\n* inter-module dependencies introduced in test code are removed\n* reduced CI run time after transitioning away from in process Tendermint\n\n### Negative\n\n* some test logic duplication between unit and integration tests during transition\n* test written using dockertest DX may be a bit worse\n\n### Neutral\n\n* some discovery required for e2e transition to dockertest\n\n## Further Discussions\n\nIt may be useful if test suites could be run in integration mode (with mocked tendermint) or\nwith e2e fixtures (with real tendermint and many nodes). Integration fixtures could be used\nfor quicker runs, e2e fixtures could be used for more battle hardening.\n\nA PoC `x/gov` was completed in PR [#12847](https://github.com/cosmos/cosmos-sdk/pull/12847)\nis in progress for unit tests demonstrating BDD [Rejected].\nObserving that a strength of BDD specifications is their readability, and a con is the\ncognitive load while writing and maintaining, current consensus is to reserve BDD use\nfor places in the SDK where complex rules and module interactions are demonstrated.\nMore straightforward or low level test cases will continue to rely on go table tests.\n\nLevels are network mocking in integration and e2e tests are still being worked on and formalized."
  },
  {
    "number": 60,
    "filename": "adr-060-abci-1.0.md",
    "title": "ADR 60: ABCI 1.0 Integration (Phase I)",
    "content": "# ADR 60: ABCI 1.0 Integration (Phase I)\n\n## Changelog\n\n* 2022-08-10: Initial Draft (@alexanderbez, @tac0turtle)\n* Nov 12, 2022: Update `PrepareProposal` and `ProcessProposal` semantics per the\n  initial implementation [PR](https://github.com/cosmos/cosmos-sdk/pull/13453) (@alexanderbez)\n\n## Status\n\nACCEPTED\n\n## Abstract\n\nThis ADR describes the initial adoption of [ABCI 1.0](https://github.com/tendermint/tendermint/blob/master/spec/abci%2B%2B/README.md),\nthe next evolution of ABCI, within the Cosmos SDK. ABCI 1.0 aims to provide\napplication developers with more flexibility and control over application and\nconsensus semantics, e.g. in-application mempools, in-process oracles, and\norder-book style matching engines.\n\n## Context\n\nTendermint will release ABCI 1.0. Notably, at the time of this writing,\nTendermint is releasing v0.37.0 which will include `PrepareProposal` and `ProcessProposal`.\n\nThe `PrepareProposal` ABCI method is concerned with a block proposer requesting\nthe application to evaluate a series of transactions to be included in the next\nblock, defined as a slice of `TxRecord` objects. The application can either\naccept, reject, or completely ignore some or all of these transactions. This is\nan important consideration to make as the application can essentially define and\ncontrol its own mempool allowing it to define sophisticated transaction priority\nand filtering mechanisms, by completely ignoring the `TxRecords` Tendermint\nsends it, favoring its own transactions. This essentially means that the Tendermint\nmempool acts more like a gossip data structure.\n\nThe second ABCI method, `ProcessProposal`, is used to process the block proposer's\nproposal as defined by `PrepareProposal`. It is important to note the following\nwith respect to `ProcessProposal`:\n\n* Execution of `ProcessProposal` must be deterministic.\n* There must be coherence between `PrepareProposal` and `ProcessProposal`. In\n  other words, for any two correct processes *p* and *q*, if *q*'s Tendermint\n\tcalls `RequestProcessProposal` on *u<sub>p</sub>*, *q*'s Application returns\n\tACCEPT in `ResponseProcessProposal`.\n\nIt is important to note that in ABCI 1.0 integration, the application\nis NOT responsible for locking semantics -- Tendermint will still be responsible\nfor that. In the future, however, the application will be responsible for locking,\nwhich allows for parallel execution possibilities.\n\n## Decision\n\nWe will integrate ABCI 1.0, which will be introduced in Tendermint\nv0.37.0, in the next major release of the Cosmos SDK. We will integrate ABCI 1.0\nmethods on the `BaseApp` type. We describe the implementations of the two methods\nindividually below.\n\nPrior to describing the implementation of the two new methods, it is important to\nnote that the existing ABCI methods, `CheckTx`, `DeliverTx`, etc, still exist and\nserve the same functions as they do now.\n\n### `PrepareProposal`\n\nPrior to evaluating the decision for how to implement `PrepareProposal`, it is\nimportant to note that `CheckTx` will still be executed and will be responsible\nfor evaluating transaction validity as it does now, with one very important\n*additive* distinction.\n\nWhen executing transactions in `CheckTx`, the application will now add valid\ntransactions, i.e. passing the AnteHandler, to its own mempool data structure.\nIn order to provide a flexible approach to meet the varying needs of application\ndevelopers, we will define both a mempool interface and a data structure utilizing\nGolang generics, allowing developers to focus only on transaction\nordering. Developers requiring absolute full control can implement their own\ncustom mempool implementation.\n\nWe define the general mempool interface as follows (subject to change):\n\n```go\ntype Mempool interface {\n\t// Insert attempts to insert a Tx into the app-side mempool returning\n\t// an error upon failure.\n\tInsert(sdk.Context, sdk.Tx) error\n\n\t// Select returns an Iterator over the app-side mempool. If txs are specified,\n\t// then they shall be incorporated into the Iterator. The Iterator must\n\t// be closed by the caller.\n\tSelect(sdk.Context, [][]byte) Iterator\n\n\t// CountTx returns the number of transactions currently in the mempool.\n\tCountTx() int\n\n\t// Remove attempts to remove a transaction from the mempool, returning an error\n\t// upon failure.\n\tRemove(sdk.Tx) error\n}\n\n// Iterator defines an app-side mempool iterator interface that is as minimal as\n// possible. The order of iteration is determined by the app-side mempool\n// implementation.\ntype Iterator interface {\n\t// Next returns the next transaction from the mempool. If there are no more\n\t// transactions, it returns nil.\n\tNext() Iterator\n\n\t// Tx returns the transaction at the current position of the iterator.\n\tTx() sdk.Tx\n}\n```\n\nWe will define an implementation of `Mempool`, defined by `nonceMempool`, that\nwill cover most basic application use-cases. Namely, it will prioritize transactions\nby transaction sender, allowing for multiple transactions from the same sender.\n\nThe default app-side mempool implementation, `nonceMempool`, will operate on a \nsingle skip list data structure. Specifically, transactions with the lowest nonce\nglobally are prioritized. Transactions with the same nonce are prioritized by\nsender address.\n\n```go\ntype nonceMempool struct {\n\ttxQueue *huandu.SkipList\n}\n```\n\nPrevious discussions<sup>1</sup> have come to the agreement that Tendermint will\nperform a request to the application, via `RequestPrepareProposal`, with a certain\namount of transactions reaped from Tendermint's local mempool. The exact amount\nof transactions reaped will be determined by a local operator configuration.\nThis is referred to as the \"one-shot approach\" seen in discussions.\n\nWhen Tendermint reaps transactions from the local mempool and sends them to the\napplication via `RequestPrepareProposal`, the application will have to evaluate\nthe transactions. Specifically, it will need to inform Tendermint if it should\nreject and or include each transaction. Note, the application can even *replace*\ntransactions entirely with other transactions.\n\nWhen evaluating transactions from `RequestPrepareProposal`, the application will\nignore *ALL* transactions sent to it in the request and instead reap up to\n`RequestPrepareProposal.max_tx_bytes` from it's own mempool.\n\nSince an application can technically insert or inject transactions on `Insert`\nduring `CheckTx` execution, it is recommended that applications ensure transaction\nvalidity when reaping transactions during `PrepareProposal`. However, what validity\nexactly means is entirely determined by the application.\n\nThe Cosmos SDK will provide a default `PrepareProposal` implementation that simply\nselect up to `MaxBytes` *valid* transactions.\n\nHowever, applications can override this default implementation with their own\nimplementation and set that on `BaseApp` via `SetPrepareProposal`.\n\n\n### `ProcessProposal`\n\nThe `ProcessProposal` ABCI method is relatively straightforward. It is responsible\nfor ensuring validity of the proposed block containing transactions that were\nselected from the `PrepareProposal` step. However, how an application determines\nvalidity of a proposed block depends on the application and its varying use cases.\nFor most applications, simply calling the `AnteHandler` chain would suffice, but\nthere could easily be other applications that need more control over the validation\nprocess of the proposed block, such as ensuring txs are in a certain order or\nthat certain transactions are included. While this theoretically could be achieved\nwith a custom `AnteHandler` implementation, it's not the cleanest UX or the most\nefficient solution.\n\nInstead, we will define an additional ABCI interface method on the existing\n`Application` interface, similar to the existing ABCI methods such as `BeginBlock`\nor `EndBlock`. This new interface method will be defined as follows:\n\n```go\nProcessProposal(sdk.Context, abci.RequestProcessProposal) error {}\n```\n\nNote, we must call `ProcessProposal` with a new internal branched state on the\n`Context` argument as we cannot simply just use the existing `checkState` because\n`BaseApp` already has a modified `checkState` at this point. So when executing\n`ProcessProposal`, we create a similar branched state, `processProposalState`,\noff of `deliverState`. Note, the `processProposalState` is never committed and\nis completely discarded after `ProcessProposal` finishes execution.\n\nThe Cosmos SDK will provide a default implementation of `ProcessProposal` in which\nall transactions are validated using the CheckTx flow, i.e. the AnteHandler, and\nwill always return ACCEPT unless any transaction cannot be decoded.\n\n### `DeliverTx`\n\nSince transactions are not truly removed from the app-side mempool during\n`PrepareProposal`, since `ProcessProposal` can fail or take multiple rounds and\nwe do not want to lose transactions, we need to finally remove the transaction\nfrom the app-side mempool during `DeliverTx` since during this phase, the\ntransactions are being included in the proposed block.\n\nAlternatively, we can keep the transactions as truly being removed during the\nreaping phase in `PrepareProposal` and add them back to the app-side mempool in\ncase `ProcessProposal` fails.\n\n## Consequences\n\n### Backwards Compatibility\n\nABCI 1.0 is naturally not backwards compatible with prior versions of the Cosmos SDK\nand Tendermint. For example, an application that requests `RequestPrepareProposal`\nto the same application that does not speak ABCI 1.0 will naturally fail.\n\nHowever, in the first phase of the integration, the existing ABCI methods as we\nknow them today will still exist and function as they currently do.\n\n### Positive\n\n* Applications now have full control over transaction ordering and priority.\n* Lays the groundwork for the full integration of ABCI 1.0, which will unlock more\n  app-side use cases around block construction and integration with the Tendermint\n  consensus engine.\n\n### Negative\n\n* Requires that the \"mempool\", as a general data structure that collects and stores\n  uncommitted transactions will be duplicated between both Tendermint and the\n  Cosmos SDK.\n* Additional requests between Tendermint and the Cosmos SDK in the context of\n  block execution. Albeit, the overhead should be negligible.\n* Not backwards compatible with previous versions of Tendermint and the Cosmos SDK.\n\n## Further Discussions\n\nIt is possible to design the app-side implementation of the `Mempool[T MempoolTx]`\nin many different ways using different data structures and implementations. All\nof which have different tradeoffs. The proposed solution keeps things simple\nand covers cases that would be required for most basic applications. There are\ntradeoffs that can be made to improve performance of reaping and inserting into\nthe provided mempool implementation.\n\n## References\n\n* https://github.com/tendermint/tendermint/blob/master/spec/abci%2B%2B/README.md\n* [1] https://github.com/tendermint/tendermint/issues/7750#issuecomment-1076806155\n* [2] https://github.com/tendermint/tendermint/issues/7750#issuecomment-1075717151"
  },
  {
    "number": 61,
    "filename": "adr-061-liquid-staking.md",
    "title": "# ADR 061: Liquid Staking",
    "content": "# ADR-061: Liquid Staking\n\n## Changelog\n\n* 2022-09-10: Initial Draft (@zmanian)\n\n## Status\n\nACCEPTED\n\n## Abstract\n\nAdd a semi-fungible liquid staking primitive to the default Cosmos SDK staking module. This upgrades proof of stake to enable safe designs with lower overall monetary issuance and integration with numerous liquid staking protocols like Stride, Persistence, Quicksilver, Lido etc.\n\n## Context\n\nThe original release of the Cosmos Hub featured the implementation of a ground breaking proof of stake mechanism featuring delegation, slashing, in protocol reward distribution and adaptive issuance. This design was state of the art for 2016 and has been deployed without major changes by many L1 blockchains.\n\nAs both Proof of Stake and blockchain use cases have matured, this design has aged poorly and should no longer be considered a good baseline Proof of Stake issuance. In the world of application specific blockchains, there cannot be a one size fits all blockchain but the Cosmos SDK does endeavour to provide a good baseline implementation and one that is suitable for the Cosmos Hub.\n\nThe most important deficiency of the legacy staking design is that it composes poorly with on chain protocols for trading, lending, derivatives that are referred to collectively as DeFi. The legacy staking implementation starves these applications of liquidity by increasing the risk free rate adaptively. It basically makes DeFi and staking security somewhat incompatible. \n\nThe Osmosis team has adopted the idea of Superfluid and Interfluid staking where assets that are participating in DeFi applications can also be used in proof of stake. This requires tight integration with an enshrined set of DeFi applications and thus is unsuitable for the Cosmos SDK.\n\nIt's also important to note that Interchain Accounts are available in the default IBC implementation and can be used to [rehypothecate](https://www.investopedia.com/terms/h/hypothecation.asp#toc-what-is-rehypothecation) delegations. Thus liquid staking is already possible and these changes merely improve the UX of liquid staking. Centralized exchanges also rehypothecate staked assets, posing challenges for decentralization. This ADR takes the position that adoption of in-protocol liquid staking is the preferable outcome and provides new levers to incentivize decentralization of stake. \n\nThese changes to the staking module have been in development for more than a year and have seen substantial industry adoption who plan to build staking UX. The internal economics at Informal team has also done a review of the impacts of these changes and this review led to the development of the exempt delegation system. This system provides governance with a tuneable parameter for modulating the risks of principal agent problem called the exemption factor. \n\n## Decision\n\nWe implement the semi-fungible liquid staking system and exemption factor system within the cosmos sdk. Though registered as fungible assets, these tokenized shares have extremely limited fungibility, only among the specific delegation record that was created when shares were tokenized. These assets can be used for OTC trades but composability with DeFi is limited. The primary expected use case is improving the user experience of liquid staking providers.\n\nA new governance parameter is introduced that defines the ratio of exempt to issued tokenized shares. This is called the exemption factor. A larger exemption factor allows more tokenized shares to be issued for a smaller amount of exempt delegations. If governance is comfortable with how the liquid staking market is evolving, it makes sense to increase this value.\n\nMin self delegation is removed from the staking system with the expectation that it will be replaced by the exempt delegations system. The exempt delegation system allows multiple accounts to demonstrate economic alignment with the validator operator as team members, partners etc. without co-mingling funds. Delegation exemption will likely be required to grow the validators' business under widespread adoption of liquid staking once governance has adjusted the exemption factor.\n\nWhen shares are tokenized, the underlying shares are transferred to a module account and rewards go to the module account for the TokenizedShareRecord. \n\nThere is no longer a mechanism to override the validators vote for TokenizedShares.\n\n\n### `MsgTokenizeShares`\n\nThe MsgTokenizeShares message is used to create tokenize delegated tokens. This message can be executed by any delegator who has positive amount of delegation and after execution the specific amount of delegation disappear from the account and share tokens are provided. Share tokens are denominated in the validator and record id of the underlying delegation.\n\nA user may tokenize some or all of their delegation.\n\nThey will receive shares with the denom of `cosmosvaloper1xxxx/5` where 5 is the record id for the validator operator.\n\nMsgTokenizeShares fails if the account is a VestingAccount. Users will have to move vested tokens to a new account and endure the unbonding period. We view this as an acceptable tradeoff vs. the complex book keeping required to track vested tokens.\n\nThe total amount of outstanding tokenized shares for the validator is checked against the sum of exempt delegations multiplied by the exemption factor. If the tokenized shares exceeds this limit, execution fails.\n\nMsgTokenizeSharesResponse provides the number of tokens generated and their denom.\n\n\n### `MsgRedeemTokensforShares`\n\nThe MsgRedeemTokensforShares message is used to redeem the delegation from share tokens. This message can be executed by any user who owns share tokens. After execution delegations will appear to the user.\n\n### `MsgTransferTokenizeShareRecord`\n\nThe MsgTransferTokenizeShareRecord message is used to transfer the ownership of rewards generated from the tokenized amount of delegation. The tokenize share record is created when a user tokenize his/her delegation and deleted when the full amount of share tokens are redeemed.\n\nThis is designed to work with liquid staking designs that do not redeem the tokenized shares and may instead want to keep the shares tokenized.\n\n\n### `MsgExemptDelegation`\n\nThe MsgExemptDelegation message is used to exempt a delegation to a validator. If the exemption factor is greater than 0, this will allow more delegation shares to be issued from the validator.\n\nThis design allows the chain to force an amount of self-delegation by validators participating in liquid staking schemes.\n\n## Consequences\n\n### Backwards Compatibility\n\nBy setting the exemption factor to zero, this module works like legacy staking. The only substantial change is the removal of min-self-bond and without any tokenized shares, there is no incentive to exempt delegation. \n\n### Positive\n\nThis approach should enable integration with liquid staking providers and improved user experience. It provides a pathway to security under non-exponential issuance policies in the baseline staking module."
  },
  {
    "number": 62,
    "filename": "adr-062-collections-state-layer.md",
    "title": "ADR 062: Collections, a simplified storage layer for cosmos-sdk modules",
    "content": "# ADR 062: Collections, a simplified storage layer for cosmos-sdk modules\n\n## Changelog\n\n* 30/11/2022: PROPOSED\n\n## Status\n\nPROPOSED - Implemented\n\n## Abstract\n\nWe propose a simplified module storage layer which leverages golang generics to allow module developers to handle module\nstorage in a simple and straightforward manner, whilst offering safety, extensibility and standardization.\n\n## Context\n\nModule developers are forced into manually implementing storage functionalities in their modules, those functionalities include\nbut are not limited to:\n\n* Defining key to bytes formats.\n* Defining value to bytes formats.\n* Defining secondary indexes.\n* Defining query methods to expose outside to deal with storage.\n* Defining local methods to deal with storage writing.\n* Dealing with genesis imports and exports.\n* Writing tests for all the above.\n\n\nThis brings in a lot of problems:\n\n* It blocks developers from focusing on the most important part: writing business logic.\n* Key to bytes formats are complex and their definition is error-prone, for example:\n    * how do I format time to bytes in such a way that bytes are sorted?\n    * how do I ensure when I don't have namespace collisions when dealing with secondary indexes?\n* The lack of standardization makes life hard for clients, and the problem is exacerbated when it comes to providing proofs for objects present in state. Clients are forced to maintain a list of object paths to gather proofs.\n\n### Current Solution: ORM\n\nThe current SDK proposed solution to this problem is [ORM](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-055-orm.md).\nWhilst ORM offers a lot of good functionality aimed at solving these specific problems, it has some downsides:\n\n* It requires migrations.\n* It uses the newest protobuf golang API, whilst the SDK still mainly uses gogoproto. \n* Integrating ORM into a module would require the developer to deal with two different golang frameworks (golang protobuf + gogoproto) representing the same API objects.\n* It has a high learning curve, even for simple storage layers as it requires developers to have knowledge around protobuf options, custom cosmos-sdk storage extensions, and tooling download. Then after this they still need to learn the code-generated API.\n\n### CosmWasm Solution: cw-storage-plus\n\nThe collections API takes inspiration from [cw-storage-plus](https://docs.cosmwasm.com/docs/1.0/smart-contracts/state/cw-plus/),\nwhich has demonstrated to be a powerful tool for dealing with storage in CosmWasm contracts.\nIt's simple, does not require extra tooling, it makes it easy to deal with complex storage structures (indexes, snapshot, etc).\nThe API is straightforward and explicit.\n\n## Decision\n\nWe propose to port the `collections` API, whose implementation lives in [NibiruChain/collections](https://github.com/NibiruChain/collections) to cosmos-sdk.\n\nCollections implements four different storage handlers types:\n\n* `Map`: which deals with simple `key=>object` mappings.\n* `KeySet`: which acts as a `Set` and only retains keys and no object (usecase: allow-lists).\n* `Item`: which always contains only one object (usecase: Params)\n* `Sequence`: which implements a simple always increasing number (usecase: Nonces)\n* `IndexedMap`: builds on top of `Map` and `KeySet` and allows to create relationships with `Objects` and `Objects` secondary keys.\n\nAll the collection APIs build on top of the simple `Map` type.\n\nCollections is fully generic, meaning that anything can be used as `Key` and `Value`. It can be a protobuf object or not.\n\nCollections types, in fact, delegate the duty of serialization of keys and values to a secondary collections API component called `ValueEncoders` and `KeyEncoders`.\n\n`ValueEncoders` take care of converting a value to bytes (relevant only for `Map`). And offers a plug and play layer which allows us to change how we encode objects, \nwhich is relevant for swapping serialization frameworks and enhancing performance.\n`Collections` already comes in with default `ValueEncoders`, specifically for: protobuf objects, special SDK types (sdk.Int, sdk.Dec).\n\n`KeyEncoders` take care of converting keys to bytes, `collections` already comes in with some default `KeyEncoders` for some primitive golang types\n(uint64, string, time.Time, ...) and some widely used sdk types (sdk.Acc/Val/ConsAddress, sdk.Int/Dec, ...).\nThese default implementations also offer safety around proper lexicographic ordering and namespace-collision.\n\nExamples of the collections API can be found here:\n\n* introduction: https://github.com/NibiruChain/collections/tree/main/examples\n* usage in nibiru: [x/oracle](https://github.com/NibiruChain/nibiru/blob/master/x/oracle/keeper/keeper.go#L32), [x/perp](https://github.com/NibiruChain/nibiru/blob/master/x/perp/keeper/keeper.go#L31)\n* cosmos-sdk's x/staking migrated: https://github.com/testinginprod/cosmos-sdk/pull/22\n\n\n## Consequences\n\n### Backwards Compatibility\n\nThe design of `ValueEncoders` and `KeyEncoders` allows modules to retain the same `byte(key)=>byte(value)` mappings, making\nthe upgrade to the new storage layer non-state breaking.\n\n\n### Positive\n\n* ADR aimed at removing code from the SDK rather than adding it. Migrating just `x/staking` to collections would yield to a net decrease in LOC (even considering the addition of collections itself).\n* Simplifies and standardizes storage layers across modules in the SDK.\n* Does not require to have to deal with protobuf.\n* It's pure golang code.\n* Generalization over `KeyEncoders` and `ValueEncoders` allows us to not tie ourself to the data serialization framework.\n* `KeyEncoders` and `ValueEncoders` can be extended to provide schema reflection.\n\n### Negative\n\n* Golang generics are not as battle-tested as other Golang features, despite being used in production right now.\n* Collection types instantiation needs to be improved.\n\n### Neutral\n\n{neutral consequences}\n\n## Further Discussions\n\n* Automatic genesis import/export (not implemented because of API breakage)\n* Schema reflection\n\n\n## References"
  },
  {
    "number": 63,
    "filename": "adr-063-core-module-api.md",
    "title": "ADR 063: Core Module API",
    "content": "# ADR 063: Core Module API\n\n## Changelog\n\n* 2022-08-18 First Draft\n* 2022-12-08 First Draft\n* 2023-01-24 Updates\n\n## Status\n\nACCEPTED Partially Implemented\n\n## Abstract\n\nA new core API is proposed as a way to develop cosmos-sdk applications that will eventually replace the existing\n`AppModule` and `sdk.Context` frameworks a set of core services and extension interfaces. This core API aims to:\n\n* be simpler\n* more extensible\n* more stable than the current framework\n* enable deterministic events and queries,\n* support event listeners\n* [ADR 033: Protobuf-based Inter-Module Communication](./adr-033-protobuf-inter-module-comm.md) clients.\n\n## Context\n\nHistorically modules have exposed their functionality to the framework via the `AppModule` and `AppModuleBasic`\ninterfaces which have the following shortcomings:\n\n* both `AppModule` and `AppModuleBasic` need to be defined and registered which is counter-intuitive\n* apps need to implement the full interfaces, even parts they don't need (although there are workarounds for this),\n* interface methods depend heavily on unstable third party dependencies, in particular Comet,\n* legacy required methods have littered these interfaces for far too long\n\nIn order to interact with the state machine, modules have needed to do a combination of these things:\n\n* get store keys from the app\n* call methods on `sdk.Context` which contains more or less the full set of capability available to modules.\n\nBy isolating all the state machine functionality into `sdk.Context`, the set of functionalities available to\nmodules are tightly coupled to this type. If there are changes to upstream dependencies (such as Comet)\nor new functionalities are desired (such as alternate store types), the changes need impact `sdk.Context` and all\nconsumers of it (basically all modules). Also, all modules now receive `context.Context` and need to convert these\nto `sdk.Context`'s with a non-ergonomic unwrapping function.\n\nAny breaking changes to these interfaces, such as ones imposed by third-party dependencies like Comet, have the\nside effect of forcing all modules in the ecosystem to update in lock-step. This means it is almost impossible to have\na version of the module which can be run with 2 or 3 different versions of the SDK or 2 or 3 different versions of\nanother module. This lock-step coupling slows down overall development within the ecosystem and causes updates to\ncomponents to be delayed longer than they would if things were more stable and loosely coupled.\n\n## Decision\n\nThe `core` API proposes a set of core APIs that modules can rely on to interact with the state machine and expose their\nfunctionalities to it that are designed in a principled way such that:\n\n* tight coupling of dependencies and unrelated functionalities is minimized or eliminated\n* APIs can have long-term stability guarantees\n* the SDK framework is extensible in a safe and straightforward way\n\nThe design principles of the core API are as follows:\n\n* everything that a module wants to interact with in the state machine is a service\n* all services coordinate state via `context.Context` and don't try to recreate the \"bag of variables\" approach of `sdk.Context`\n* all independent services are isolated in independent packages with minimal APIs and minimal dependencies\n* the core API should be minimalistic and designed for long-term support (LTS)\n* a \"runtime\" module will implement all the \"core services\" defined by the core API and can handle all module\n  functionalities exposed by core extension interfaces\n* other non-core and/or non-LTS services can be exposed by specific versions of runtime modules or other modules \nfollowing the same design principles, this includes functionality that interacts with specific non-stable versions of\nthird party dependencies such as Comet\n* the core API doesn't implement *any* functionality, it just defines types\n* go stable API compatibility guidelines are followed: https://go.dev/blog/module-compatibility\n\nA \"runtime\" module is any module which implements the core functionality of composing an ABCI app, which is currently\nhandled by `BaseApp` and the `ModuleManager`. Runtime modules which implement the core API are *intentionally* separate\nfrom the core API in order to enable more parallel versions and forks of the runtime module than is possible with the\nSDK's current tightly coupled `BaseApp` design while still allowing for a high degree of composability and\ncompatibility.\n\nModules which are built only against the core API don't need to know anything about which version of runtime,\n`BaseApp` or Comet in order to be compatible. Modules from the core mainline SDK could be easily composed\nwith a forked version of runtime with this pattern.\n\nThis design is intended to enable matrices of compatible dependency versions. Ideally a given version of any module\nis compatible with multiple versions of the runtime module and other compatible modules. This will allow dependencies\nto be selectively updated based on battle-testing. More conservative projects may want to update some dependencies\nslower than more fast moving projects.\n\n### Core Services\n\nThe following \"core services\" are defined by the core API. All valid runtime module implementations should provide\nimplementations of these services to modules via both [dependency injection](./adr-057-app-wiring.md) and\nmanual wiring. The individual services described below are all bundled in a convenient `appmodule.Service`\n\"bundle service\" so that for simplicity modules can declare a dependency on a single service.\n\n#### Store Services\n\nStore services will be defined in the `cosmossdk.io/core/store` package.\n\nThe generic `store.KVStore` interface is the same as current SDK `KVStore` interface. Store keys have been refactored\ninto store services which, instead of expecting the context to know about stores, invert the pattern and allow\nretrieving a store from a generic context. There are three store services for the three types of currently supported\nstores - regular kv-store, memory, and transient:\n\n```go\ntype KVStoreService interface {\n    OpenKVStore(context.Context) KVStore\n}\n\ntype MemoryStoreService interface {\n    OpenMemoryStore(context.Context) KVStore\n}\ntype TransientStoreService interface {\n    OpenTransientStore(context.Context) KVStore\n}\n```\n\nModules can use these services like this:\n\n```go\nfunc (k msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\n    store := k.kvStoreSvc.OpenKVStore(ctx)\n}\n```\n\nJust as with the current runtime module implementation, modules will not need to explicitly name these store keys,\nbut rather the runtime module will choose an appropriate name for them and modules just need to request the\ntype of store they need in their dependency injection (or manual) constructors.\n\n#### Event Service\n\nThe event `Service` will be defined in the `cosmossdk.io/core/event` package.\n\nThe event `Service` allows modules to emit typed and legacy untyped events:\n\n```go\npackage event\n\ntype Service interface {\n  // EmitProtoEvent emits events represented as a protobuf message (as described in ADR 032).\n  //\n  // Callers SHOULD assume that these events may be included in consensus. These events\n  // MUST be emitted deterministically and adding, removing or changing these events SHOULD\n  // be considered state-machine breaking.\n  EmitProtoEvent(ctx context.Context, event protoiface.MessageV1) error\n\n  // EmitKVEvent emits an event based on an event and kv-pair attributes.\n  //\n  // These events will not be part of consensus and adding, removing or changing these events is\n  // not a state-machine breaking change.\n  EmitKVEvent(ctx context.Context, eventType string, attrs ...KVEventAttribute) error\n\n  // EmitProtoEventNonConsensus emits events represented as a protobuf message (as described in ADR 032), without\n  // including it in blockchain consensus.\n  //\n  // These events will not be part of consensus and adding, removing or changing events is\n  // not a state-machine breaking change.\n  EmitProtoEventNonConsensus(ctx context.Context, event protoiface.MessageV1) error\n}\n```\n\nTyped events emitted with `EmitProto`  should be assumed to be part of blockchain consensus (whether they are part of\nthe block or app hash is left to the runtime to specify).\n\nEvents emitted by `EmitKVEvent` and `EmitProtoEventNonConsensus` are not considered to be part of consensus and cannot be observed\nby other modules. If there is a client-side need to add events in patch releases, these methods can be used.\n\n#### Logger\n\nA logger (`cosmossdk.io/log`) must be supplied using `depinject`, and will\nbe made available for modules to use via `depinject.In`.\nModules using it should follow the current pattern in the SDK by adding the module name before using it.\n\n```go\ntype ModuleInputs struct {\n  depinject.In\n\n  Logger log.Logger\n}\n\nfunc ProvideModule(in ModuleInputs) ModuleOutputs {\n  keeper := keeper.NewKeeper(\n    in.logger,\n  )\n}\n\nfunc NewKeeper(logger log.Logger) Keeper {\n  return Keeper{\n    logger: logger.With(log.ModuleKey, \"x/\"+types.ModuleName),\n  }\n}\n```\n\n### Core `AppModule` extension interfaces\n\n\nModules will provide their core services to the runtime module via extension interfaces built on top of the\n`cosmossdk.io/core/appmodule.AppModule` tag interface. This tag interface requires only two empty methods which\nallow `depinject` to identify implementers as `depinject.OnePerModule` types and as app module implementations:\n\n```go\ntype AppModule interface {\n  depinject.OnePerModuleType\n\n  // IsAppModule is a dummy method to tag a struct as implementing an AppModule.\n  IsAppModule()\n}\n```\n\nOther core extension interfaces will be defined in `cosmossdk.io/core` should be supported by valid runtime\nimplementations.\n\n#### `MsgServer` and `QueryServer` registration\n\n`MsgServer` and `QueryServer` registration is done by implementing the `HasServices` extension interface:\n\n```go\ntype HasServices interface {\n\tAppModule\n\n\tRegisterServices(grpc.ServiceRegistrar)\n}\n\n```\n\nBecause of the `cosmos.msg.v1.service` protobuf option, required for `Msg` services, the same `ServiceRegistrar` can be\nused to register both `Msg` and query services.\n\n#### Genesis\n\nThe genesis `Handler` functions - `DefaultGenesis`, `ValidateGenesis`, `InitGenesis` and `ExportGenesis` - are specified\nagainst the `GenesisSource` and `GenesisTarget` interfaces which will abstract over genesis sources which may be a single\nJSON object or collections of JSON objects that can be efficiently streamed.\n\n```go\n// GenesisSource is a source for genesis data in JSON format. It may abstract over a\n// single JSON object or separate files for each field in a JSON object that can\n// be streamed over. Modules should open a separate io.ReadCloser for each field that\n// is required. When fields represent arrays they can efficiently be streamed\n// over. If there is no data for a field, this function should return nil, nil. It is\n// important that the caller closes the reader when done with it.\ntype GenesisSource = func(field string) (io.ReadCloser, error)\n\n// GenesisTarget is a target for writing genesis data in JSON format. It may\n// abstract over a single JSON object or JSON in separate files that can be\n// streamed over. Modules should open a separate io.WriteCloser for each field\n// and should prefer writing fields as arrays when possible to support efficient\n// iteration. It is important the caller closers the writer AND checks the error\n// when done with it. It is expected that a stream of JSON data is written\n// to the writer.\ntype GenesisTarget = func(field string) (io.WriteCloser, error)\n```\n\nAll genesis objects for a given module are expected to conform to the semantics of a JSON object.\nEach field in the JSON object should be read and written separately to support streaming genesis.\nThe [ORM](./adr-055-orm.md) and [collections](./adr-062-collections-state-layer.md) both support\nstreaming genesis and modules using these frameworks generally do not need to write any manual\ngenesis code.\n\nTo support genesis, modules should implement the `HasGenesis` extension interface:\n\n```go\ntype HasGenesis interface {\n\tAppModule\n\n\t// DefaultGenesis writes the default genesis for this module to the target.\n\tDefaultGenesis(GenesisTarget) error\n\n\t// ValidateGenesis validates the genesis data read from the source.\n\tValidateGenesis(GenesisSource) error\n\n\t// InitGenesis initializes module state from the genesis source.\n\tInitGenesis(context.Context, GenesisSource) error\n\n\t// ExportGenesis exports module state to the genesis target.\n\tExportGenesis(context.Context, GenesisTarget) error\n}\n```\n\n#### Pre Blockers\n\nModules that have functionality that runs before BeginBlock and should implement the `HasPreBlocker` interfaces:\n\n```go\ntype HasPreBlocker interface {\n  AppModule\n  PreBlock(context.Context) error\n}\n```\n\n#### Begin and End Blockers\n\nModules that have functionality that runs before transactions (begin blockers) or after transactions\n(end blockers) should implement the has `HasBeginBlocker` and/or `HasEndBlocker` interfaces:\n\n```go\ntype HasBeginBlocker interface {\n  AppModule\n  BeginBlock(context.Context) error\n}\n\ntype HasEndBlocker interface {\n  AppModule\n  EndBlock(context.Context) error\n}\n```\n\nThe `BeginBlock` and `EndBlock` methods will take a `context.Context`, because:\n\n* most modules don't need Comet information other than `BlockInfo` so we can eliminate dependencies on specific\nComet versions\n* for the few modules that need Comet block headers and/or return validator updates, specific versions of the\nruntime module will provide specific functionality for interacting with the specific version(s) of Comet\nsupported\n\nIn order for `BeginBlock`, `EndBlock` and `InitGenesis` to send back validator updates and retrieve full Comet\nblock headers, the runtime module for a specific version of Comet could provide services like this:\n\n```go\ntype ValidatorUpdateService interface {\n    SetValidatorUpdates(context.Context, []abci.ValidatorUpdate)\n}\n```\n\nHeader Service defines a way to get header information about a block. This information is generalized for all implementations: \n\n```go \n\ntype Service interface {\n\tGetHeaderInfo(context.Context) Info\n}\n\ntype Info struct {\n\tHeight int64      // Height returns the height of the block\n\tHash []byte       // Hash returns the hash of the block header\n\tTime time.Time    // Time returns the time of the block\n\tChainID string    // ChainId returns the chain ID of the block\n}\n```\n\nComet Service provides a way to get comet specific information: \n\n```go\ntype Service interface {\n\tGetCometInfo(context.Context) Info\n}\n\ntype CometInfo struct {\n  Evidence []abci.Misbehavior // Misbehavior returns the misbehavior of the block\n\t// ValidatorsHash returns the hash of the validators\n\t// For Comet, it is the hash of the next validators\n\tValidatorsHash []byte\n\tProposerAddress []byte            // ProposerAddress returns the address of the block proposer\n\tDecidedLastCommit abci.CommitInfo // DecidedLastCommit returns the last commit info\n}\n```\n\nIf a user would like to provide a module other information they would need to implement another service like:\n\n```go\ntype RollKit Interface {\n  ...\n}\n```\n\nWe know these types will change at the Comet level and that also a very limited set of modules actually need this\nfunctionality, so they are intentionally kept out of core to keep core limited to the necessary, minimal set of stable\nAPIs.\n\n#### Remaining Parts of AppModule\n\nThe current `AppModule` framework handles a number of additional concerns which aren't addressed by this core API.\nThese include:\n\n* gas\n* block headers\n* upgrades\n* registration of gogo proto and amino interface types\n* cobra query and tx commands\n* gRPC gateway \n* crisis module invariants\n* simulations\n\nAdditional `AppModule` extension interfaces either inside or outside of core will need to be specified to handle\nthese concerns.\n\nIn the case of gogo proto and amino interfaces, the registration of these generally should happen as early\nas possible during initialization and in [ADR 057: App Wiring](./adr-057-app-wiring.md), protobuf type registration  \nhappens before dependency injection (although this could alternatively be done dedicated DI providers).\n\ngRPC gateway registration should probably be handled by the runtime module, but the core API shouldn't depend on gRPC\ngateway types as 1) we are already using an older version and 2) it's possible the framework can do this registration\nautomatically in the future. So for now, the runtime module should probably provide some sort of specific type for doing\nthis registration ex:\n\n```go\ntype GrpcGatewayInfo struct {\n    Handlers []GrpcGatewayHandler\n}\n\ntype GrpcGatewayHandler func(ctx context.Context, mux *runtime.ServeMux, client QueryClient) error\n```\n\nwhich modules can return in a provider:\n\n```go\nfunc ProvideGrpcGateway() GrpcGatewayInfo {\n    return GrpcGatewayInfo {\n        Handlers: []Handler {types.RegisterQueryHandlerClient}\n    }\n}\n```\n\nCrisis module invariants and simulations are subject to potential redesign and should be managed with types\ndefined in the crisis and simulation modules respectively.\n\nExtension interface for CLI commands will be provided via the `cosmossdk.io/client/v2` module and its\n[autocli](./adr-058-auto-generated-cli.md) framework.\n\n#### Example Usage\n\nHere is an example of setting up a hypothetical `foo` v2 module which uses the [ORM](./adr-055-orm.md) for its state\nmanagement and genesis.\n\n```go\n\ntype Keeper struct {\n\tdb orm.ModuleDB\n\tevtSrv event.Service\n}\n\nfunc (k Keeper) RegisterServices(r grpc.ServiceRegistrar) {\n  foov1.RegisterMsgServer(r, k)\n  foov1.RegisterQueryServer(r, k)\n}\n\nfunc (k Keeper) BeginBlock(context.Context) error {\n\treturn nil\n}\n\nfunc ProvideApp(config *foomodulev2.Module, evtSvc event.EventService, db orm.ModuleDB) (Keeper, appmodule.AppModule){\n    k := &Keeper{db: db, evtSvc: evtSvc}\n    return k, k\n}\n```\n\n### Runtime Compatibility Version\n\nThe `core` module will define a static integer var, `cosmossdk.io/core.RuntimeCompatibilityVersion`, which is\na minor version indicator of the core module that is accessible at runtime. Correct runtime module implementations\nshould check this compatibility version and return an error if the current `RuntimeCompatibilityVersion` is higher\nthan the version of the core API that this runtime version can support. When new features are adding to the `core`\nmodule API that runtime modules are required to support, this version should be incremented.\n\n### Runtime Modules\n\nThe initial `runtime` module will simply be created within the existing `github.com/cosmos/cosmos-sdk` go module\nunder the `runtime` package. This module will be a small wrapper around the existing `BaseApp`, `sdk.Context` and\nmodule manager and follow the Cosmos SDK's existing [0-based versioning](https://0ver.org). To move to semantic\nversioning as well as runtime modularity, new officially supported runtime modules will be created under the\n`cosmossdk.io/runtime` prefix. For each supported consensus engine a semantically-versioned go module should be created\nwith a runtime implementation for that consensus engine. For example:\n\n* `cosmossdk.io/runtime/comet`\n* `cosmossdk.io/runtime/comet/v2`\n* `cosmossdk.io/runtime/rollkit`\n* etc.\n\nThese runtime modules should attempt to be semantically versioned even if the underlying consensus engine is not. Also,\nbecause a runtime module is also a first class Cosmos SDK module, it should have a protobuf module config type.\nA new semantically versioned module config type should be created for each of these runtime module such that there is a\n1:1 correspondence between the go module and module config type. This is the same practice should be followed for every \nsemantically versioned Cosmos SDK module as described in [ADR 057: App Wiring](./adr-057-app-wiring.md).\n\nCurrently, `github.com/cosmos/cosmos-sdk/runtime` uses the protobuf config type `cosmos.app.runtime.v1alpha1.Module`.\nWhen we have a standalone v1 comet runtime, we should use a dedicated protobuf module config type such as\n`cosmos.runtime.comet.v1.Module1`. When we release v2 of the comet runtime (`cosmossdk.io/runtime/comet/v2`) we should\nhave a corresponding `cosmos.runtime.comet.v2.Module` protobuf type.\n\nIn order to make it easier to support different consensus engines that support the same core module functionality as\ndescribed in this ADR, a common go module should be created with shared runtime components. The easiest runtime components\nto share initially are probably the message/query router, inter-module client, service register, and event router.\nThis common runtime module should be created initially as the `cosmossdk.io/runtime/common` go module.\n\nWhen this new architecture has been implemented, the main dependency for a Cosmos SDK module would be\n`cosmossdk.io/core` and that module should be able to be used with any supported consensus engine (to the extent\nthat it does not explicitly depend on consensus engine specific functionality such as Comet's block headers). An\napp developer would then be able to choose which consensus engine they want to use by importing the corresponding\nruntime module. The current `BaseApp` would be refactored into the `cosmossdk.io/runtime/comet` module, the router\ninfrastructure in `baseapp/` would be refactored into `cosmossdk.io/runtime/common` and support ADR 033, and eventually\na dependency on `github.com/cosmos/cosmos-sdk` would no longer be required.\n\nIn short, modules would depend primarily on `cosmossdk.io/core`, and each `cosmossdk.io/runtime/{consensus-engine}`\nwould implement the `cosmossdk.io/core` functionality for that consensus engine.\n\nOne additional piece that would need to be resolved as part of this architecture is how runtimes relate to the server.\nLikely it would make sense to modularize the current server architecture so that it can be used with any runtime even\nif that is based on a consensus engine besides Comet. This means that eventually the Comet runtime would need to\nencapsulate the logic for starting Comet and the ABCI app.\n\n### Testing\n\nA mock implementation of all services should be provided in core to allow for unit testing of modules\nwithout needing to depend on any particular version of runtime. Mock services should\nallow tests to observe service behavior or provide a non-production implementation - for instance memory\nstores can be used to mock stores.\n\nFor integration testing, a mock runtime implementation should be provided that allows composing different app modules\ntogether for testing without a dependency on runtime or Comet.\n\n## Consequences\n\n### Backwards Compatibility\n\nEarly versions of runtime modules should aim to support as much as possible modules built with the existing\n`AppModule`/`sdk.Context` framework. As the core API is more widely adopted, later runtime versions may choose to\ndrop support and only support the core API plus any runtime module specific APIs (like specific versions of Comet).\n\nThe core module itself should strive to remain at the go semantic version `v1` as long as possible and follow design\nprinciples that allow for strong long-term support (LTS).\n\nOlder versions of the SDK can support modules built against core with adaptors that convert wrap core `AppModule`\nimplementations in implementations of `AppModule` that conform to that version of the SDK's semantics as well\nas by providing service implementations by wrapping `sdk.Context`.\n\n### Positive\n\n* better API encapsulation and separation of concerns\n* more stable APIs\n* more framework extensibility\n* deterministic events and queries\n* event listeners\n* inter-module msg and query execution support\n* more explicit support for forking and merging of module versions (including runtime)\n\n### Negative\n\n### Neutral\n\n* modules will need to be refactored to use this API\n* some replacements for `AppModule` functionality still need to be defined in follow-ups\n  (type registration, commands, invariants, simulations) and this will take additional design work\n\n## Further Discussions\n\n* gas\n* block headers\n* upgrades\n* registration of gogo proto and amino interface types\n* cobra query and tx commands\n* gRPC gateway\n* crisis module invariants\n* simulations\n\n## References\n\n* [ADR 033: Protobuf-based Inter-Module Communication](./adr-033-protobuf-inter-module-comm.md)\n* [ADR 057: App Wiring](./adr-057-app-wiring.md)\n* [ADR 055: ORM](./adr-055-orm.md)\n* [ADR 028: Public Key Addresses](./adr-028-public-key-addresses.md)\n* [Keeping Your Modules Compatible](https://go.dev/blog/module-compatibility)"
  },
  {
    "number": 64,
    "filename": "adr-064-abci-2.0.md",
    "title": "ADR 64: ABCI 2.0 Integration (Phase II)",
    "content": "# ADR 64: ABCI 2.0 Integration (Phase II)\n\n## Changelog\n\n* 2023-01-17: Initial Draft (@alexanderbez)\n* 2023-04-06: Add upgrading section (@alexanderbez)\n* 2023-04-10: Simplify vote extension state persistence (@alexanderbez)\n* 2023-07-07: Revise vote extension state persistence (@alexanderbez)\n* 2023-08-24: Revise vote extension power calculations and staking interface (@davidterpay)\n\n## Status\n\nACCEPTED\n\n## Abstract\n\nThis ADR outlines the continuation of the efforts to implement ABCI++ in the Cosmos\nSDK outlined in [ADR 060: ABCI 1.0 (Phase I)](adr-060-abci-1.0.md).\n\nSpecifically, this ADR outlines the design and implementation of ABCI 2.0, which\nincludes `ExtendVote`, `VerifyVoteExtension` and `FinalizeBlock`.\n\n## Context\n\nABCI 2.0 continues the promised updates from ABCI++, specifically three additional\nABCI methods that the application can implement in order to gain further control,\ninsight and customization of the consensus process, unlocking many novel use-cases\nthat were previously not possible. We describe these three new methods below:\n\n### `ExtendVote`\n\nThis method allows each validator process to extend the pre-commit phase of the\nCometBFT consensus process. Specifically, it allows the application to perform\ncustom business logic that extends the pre-commit vote and supply additional data\nas part of the vote, although they are signed separately by the same key.\n\nThe data, called vote extension, will be broadcast and received together with the\nvote it is extending, and will be made available to the application in the next\nheight. Specifically, the proposer of the next block will receive the vote extensions\nin `RequestPrepareProposal.local_last_commit.votes`.\n\nIf the application does not have vote extension information to provide, it\nreturns a 0-length byte array as its vote extension.\n\n**NOTE**: \n\n* Although each validator process submits its own vote extension, ONLY the *proposer*\n  of the *next* block will receive all the vote extensions included as part of the\n  pre-commit phase of the previous block. This means only the proposer will\n  implicitly have access to all the vote extensions, via `RequestPrepareProposal`,\n  and that not all vote extensions may be included, since a validator does not\n  have to wait for all pre-commits, only 2/3.\n* The pre-commit vote is signed independently from the vote extension.\n\n### `VerifyVoteExtension`\n\nThis method allows validators to validate the vote extension data attached to\neach pre-commit message it receives. If the validation fails, the whole pre-commit\nmessage will be deemed invalid and ignored by CometBFT.\n\nCometBFT uses `VerifyVoteExtension` when validating a pre-commit vote. Specifically,\nfor a pre-commit, CometBFT will:\n\n* Reject the message if it doesn't contain a signed vote AND a signed vote extension\n* Reject the message if the vote's signature OR the vote extension's signature fails to verify\n* Reject the message if `VerifyVoteExtension` was rejected by the app\n\nOtherwise, CometBFT will accept the pre-commit message.\n\nNote, this has important consequences on liveness, i.e., if vote extensions repeatedly\ncannot be verified by correct validators, CometBFT may not be able to finalize\na block even if sufficiently many (+2/3) validators send pre-commit votes for\nthat block. Thus, `VerifyVoteExtension` should be used with special care.\n\nCometBFT recommends that an application that detects an invalid vote extension\nSHOULD accept it in `ResponseVerifyVoteExtension` and ignore it in its own logic.\n\n### `FinalizeBlock`\n\nThis method delivers a decided block to the application. The application must\nexecute the transactions in the block deterministically and update its state\naccordingly. Cryptographic commitments to the block and transaction results,\nreturned via the corresponding parameters in `ResponseFinalizeBlock`, are\nincluded in the header of the next block. CometBFT calls it when a new block\nis decided.\n\nIn other words, `FinalizeBlock` encapsulates the current ABCI execution flow of\n`BeginBlock`, one or more `DeliverTx`, and `EndBlock` into a single ABCI method.\nCometBFT will no longer execute requests for these legacy methods and instead\nwill just simply call `FinalizeBlock`.\n\n## Decision\n\nWe will discuss changes to the Cosmos SDK to implement ABCI 2.0 in two distinct\nphases, `VoteExtensions` and `FinalizeBlock`.\n\n### `VoteExtensions`\n\nSimilarly for `PrepareProposal` and `ProcessProposal`, we propose to introduce\ntwo new handlers that an application can implement in order to provide and verify\nvote extensions.\n\nWe propose the following new handlers for applications to implement:\n\n```go\ntype ExtendVoteHandler func(sdk.Context, abci.RequestExtendVote) abci.ResponseExtendVote\ntype VerifyVoteExtensionHandler func(sdk.Context, abci.RequestVerifyVoteExtension) abci.ResponseVerifyVoteExtension\n```\n\nAn ephemeral context and state will be supplied to both handlers. The\ncontext will contain relevant metadata such as the block height and block hash.\nThe state will be a cached version of the committed state of the application and\nwill be discarded after the execution of the handler, this means that both handlers\nget a fresh state view and no changes made to it will be written.\n\nIf an application decides to implement `ExtendVoteHandler`, it must return a\nnon-nil `ResponseExtendVote.VoteExtension`.\n\nRecall, an implementation of `ExtendVoteHandler` does NOT need to be deterministic,\nhowever, given a set of vote extensions, `VerifyVoteExtensionHandler` must be\ndeterministic, otherwise the chain may suffer from liveness faults. In addition,\nrecall CometBFT proceeds in rounds for each height, so if a decision cannot be\nmade about a block proposal at a given height, CometBFT will proceed to the\nnext round and thus will execute `ExtendVote` and `VerifyVoteExtension` again for\nthe new round for each validator until 2/3 valid pre-commits can be obtained.\n\nGiven the broad scope of potential implementations and use-cases of vote extensions,\nand how to verify them, most applications should choose to implement the handlers\nthrough a single handler type, which can have any number of dependencies injected\nsuch as keepers. In addition, this handler type could contain some notion of\nvolatile vote extension state management which would assist in vote extension\nverification. This state management could be ephemeral or could be some form of\non-disk persistence.\n\nExample:\n\n```go\n// VoteExtensionHandler implements an Oracle vote extension handler.\ntype VoteExtensionHandler struct {\n\tcdc   Codec\n\tmk    MyKeeper\n\tstate VoteExtState // This could be a map or a DB connection object\n}\n\n// ExtendVoteHandler can do something with h.mk and possibly h.state to create\n// a vote extension, such as fetching a series of prices for supported assets.\nfunc (h VoteExtensionHandler) ExtendVoteHandler(ctx sdk.Context, req abci.RequestExtendVote) abci.ResponseExtendVote {\n\tprices := GetPrices(ctx, h.mk.Assets())\n\tbz, err := EncodePrices(h.cdc, prices)\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"failed to encode prices for vote extension: %w\", err))\n\t}\n\n\t// store our vote extension at the given height\n\t//\n\t// NOTE: Vote extensions can be overridden since we can timeout in a round.\n\tSetPrices(h.state, req, bz)\n\n\treturn abci.ResponseExtendVote{VoteExtension: bz}\n}\n\n// VerifyVoteExtensionHandler can do something with h.state and req to verify\n// the req.VoteExtension field, such as ensuring the provided oracle prices are\n// within some valid range of our prices.\nfunc (h VoteExtensionHandler) VerifyVoteExtensionHandler(ctx sdk.Context, req abci.RequestVerifyVoteExtension) abci.ResponseVerifyVoteExtension {\n\tprices, err := DecodePrices(h.cdc, req.VoteExtension)\n\tif err != nil {\n\t\tlog(\"failed to decode vote extension\", \"err\", err)\n\t\treturn abci.ResponseVerifyVoteExtension{Status: REJECT}\n\t}\n\n\tif err := ValidatePrices(h.state, req, prices); err != nil {\n\t\tlog(\"failed to validate vote extension\", \"prices\", prices, \"err\", err)\n\t\treturn abci.ResponseVerifyVoteExtension{Status: REJECT}\n\t}\n\n\t// store updated vote extensions at the given height\n\t//\n\t// NOTE: Vote extensions can be overridden since we can timeout in a round.\n\tSetPrices(h.state, req, req.VoteExtension)\n\n\treturn abci.ResponseVerifyVoteExtension{Status: ACCEPT}\n}\n```\n\n#### Vote Extension Propagation & Verification\n\nAs mentioned previously, vote extensions for height `H` are only made available\nto the proposer at height `H+1` during `PrepareProposal`. However, in order to\nmake vote extensions useful, all validators should have access to the agreed upon\nvote extensions at height `H` during `H+1`.\n\nSince CometBFT includes all the vote extension signatures in `RequestPrepareProposal`,\nwe propose that the proposing validator manually \"inject\" the vote extensions\nalong with their respective signatures via a special transaction, `VoteExtsTx`,\ninto the block proposal during `PrepareProposal`. The `VoteExtsTx` will be\npopulated with a single `ExtendedCommitInfo` object which is received directly\nfrom `RequestPrepareProposal`.\n\nFor convention, the `VoteExtsTx` transaction should be the first transaction in\nthe block proposal, although chains can implement their own preferences. For\nsafety purposes, we also propose that the proposer itself verify all the vote\nextension signatures it receives in `RequestPrepareProposal`.\n\nA validator, upon a `RequestProcessProposal`, will receive the injected `VoteExtsTx`\nwhich includes the vote extensions along with their signatures. If no such transaction\nexists, the validator MUST REJECT the proposal.\n\nWhen a validator inspects a `VoteExtsTx`, it will evaluate each `SignedVoteExtension`.\nFor each signed vote extension, the validator will generate the signed bytes and\nverify the signature. At least 2/3 valid signatures, based on voting power, must\nbe received in order for the block proposal to be valid, otherwise the validator\nMUST REJECT the proposal.\n\nIn order to have the ability to validate signatures, `BaseApp` must have access\nto the `x/staking` module, since this module stores an index from consensus\naddress to public key. However, we will avoid a direct dependency on `x/staking`\nand instead rely on an interface instead. In addition, the Cosmos SDK will expose\na default signature verification method which applications can use:\n\n```go\ntype ValidatorStore interface {\n\tGetPubKeyByConsAddr(context.Context, sdk.ConsAddress) (cmtprotocrypto.PublicKey, error)\n}\n\n// ValidateVoteExtensions is a function that an application can execute in\n// ProcessProposal to verify vote extension signatures.\nfunc (app *BaseApp) ValidateVoteExtensions(ctx sdk.Context, currentHeight int64, extCommit abci.ExtendedCommitInfo) error {\n\tvotingPower := 0\n\ttotalVotingPower := 0\n\n\tfor _, vote := range extCommit.Votes {\n\t\ttotalVotingPower += vote.Validator.Power\n\n\t\tif !vote.SignedLastBlock || len(vote.VoteExtension) == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tvalConsAddr := sdk.ConsAddress(vote.Validator.Address)\n\t\tpubKeyProto, err := valStore.GetPubKeyByConsAddr(ctx, valConsAddr)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to get public key for validator %s: %w\", valConsAddr, err)\n\t\t}\n\n\t\tif len(vote.ExtensionSignature) == 0 {\n\t\t\treturn fmt.Errorf(\"received a non-empty vote extension with empty signature for validator %s\", valConsAddr)\n\t\t}\n\n\t\tcmtPubKey, err := cryptoenc.PubKeyFromProto(pubKeyProto)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to convert validator %X public key: %w\", valConsAddr, err)\n\t\t}\n\n\t\tcve := cmtproto.CanonicalVoteExtension{\n\t\t\tExtension: vote.VoteExtension,\n\t\t\tHeight:    currentHeight - 1, // the vote extension was signed in the previous height\n\t\t\tRound:     int64(extCommit.Round),\n\t\t\tChainId:   app.GetChainID(),\n\t\t}\n\n\t\textSignBytes, err := cosmosio.MarshalDelimited(&cve)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to encode CanonicalVoteExtension: %w\", err)\n\t\t}\n\n\t\tif !cmtPubKey.VerifySignature(extSignBytes, vote.ExtensionSignature) {\n\t\t\treturn errors.New(\"received vote with invalid signature\")\n\t\t}\n\n\t\tvotingPower += vote.Validator.Power\n\t}\n\n\tif (votingPower / totalVotingPower) < threshold {\n\t\treturn errors.New(\"not enough voting power for the vote extensions\")\n\t}\n\n\treturn nil\n}\n```\n\nOnce at least 2/3 signatures, by voting power, are received and verified, the\nvalidator can use the vote extensions to derive additional data or come to some\ndecision based on the vote extensions.\n\n> NOTE: It is very important to state, that neither the vote propagation technique\n> nor the vote extension verification mechanism described above is required for\n> applications to implement. In other words, a proposer is not required to verify\n> and propagate vote extensions along with their signatures nor are proposers\n> required to verify those signatures. An application can implement it's own\n> PKI mechanism and use that to sign and verify vote extensions.\n\n#### Vote Extension Persistence\n\nIn certain contexts, it may be useful or necessary for applications to persist\ndata derived from vote extensions. In order to facilitate this use case, we propose\nto allow app developers to define a pre-Blocker hook which will be called\nat the very beginning of `FinalizeBlock`, i.e. before `BeginBlock` (see below).\n\nNote, we cannot allow applications to directly write to the application state\nduring `ProcessProposal` because during replay, CometBFT will NOT call `ProcessProposal`,\nwhich would result in an incomplete state view.\n\n```go\nfunc (a MyApp) PreBlocker(ctx sdk.Context, req *abci.RequestFinalizeBlock) error {\n\tvoteExts := GetVoteExtensions(ctx, req.Txs)\n\t\n\t// Process and perform some compute on vote extensions, storing any resulting\n\t// state.\n\tif err a.processVoteExtensions(ctx, voteExts); if err != nil {\n\t\treturn err\n\t}\n}\n```\n\n### `FinalizeBlock`\n\nThe existing ABCI methods `BeginBlock`, `DeliverTx`, and `EndBlock` have existed\nsince the dawn of ABCI-based applications. Thus, applications, tooling, and developers\nhave grown used to these methods and their use-cases. Specifically, `BeginBlock`\nand `EndBlock` have grown to be pretty integral and powerful within ABCI-based\napplications. E.g. an application might want to run distribution and inflation\nrelated operations prior to executing transactions and then have staking related\nchanges to happen after executing all transactions.\n\nWe propose to keep `BeginBlock` and `EndBlock` within the SDK's core module\ninterfaces only so application developers can continue to build against existing\nexecution flows. However, we will remove `BeginBlock`, `DeliverTx` and `EndBlock`\nfrom the SDK's `BaseApp` implementation and thus the ABCI surface area.\n\nWhat will then exist is a single `FinalizeBlock` execution flow. Specifically, in\n`FinalizeBlock` we will execute the application's `BeginBlock`, followed by\nexecution of all the transactions, finally followed by execution of the application's\n`EndBlock`.\n\nNote, we will still keep the existing transaction execution mechanics within\n`BaseApp`, but all notions of `DeliverTx` will be removed, i.e. `deliverState`\nwill be replace with `finalizeState`, which will be committed on `Commit`.\n\nHowever, there are current parameters and fields that exist in the existing\n`BeginBlock` and `EndBlock` ABCI types, such as votes that are used in distribution\nand byzantine validators used in evidence handling. These parameters exist in the\n`FinalizeBlock` request type, and will need to be passed to the application's\nimplementations of `BeginBlock` and `EndBlock`.\n\nThis means the Cosmos SDK's core module interfaces will need to be updated to\nreflect these parameters. The easiest and most straightforward way to achieve\nthis is to just pass `RequestFinalizeBlock` to `BeginBlock` and `EndBlock`.\nAlternatively, we can create dedicated proxy types in the SDK that reflect these\nlegacy ABCI types, e.g. `LegacyBeginBlockRequest` and `LegacyEndBlockRequest`. Or,\nwe can come up with new types and names altogether.\n\n```go\nfunc (app *BaseApp) FinalizeBlock(req abci.RequestFinalizeBlock) (*abci.ResponseFinalizeBlock, error) {\n\tctx := ...\n\n\tif app.preBlocker != nil {\n\t\tctx := app.finalizeBlockState.ctx\n\t\trsp, err := app.preBlocker(ctx, req)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif rsp.ConsensusParamsChanged {\n\t\t\tapp.finalizeBlockState.ctx = ctx.WithConsensusParams(app.GetConsensusParams(ctx))\n\t\t}\n\t}\n\tbeginBlockResp, err := app.beginBlock(req)\n\tappendBlockEventAttr(beginBlockResp.Events, \"begin_block\")\n\n\ttxExecResults := make([]abci.ExecTxResult, 0, len(req.Txs))\n\tfor _, tx := range req.Txs {\n\t\tresult := app.runTx(runTxModeFinalize, tx)\n\t\ttxExecResults = append(txExecResults, result)\n\t}\n\n\tendBlockResp, err := app.endBlock(app.finalizeBlockState.ctx)\n\tappendBlockEventAttr(beginBlockResp.Events, \"end_block\")\n\n\treturn abci.ResponseFinalizeBlock{\n\t\tTxResults:             txExecResults,\n\t\tEvents:                joinEvents(beginBlockResp.Events, endBlockResp.Events),\n\t\tValidatorUpdates:      endBlockResp.ValidatorUpdates,\n\t\tConsensusParamUpdates: endBlockResp.ConsensusParamUpdates,\n\t\tAppHash:               nil,\n\t}\n}\n```\n\n#### Events\n\nMany tools, indexers and ecosystem libraries rely on the existence `BeginBlock`\nand `EndBlock` events. Since CometBFT now only exposes `FinalizeBlockEvents`, we\nfind that it will still be useful for these clients and tools to still query for\nand rely on existing events, especially since applications will still define\n`BeginBlock` and `EndBlock` implementations.\n\nIn order to facilitate existing event functionality, we propose that all `BeginBlock`\nand `EndBlock` events have a dedicated `EventAttribute` with `key=block` and\n`value=begin_block|end_block`. The `EventAttribute` will be appended to each event\nin both `BeginBlock` and `EndBlock` events. \n\n\n### Upgrading\n\nCometBFT defines a consensus parameter, [`VoteExtensionsEnableHeight`](https://github.com/cometbft/cometbft/blob/v0.38.0-alpha.1/spec/abci/abci%2B%2B_app_requirements.md#abciparamsvoteextensionsenableheight),\nwhich specifies the height at which vote extensions are enabled and **required**.\nIf the value is set to zero, which is the default, then vote extensions are\ndisabled and an application is not required to implement and use vote extensions.\n\nHowever, if the value `H` is positive, at all heights greater than the configured\nheight `H` vote extensions must be present (even if empty). When the configured\nheight `H` is reached, `PrepareProposal` will not include vote extensions yet,\nbut `ExtendVote` and `VerifyVoteExtension` will be called. Then, when reaching\nheight `H+1`, `PrepareProposal` will include the vote extensions from height `H`.\n\nIt is very important to note, for all heights after H:\n\n* Vote extensions CANNOT be disabled\n* They are mandatory, i.e. all pre-commit messages sent MUST have an extension\n  attached (even if empty)\n\nWhen an application updates to the Cosmos SDK version with CometBFT v0.38 support,\nin the upgrade handler it must ensure to set the consensus parameter\n`VoteExtensionsEnableHeight` to the correct value. E.g. if an application is set\nto perform an upgrade at height `H`, then the value of `VoteExtensionsEnableHeight`\nshould be set to any value `>=H+1`. This means that at the upgrade height, `H`,\nvote extensions will not be enabled yet, but at height `H+1` they will be enabled.\n\n## Consequences\n\n### Backwards Compatibility\n\nABCI 2.0 is naturally not backwards compatible with prior versions of the Cosmos SDK\nand CometBFT. For example, an application that requests `RequestFinalizeBlock`\nto the same application that does not speak ABCI 2.0 will naturally fail.\n\nIn addition, `BeginBlock`, `DeliverTx` and `EndBlock` will be removed from the\napplication ABCI interfaces and along with the inputs and outputs being modified\nin the module interfaces.\n\n### Positive\n\n* `BeginBlock` and `EndBlock` semantics remain, so burden on application developers\n  should be limited.\n* Less communication overhead as multiple ABCI requests are condensed into a single\n  request.\n* Sets the groundwork for optimistic execution.\n* Vote extensions allow for an entirely new set of application primitives to be\n  developed, such as in-process price oracles and encrypted mempools.\n\n### Negative\n\n* Some existing Cosmos SDK core APIs may need to be modified and thus broken.\n* Signature verification in `ProcessProposal` of 100+ vote extension signatures\n  will add significant performance overhead to `ProcessProposal`. Granted, the\n\tsignature verification process can happen concurrently using an error group\n\twith `GOMAXPROCS` goroutines.\n\n### Neutral\n\n* Having to manually \"inject\" vote extensions into the block proposal during\n  `PrepareProposal` is an awkward approach and takes up block space unnecessarily.\n* The requirement of `ResetProcessProposalState` can create a footgun for\n  application developers if they're not careful, but this is necessary in order\n\tfor applications to be able to commit state from vote extension computation.\n\n## Further Discussions\n\nFuture discussions include design and implementation of ABCI 3.0, which is a\ncontinuation of ABCI++ and the general discussion of optimistic execution.\n\n## References\n\n* [ADR 060: ABCI 1.0 (Phase I)](adr-060-abci-1.0.md)"
  },
  {
    "number": 65,
    "filename": "adr-065-store-v2.md",
    "title": "# ADR 065: Store V2",
    "content": "# ADR-065: Store V2\n\n## Changelog\n\n* Feb 14, 2023: Initial Draft (@alexanderbez)\n\n## Status\n\nDRAFT\n\n## Abstract\n\nThe storage and state primitives that Cosmos SDK based applications have used have\nby and large not changed since the launch of the inaugural Cosmos Hub. The demands\nand needs of Cosmos SDK based applications, from both developer and client UX\nperspectives, have evolved and outgrown the ecosystem since these primitives\nwere first introduced.\n\nOver time as these applications have gained significant adoption, many critical\nshortcomings and flaws have been exposed in the state and storage primitives of\nthe Cosmos SDK.\n\nIn order to keep up with the evolving demands and needs of both clients and developers,\na major overhaul to these primitives is necessary.\n\n## Context\n\nThe Cosmos SDK provides application developers with various storage primitives\nfor dealing with application state. Specifically, each module contains its own\nmerkle commitment data structure -- an IAVL tree. In this data structure, a module\ncan store and retrieve key-value pairs along with Merkle commitments, i.e. proofs,\nto those key-value pairs indicating that they do or do not exist in the global\napplication state. This data structure is the base layer `KVStore`.\n\nIn addition, the SDK provides abstractions on top of this Merkle data structure.\nNamely, a root multi-store (RMS) is a collection of each module's `KVStore`.\nThrough the RMS, the application can serve queries and provide proofs to clients\nin addition to providing a module access to its own unique `KVStore` through the use\nof `StoreKey`, which is an OCAP primitive.\n\nThere are further layers of abstraction that sit between the RMS and the underlying\nIAVL `KVStore`. A `GasKVStore` is responsible for tracking gas IO consumption for\nstate machine reads and writes. A `CacheKVStore` is responsible for providing a\nway to cache reads and buffer writes to make state transitions atomic, e.g.\ntransaction execution or governance proposal execution.\n\nThere are a few critical drawbacks to these layers of abstraction and the overall\ndesign of storage in the Cosmos SDK:\n\n* Since each module has its own IAVL `KVStore`, commitments are not [atomic](https://github.com/cosmos/cosmos-sdk/issues/14625)\n    * Note, we can still allow modules to have their own IAVL `KVStore`, but the\n      IAVL library will need to support the ability to pass a DB instance as an\n      argument to various IAVL APIs.\n* Since IAVL is responsible for both state storage and commitment, running an \n  archive node becomes increasingly expensive as disk space grows exponentially.\n* As the size of a network increases, various performance bottlenecks start to\n  emerge in many areas such as query performance, network upgrades, state\n  migrations, and general application performance.\n* Developer UX is poor as it does not allow application developers to experiment\n  with different types of approaches to storage and commitments, along with the\n  complications of many layers of abstractions referenced above.\n\nSee the [Storage Discussion](https://github.com/cosmos/cosmos-sdk/discussions/13545) for more information.\n\n## Alternatives\n\nThere was a previous attempt to refactor the storage layer described in [ADR-040](./adr-040-storage-and-smt-state-commitments.md).\nHowever, this approach mainly stems from the shortcomings of IAVL and various performance\nissues around it. While there was a (partial) implementation of [ADR-040](./adr-040-storage-and-smt-state-commitments.md),\nit was never adopted for a variety of reasons, such as the reliance on using an\nSMT, which was more in a research phase, and some design choices that couldn't\nbe fully agreed upon, such as the snapshotting mechanism that would result in\nmassive state bloat.\n\n## Decision\n\nWe propose to build upon some of the great ideas introduced in [ADR-040](./adr-040-storage-and-smt-state-commitments.md),\nwhile being a bit more flexible with the underlying implementations and overall\nless intrusive. Specifically, we propose to:\n\n* Separate the concerns of state commitment (**SC**), needed for consensus, and\n  state storage (**SS**), needed for state machine and clients.\n* Reduce layers of abstractions necessary between the RMS and underlying stores.\n* Provide atomic module store commitments by providing a batch database object\n  to core IAVL APIs.\n* Reduce complexities in the `CacheKVStore` implementation while also improving\n  performance<sup>[3]</sup>.\n\nFurthermore, we will keep the IAVL is the backing [commitment](https://cryptography.fandom.com/wiki/Commitment_scheme)\nstore for the time being. While we might not fully settle on the use of IAVL in\nthe long term, we do not have strong empirical evidence to suggest a better\nalternative. Given that the SDK provides interfaces for stores, it should be sufficient\nto change the backing commitment store in the future should evidence arise to\nwarrant a better alternative. However there is promising work being done to IAVL\nthat should result in significant performance improvement <sup>[1,2]</sup>.\n\n### Separating SS and SC\n\nBy separating SS and SC, it will allow for us to optimize against primary use cases\nand access patterns to state. Specifically, The SS layer will be responsible for\ndirect access to data in the form of (key, value) pairs, whereas the SC layer (IAVL)\nwill be responsible for committing to data and providing Merkle proofs.\n\nNote, the underlying physical storage database will be the same between both the\nSS and SC layers. So to avoid collisions between (key, value) pairs, both layers\nwill be namespaced.\n\n#### State Commitment (SC)\n\nGiven that the existing solution today acts as both SS and SC, we can simply\nrepurpose it to act solely as the SC layer without any significant changes to\naccess patterns or behavior. In other words, the entire collection of existing\nIAVL-backed module `KVStore`s will act as the SC layer.\n\nHowever, in order for the SC layer to remain lightweight and not duplicate a\nmajority of the data held in the SS layer, we encourage node operators to keep\ntight pruning strategies.\n\n#### State Storage (SS)\n\nIn the RMS, we will expose a *single* `KVStore` backed by the same physical\ndatabase that backs the SC layer. This `KVStore` will be explicitly namespaced\nto avoid collisions and will act as the primary storage for (key, value) pairs.\n\nWhile we most likely will continue the use of `cosmos-db`, or some local interface,\nto allow for flexibility and iteration over preferred physical storage backends\nas research and benchmarking continues. However, we propose to hardcode the use\nof RocksDB as the primary physical storage backend.\n\nSince the SS layer will be implemented as a `KVStore`, it will support the\nfollowing functionality:\n\n* Range queries\n* CRUD operations\n* Historical queries and versioning\n* Pruning\n\nThe RMS will keep track of all buffered writes using a dedicated and internal\n`MemoryListener` for each `StoreKey`. For each block height, upon `Commit`, the\nSS layer will write all buffered (key, value) pairs under a [RocksDB user-defined timestamp](https://github.com/facebook/rocksdb/wiki/User-defined-Timestamp-%28Experimental%29) column\nfamily using the block height as the timestamp, which is an unsigned integer.\nThis will allow a client to fetch (key, value) pairs at historical and current\nheights along with making iteration and range queries relatively performant as\nthe timestamp is the key suffix.\n\nNote, we choose not to use a more general approach of allowing any embedded key/value\ndatabase, such as LevelDB or PebbleDB, using height key-prefixed keys to\neffectively version state because most of these databases use variable length\nkeys which would effectively make actions likes iteration and range queries less\nperformant.\n\nSince operators might want pruning strategies to differ in SS compared to SC,\ne.g. having a very tight pruning strategy in SC while having a looser pruning\nstrategy for SS, we propose to introduce an additional pruning configuration,\nwith parameters that are identical to what exists in the SDK today, and allow\noperators to control the pruning strategy of the SS layer independently of the\nSC layer.\n\nNote, the SC pruning strategy must be congruent with the operator's state sync\nconfiguration. This is so as to allow state sync snapshots to execute successfully,\notherwise, a snapshot could be triggered on a height that is not available in SC.\n\n#### State Sync\n\nThe state sync process should be largely unaffected by the separation of the SC\nand SS layers. However, if a node syncs via state sync, the SS layer of the node\nwill not have the state synced height available, since the IAVL import process is\nnot setup in way to easily allow direct key/value insertion. A modification of\nthe IAVL import process would be necessary to facilitate having the state sync\nheight available.\n\nNote, this is not problematic for the state machine itself because when a query\nis made, the RMS will automatically direct the query correctly (see [Queries](#queries)).\n\n#### Queries\n\nTo consolidate the query routing between both the SC and SS layers, we propose to\nhave a notion of a \"query router\" that is constructed in the RMS. This query router\nwill be supplied to each `KVStore` implementation. The query router will route\nqueries to either the SC layer or the SS layer based on a few parameters. If\n`prove: true`, then the query must be routed to the SC layer. Otherwise, if the\nquery height is available in the SS layer, the query will be served from the SS\nlayer. Otherwise, we fall back on the SC layer.\n\nIf no height is provided, the SS layer will assume the latest height. The SS\nlayer will store a reverse index to lookup `LatestVersion -> timestamp(version)`\nwhich is set on `Commit`.\n\n#### Proofs\n\nSince the SS layer is naturally a storage layer only, without any commitments\nto (key, value) pairs, it cannot provide Merkle proofs to clients during queries.\n\nSince the pruning strategy against the SC layer is configured by the operator,\nwe can therefore have the RMS route the query to the SC layer if the version exists and\n`prove: true`. Otherwise, the query will fall back to the SS layer without a proof.\n\nWe could explore the idea of using state snapshots to rebuild an in-memory IAVL\ntree in real time against a version closest to the one provided in the query.\nHowever, it is not clear what the performance implications will be of this approach.\n\n### Atomic Commitment\n\nWe propose to modify the existing IAVL APIs to accept a batch DB object instead\nof relying on an internal batch object in `nodeDB`. Since each underlying IAVL\n`KVStore` shares the same DB in the SC layer, this will allow commits to be\natomic.\n\nSpecifically, we propose to:\n\n* Remove the `dbm.Batch` field from `nodeDB`\n* Update the `SaveVersion` method of the `MutableTree` IAVL type to accept a batch object\n* Update the `Commit` method of the `CommitKVStore` interface to accept a batch object\n* Create a batch object in the RMS during `Commit` and pass this object to each\n  `KVStore`\n* Write the database batch after all stores have committed successfully\n\nNote, this will require IAVL to be updated to not rely or assume on any batch\nbeing present during `SaveVersion`.\n\n## Consequences\n\nAs a result of a new store V2 package, we should expect to see improved performance\nfor queries and transactions due to the separation of concerns. We should also\nexpect to see improved developer UX around experimentation of commitment schemes\nand storage backends for further performance, in addition to a reduced amount of\nabstraction around KVStores making operations such as caching and state branching\nmore intuitive.\n\nHowever, due to the proposed design, there are drawbacks around providing state\nproofs for historical queries.\n\n### Backwards Compatibility\n\nThis ADR proposes changes to the storage implementation in the Cosmos SDK through\nan entirely new package. Interfaces may be borrowed and extended from existing\ntypes that exist in `store`, but no existing implementations or interfaces will\nbe broken or modified.\n\n### Positive\n\n* Improved performance of independent SS and SC layers\n* Reduced layers of abstraction making storage primitives easier to understand\n* Atomic commitments for SC\n* Redesign of storage types and interfaces will allow for greater experimentation\n  such as different physical storage backends and different commitment schemes\n  for different application modules\n\n### Negative\n\n* Providing proofs for historical state is challenging\n\n### Neutral\n\n* Keeping IAVL as the primary commitment data structure, although drastic\n  performance improvements are being made\n\n## Further Discussions\n\n### Module Storage Control\n\nMany modules store secondary indexes that are typically solely used to support\nclient queries, but are actually not needed for the state machine's state\ntransitions. What this means is that these indexes technically have no reason to\nexist in the SC layer at all, as they take up unnecessary space. It is worth\nexploring what an API would look like to allow modules to indicate what (key, value)\npairs they want to be persisted in the SC layer, implicitly indicating the SS\nlayer as well, as opposed to just persisting the (key, value) pair only in the\nSS layer.\n\n### Historical State Proofs\n\nIt is not clear what the importance or demand is within the community of providing\ncommitment proofs for historical state. While solutions can be devised such as\nrebuilding trees on the fly based on state snapshots, it is not clear what the\nperformance implications are for such solutions.\n\n### Physical DB Backends\n\nThis ADR proposes usage of RocksDB to utilize user-defined timestamps as a\nversioning mechanism. However, other physical DB backends are available that may\noffer alternative ways to implement versioning while also providing performance\nimprovements over RocksDB. E.g. PebbleDB supports MVCC timestamps as well, but\nwe'll need to explore how PebbleDB handles compaction and state growth over time.\n\n## References\n\n* [1] https://github.com/cosmos/iavl/pull/676\n* [2] https://github.com/cosmos/iavl/pull/664\n* [3] https://github.com/cosmos/cosmos-sdk/issues/14990"
  },
  {
    "number": 68,
    "filename": "adr-068-preblock.md",
    "title": "ADR 068: Preblock",
    "content": "# ADR 068: Preblock\n\n## Changelog\n\n* Sept 13, 2023: Initial Draft\n\n## Status\n\nDRAFT\n\n## Abstract\n\nIntroduce `PreBlock`, which runs before the begin blocker of other modules, and allows modifying consensus parameters, and the changes are visible to the following state machine logics.\n\n## Context\n\nWhen upgrading to sdk 0.47, the storage format for consensus parameters changed, but in the migration block, `ctx.ConsensusParams()` is always `nil`, because it fails to load the old format using new code, it's supposed to be migrated by the `x/upgrade` module first, but unfortunately, the migration happens in `BeginBlocker` handler, which runs after the `ctx` is initialized.\nWhen we try to solve this, we find the `x/upgrade` module can't modify the context to make the consensus parameters visible for the other modules, the context is passed by value, and sdk team want to keep it that way, that's good for isolation between modules.\n\n## Alternatives\n\nThe first alternative solution introduced a `MigrateModuleManager`, which only includes the `x/upgrade` module right now, and baseapp will run their `BeginBlocker`s before the other modules, and reload context's consensus parameters in between.\n\n## Decision\n\nSuggested this new lifecycle method.\n\n### `PreBlocker`\n\nThere are two semantics around the new lifecycle method:\n\n* It runs before the `BeginBlocker` of all modules\n* It can modify consensus parameters in storage, and signal the caller through the return value.\n\nWhen it returns `ConsensusParamsChanged=true`, the caller must refresh the consensus parameters in the finalize context:\n\n```\napp.finalizeBlockState.ctx = app.finalizeBlockState.ctx.WithConsensusParams(app.GetConsensusParams())\n```\n\nThe new ctx must be passed to all the other lifecycle methods.\n\n\n## Consequences\n\n### Backwards Compatibility\n\n### Positive\n\n### Negative\n\n### Neutral\n\n## Further Discussions\n\n## Test Cases\n\n## References\n\n* [1] https://github.com/cosmos/cosmos-sdk/issues/16494\n* [2] https://github.com/cosmos/cosmos-sdk/pull/16583\n* [3] https://github.com/cosmos/cosmos-sdk/pull/17421\n* [4] https://github.com/cosmos/cosmos-sdk/pull/17713"
  },
  {
    "number": 70,
    "filename": "adr-070-unordered-account.md",
    "title": "ADR 070: Unordered Transactions",
    "content": "# ADR 070: Unordered Transactions\n\n## Changelog\n\n* Dec 4, 2023: Initial Draft (@yihuang, @tac0turtle, @alexanderbez)\n* Jan 30, 2024: Include section on deterministic transaction encoding\n* Mar 18, 2025: Revise implementation to use Cosmos SDK KV Store and require unique timeouts per-address (@technicallyty)\n* Apr 25, 2025: Add note about rejecting unordered txs with sequence values.\n\n## Status\n\nACCEPTED Not Implemented\n\n## Abstract\n\nWe propose a way to do replay-attack protection without enforcing the order of\ntransactions and without requiring the use of monotonically increasing sequences. Instead, we propose\nthe use of a time-based, ephemeral sequence.\n\n## Context\n\nAccount sequence values serve to prevent replay attacks and ensure transactions from the same sender are included in blocks and executed\nin sequential order. Unfortunately, this makes it difficult to reliably send many concurrent transactions from the\nsame sender. Victims of such limitations include IBC relayers and crypto exchanges.\n\n## Decision\n\nWe propose adding a boolean field `unordered` and a google.protobuf.Timestamp field `timeout_timestamp` to the transaction body.\n\nUnordered transactions will bypass the traditional account sequence rules and follow the rules described\nbelow, without impacting traditional ordered transactions which will follow the same sequence rules as before.\n\nWe will introduce new storage of time-based, ephemeral unordered sequences using the SDK's existing KV Store library. \nSpecifically, we will leverage the existing x/auth KV store to store the unordered sequences.\n\nWhen an unordered transaction is included in a block, a concatenation of the `timeout_timestamp` and sender’s address bytes\nwill be recorded to state (i.e. `542939323/<address_bytes>`). In cases of multi-party signing, one entry per signer\nwill be recorded to state.\n\nNew transactions will be checked against the state to prevent duplicate submissions. To prevent the state from growing indefinitely, we propose the following:\n\n* Define an upper bound for the value of `timeout_timestamp` (i.e. 10 minutes).\n* Add PreBlocker method to x/auth that removes state entries with a `timeout_timestamp` earlier than the current block time.\n\n### Transaction Format\n\n```protobuf\nmessage TxBody {\n  ...\n          \n  bool unordered = 4;\n  google.protobuf.Timestamp timeout_timestamp = 5;\n}\n```\n\n### Replay Protection\n\nWe facilitate replay protection by storing the unordered sequence in the Cosmos SDK KV store. Upon transaction ingress, we check if the transaction's unordered\nsequence exists in state, or if the TTL value is stale, i.e. before the current block time. If so, we reject it. Otherwise,\nwe add the unordered sequence to the state. This section of the state will belong to the `x/auth` module.\n\nThe state is evaluated during x/auth's `PreBlocker`. All transactions with an unordered sequence earlier than the current block time\nwill be deleted.\n\n```go\nfunc (am AppModule) PreBlock(ctx context.Context) (appmodule.ResponsePreBlock, error) {\n\terr := am.accountKeeper.RemoveExpired(sdk.UnwrapSDKContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &sdk.ResponsePreBlock{ConsensusParamsChanged: false}, nil\n}\n```\n\n```golang\npackage keeper\n\nimport (\n\tsdk \"github.com/cosmos/cosmos-sdk/types\"\n\n\t\"cosmossdk.io/collections\"\n\t\"cosmossdk.io/core/store\"\n)\n\nvar (\n\t// just arbitrarily picking some upper bound number.\n\tunorderedSequencePrefix = collections.NewPrefix(90)\n)\n\ntype AccountKeeper struct {\n\t// ...\n\tunorderedSequences collections.KeySet[collections.Pair[uint64, []byte]]\n}\n\nfunc (m *AccountKeeper) Contains(ctx sdk.Context, sender []byte, timestamp uint64) (bool, error) {\n\treturn m.unorderedSequences.Has(ctx, collections.Join(timestamp, sender))\n}\n\nfunc (m *AccountKeeper) Add(ctx sdk.Context, sender []byte, timestamp uint64) error {\n\treturn m.unorderedSequences.Set(ctx, collections.Join(timestamp, sender))\n}\n\nfunc (m *AccountKeeper) RemoveExpired(ctx sdk.Context) error {\n\tblkTime := ctx.BlockTime().UnixNano()\n\tit, err := m.unorderedSequences.Iterate(ctx, collections.NewPrefixUntilPairRange[uint64, []byte](uint64(blkTime)))\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer it.Close()\n\n\tkeys, err := it.Keys()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor _, key := range keys {\n\t\tif err := m.unorderedSequences.Remove(ctx, key); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n```\n\n### AnteHandler Decorator\n\nTo facilitate bypassing nonce verification, we must modify the existing\n`IncrementSequenceDecorator` AnteHandler decorator to skip the nonce verification\nwhen the transaction is marked as unordered.\n\n```golang\nfunc (isd IncrementSequenceDecorator) AnteHandle(ctx sdk.Context, tx sdk.Tx, simulate bool, next sdk.AnteHandler) (sdk.Context, error) {\n  if tx.UnOrdered() {\n    return next(ctx, tx, simulate)\n  }\n\n  // ...\n}\n```\n\nWe also introduce a new decorator to perform the unordered transaction verification.\n\n```golang\npackage ante\n\nimport (\n\t\"slices\"\n\t\"strings\"\n\t\"time\"\n\n\tsdk \"github.com/cosmos/cosmos-sdk/types\"\n\tsdkerrors \"github.com/cosmos/cosmos-sdk/types/errors\"\n\tauthkeeper \"github.com/cosmos/cosmos-sdk/x/auth/keeper\"\n\tauthsigning \"github.com/cosmos/cosmos-sdk/x/auth/signing\"\n\n\terrorsmod \"cosmossdk.io/errors\"\n)\n\nvar _ sdk.AnteDecorator = (*UnorderedTxDecorator)(nil)\n\n// UnorderedTxDecorator defines an AnteHandler decorator that is responsible for\n// checking if a transaction is intended to be unordered and, if so, evaluates\n// the transaction accordingly. An unordered transaction will bypass having its\n// nonce incremented, which allows fire-and-forget transaction broadcasting,\n// removing the necessity of ordering on the sender-side.\n//\n// The transaction sender must ensure that unordered=true and a timeout_height\n// is appropriately set. The AnteHandler will check that the transaction is not\n// a duplicate and will evict it from state when the timeout is reached.\n//\n// The UnorderedTxDecorator should be placed as early as possible in the AnteHandler\n// chain to ensure that during DeliverTx, the transaction is added to the unordered sequence state.\ntype UnorderedTxDecorator struct {\n\t// maxUnOrderedTTL defines the maximum TTL a transaction can define.\n\tmaxTimeoutDuration time.Duration\n\ttxManager          authkeeper.UnorderedTxManager\n}\n\nfunc NewUnorderedTxDecorator(\n\tutxm authkeeper.UnorderedTxManager,\n) *UnorderedTxDecorator {\n\treturn &UnorderedTxDecorator{\n\t\tmaxTimeoutDuration: 10 * time.Minute,\n\t\ttxManager:          utxm,\n\t}\n}\n\nfunc (d *UnorderedTxDecorator) AnteHandle(\n\tctx sdk.Context,\n\ttx sdk.Tx,\n\t_ bool,\n\tnext sdk.AnteHandler,\n) (sdk.Context, error) {\n\tif err := d.ValidateTx(ctx, tx); err != nil {\n\t\treturn ctx, err\n\t}\n\treturn next(ctx, tx, false)\n}\n\nfunc (d *UnorderedTxDecorator) ValidateTx(ctx sdk.Context, tx sdk.Tx) error {\n\tunorderedTx, ok := tx.(sdk.TxWithUnordered)\n\tif !ok || !unorderedTx.GetUnordered() {\n\t\t// If the transaction does not implement unordered capabilities or has the\n\t\t// unordered value as false, we bypass.\n\t\treturn nil\n\t}\n\n\tblockTime := ctx.BlockTime()\n\ttimeoutTimestamp := unorderedTx.GetTimeoutTimeStamp()\n\tif timeoutTimestamp.IsZero() || timeoutTimestamp.Unix() == 0 {\n\t\treturn errorsmod.Wrap(\n\t\t\tsdkerrors.ErrInvalidRequest,\n\t\t\t\"unordered transaction must have timeout_timestamp set\",\n\t\t)\n\t}\n\tif timeoutTimestamp.Before(blockTime) {\n\t\treturn errorsmod.Wrap(\n\t\t\tsdkerrors.ErrInvalidRequest,\n\t\t\t\"unordered transaction has a timeout_timestamp that has already passed\",\n\t\t)\n\t}\n\tif timeoutTimestamp.After(blockTime.Add(d.maxTimeoutDuration)) {\n\t\treturn errorsmod.Wrapf(\n\t\t\tsdkerrors.ErrInvalidRequest,\n\t\t\t\"unordered tx ttl exceeds %s\",\n\t\t\td.maxTimeoutDuration.String(),\n\t\t)\n\t}\n\n\texecMode := ctx.ExecMode()\n\tif execMode == sdk.ExecModeSimulate {\n\t\treturn nil\n\t}\n\n\tsignerAddrs, err := getSigners(tx)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\n\tfor _, signer := range signerAddrs {\n\t\tcontains, err := d.txManager.Contains(ctx, signer, uint64(unorderedTx.GetTimeoutTimeStamp().Unix()))\n\t\tif err != nil {\n\t\t\treturn errorsmod.Wrap(\n\t\t\t\tsdkerrors.ErrIO,\n\t\t\t\t\"failed to check contains\",\n\t\t\t)\n\t\t}\n\t\tif contains {\n\t\t\treturn errorsmod.Wrapf(\n\t\t\t\tsdkerrors.ErrInvalidRequest,\n\t\t\t\t\"tx is duplicated for signer %x\", signer,\n\t\t\t)\n\t\t}\n\n\t\tif err := d.txManager.Add(ctx, signer, uint64(unorderedTx.GetTimeoutTimeStamp().Unix())); err != nil {\n\t\t\treturn errorsmod.Wrap(\n\t\t\t\tsdkerrors.ErrIO,\n\t\t\t\t\"failed to add unordered sequence to state\",\n\t\t\t)\n\t\t}\n    }\n\t\n\t\n\treturn nil\n}\n\nfunc getSigners(tx sdk.Tx) ([][]byte, error) {\n\tsigTx, ok := tx.(authsigning.SigVerifiableTx)\n\tif !ok {\n\t\treturn nil, errorsmod.Wrap(sdkerrors.ErrTxDecode, \"invalid tx type\")\n\t}\n\treturn sigTx.GetSigners()\n}\n\n```\n\n### Unordered Sequences\n\nUnordered sequences provide a simple, straightforward mechanism to protect against both transaction malleability and\ntransaction duplication. It is important to note that the unordered sequence must still be unique. However,\nthe value is not required to be strictly increasing as with regular sequences, and the order in which the node receives\nthe transactions no longer matters. Clients can handle building unordered transactions similarly to the code below:\n\n```go\nfor _, tx := range txs {\n\ttx.SetUnordered(true)\n\ttx.SetTimeoutTimestamp(time.Now() + 1 * time.Nanosecond)\n}\n```\n\nWe will reject transactions that have both sequence and unordered timeouts set. We do this to avoid assuming the intent of the user.\n\n### State Management\n\nThe storage of unordered sequences will be facilitated using the Cosmos SDK's KV Store service.\n\n## Note On Previous Design Iteration\n\nThe previous iteration of unordered transactions worked by using an ad-hoc state-management system that posed severe \nrisks and a vector for duplicated tx processing. It relied on graceful app closure which would flush the current state\nof the unordered sequence mapping. If 2/3 of the network crashed, and the graceful closure did not trigger, \nthe system would lose track of all sequences in the mapping, allowing those transactions to be replayed. The \nimplementation proposed in the updated version of this ADR solves this by writing directly to the Cosmos KV Store.\nWhile this is less performant, for the initial implementation, we opted to choose a safer path and postpone performance optimizations until we have more data on real-world impacts and a more battle-tested approach to optimization.\n\nAdditionally, the previous iteration relied on using hashes to create what we call an \"unordered sequence.\" There are known\nissues with transaction malleability in Cosmos SDK signing modes. This ADR gets away from this problem by enforcing\nsingle-use unordered nonces, instead of deriving nonces from bytes in the transaction.\n\n## Consequences\n\n### Positive\n\n* Support unordered transaction inclusion, enabling the ability to \"fire and forget\" many transactions at once.\n\n### Negative\n\n* Requires additional storage overhead.\n* Requirement of unique timestamps per transaction causes a small amount of additional overhead for clients. Clients must ensure each transaction's timeout timestamp is different. However, nanosecond differentials suffice.\n* Usage of Cosmos SDK KV store is slower in comparison to using a non-merkleized store or ad-hoc methods, and block times may slow down as a result.\n\n## References\n\n* https://github.com/cosmos/cosmos-sdk/issues/13009"
  },
  {
    "number": 76,
    "filename": "adr-076-tx-malleability.md",
    "title": "# Cosmos SDK Transaction Malleability Risk Review and Recommendations",
    "content": "# Cosmos SDK Transaction Malleability Risk Review and Recommendations\n\n## Changelog\n\n* 2025-03-10: Initial draft (@aaronc)\n\n## Status\n\nPROPOSED: Not Implemented\n\n## Abstract\n\nSeveral encoding and sign mode related issues have historically resulted in the possibility\nthat Cosmos SDK transactions may be re-encoded in such a way as to change their hash\n(and in rare cases, their meaning) without invalidating the signature.\nThis document details these cases, their potential risks, the extent to which they have been\naddressed, and provides recommendations for future improvements.\n\n## Review\n\nOne naive assumption about Cosmos SDK transactions is that hashing the raw bytes of a submitted transaction creates a safe unique identifier for the transaction. In reality, there are multiple ways in which transactions could be manipulated to create different transaction bytes (and as a result different hashes) that still pass signature verification.\n\nThis document attempts to enumerate the various potential transaction \"malleability\" risks that we have identified and the extent to which they have or have not been addressed in various sign modes. We also identify vulnerabilities that could be introduced if developers make changes in the future without careful consideration of the complexities involved with transaction encoding, sign modes and signatures.\n\n### Risks Associated with Malleability\n\nThe malleability of transactions poses the following potential risks to end users:\n\n* unsigned data could get added to transactions and be processed by state machines\n* clients often rely on transaction hashes for checking transaction status, but whether or not submitted transaction hashes match processed transaction hashes depends primarily on good network actors rather than fundamental protocol guarantees\n* transactions could potentially get executed more than once (faulty replay protection)\n\nIf a client generates a transaction, keeps a record of its hash and then attempts to query nodes to check the transaction's status, this process may falsely conclude that the transaction had not been processed if an intermediary\nprocessor decoded and re-encoded the transaction with different encoding rules (either maliciously or unintentionally).\nAs long as no malleability is present in the signature bytes themselves, clients _should_ query transactions by signature instead of hash.\n\nNot being cognizant of this risk may lead clients to submit the same transaction multiple times if they believe that \nearlier transactions had failed or gotten lost in processing.\nThis could be an attack vector against users if wallets primarily query transactions by hash.\n\nIf the state machine were to rely on transaction hashes as a replay mechanism itself, this would be faulty and not \nprovide the intended replay protection. Instead, the state machine should rely on deterministic representations of\ntransactions rather than the raw encoding, or other nonces,\nif they want to provide some replay protection that doesn't rely on a monotonically\nincreasing account sequence number.\n\n\n### Sources of Malleability\n\n#### Non-deterministic Protobuf Encoding\n\nCosmos SDK transactions are encoded using protobuf binary encoding when they are submitted to the network. Protobuf binary is not inherently a deterministic encoding meaning that the same logical payload could have several valid bytes representations. In a basic sense, this means that protobuf in general can be decoded and re-encoded to produce a different byte stream (and thus different hash) without changing the logical meaning of the bytes. [ADR 027: Deterministic Protobuf Serialization](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-027-deterministic-protobuf-serialization.md) describes in detail what needs to be done to produce what we consider to be a \"canonical\", deterministic protobuf serialization. Briefly, the following sources of malleability at the encoding level have been identified and are addressed by this specification:\n\n* fields can be emitted in any order\n* default field values can be included or omitted, and this doesn't change meaning unless `optional` is used\n* `repeated` fields of scalars may use packed or \"regular\" encoding\n* `varint`s can include extra ignored bits\n* extra fields may be added and are usually simply ignored by decoders. [ADR 020](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-020-protobuf-transaction-encoding.md#unknown-field-filtering) specifies that in general such extra fields should cause messages and transactions to be rejected\n\nWhen using `SIGN_MODE_DIRECT` none of the above malleabilities will be tolerated because:\n\n* signatures of messages and extensions must be done over the raw encoded bytes of those fields\n* the outer tx envelope (`TxRaw`) must follow ADR 027 rules or be rejected\n\nTransactions signed with `SIGN_MODE_LEGACY_AMINO_JSON`, however, have no way of protecting against the above malleabilities because what is signed is a JSON representation of the logical contents of the transaction. These logical contents could have any number of valid protobuf binary encodings, so in general there are no guarantees regarding transaction hash with Amino JSON signing.\n\nIn addition to being aware of the general non-determinism of protobuf binary, developers need to pay special attention to make sure that unknown protobuf fields get rejected when developing new capabilities related to protobuf transactions. The protobuf serialization format was designed with the assumption that unknown data known to encoders could safely be ignored by decoders. This assumption may have been fairly safe within the walled garden of Google's centralized infrastructure. However, in distributed blockchain systems, this assumption is generally unsafe. If a newer client encodes a protobuf message with data intended for a newer server, it is not safe for an older server to simply ignore and discard instructions that it does not understand. These instructions could include critical information that the transaction signer is relying upon and just assuming that it is unimportant is not safe.\n\n[ADR 020](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-020-protobuf-transaction-encoding.md#unknown-field-filtering) specifies some provisions for \"non-critical\" fields which can safely be ignored by older servers. In practice, I have not seen any valid usages of this. It is something in the design that maintainers should be aware of, but it may not be necessary or even 100% safe.\n\n#### Non-deterministic Value Encoding\n\nIn addition to the non-determinism present in protobuf binary itself, some protobuf field data is encoded using a micro-format which itself may not be deterministic. Consider for instance integer or decimal encoding. Some decoders may allow for the presence of leading or trailing zeros without changing the logical meaning, ex. `00100` vs `100` or `100.00` vs `100`. So if a sign mode encodes numbers deterministically, but decoders accept multiple representations,\na user may sign over the value `100` while `0100` gets encoded. This would be possible with Amino JSON to the extent that the integer decoder accepts leading zeros. I believe the current `Int` implementation will reject this, however, it is\nprobably possible to encode an octal or hexadecimal representation in the transaction whereas the user signs over a decimal integer.\n\n#### Signature Encoding\n\nSignatures themselves are encoded using a micro-format specific to the signature algorithm being used and sometimes these\nmicro-formats can allow for non-determinism (multiple valid bytes for the same signature).\nMost of the signature algorithms supported by the SDK should reject non-canonical bytes in their current implementation.\nHowever, the `Multisignature` protobuf type uses normal protobuf encoding and there is no check as to whether the\ndecoded bytes followed canonical ADR 027 rules or not. Therefore, multisig transactions can have malleability in\ntheir signatures.\nAny new or custom signature algorithms must make sure that they reject any non-canonical bytes, otherwise even\nwith `SIGN_MODE_DIRECT` there can be transaction hash malleability by re-encoding signatures with a non-canonical\nrepresentation.\n\n#### Fields not covered by Amino JSON\n\nAnother area that needs to be addressed carefully is the discrepancy between `AminoSignDoc` (see [`aminojson.proto`](../../x/tx/signing/aminojson/internal/aminojsonpb/aminojson.proto)) used for `SIGN_MODE_LEGACY_AMINO_JSON` and the actual contents of `TxBody` and `AuthInfo` (see [`tx.proto`](../../proto/cosmos/tx/v1beta1/tx.proto)).\nIf fields get added to `TxBody` or `AuthInfo`, they must either have a corresponding representation in `AminoSignDoc` or Amino JSON signatures must be rejected when those new fields are set. Making sure that this is done is a\nhighly manual process, and developers could easily make the mistake of updating `TxBody` or `AuthInfo`\nwithout paying any attention to the implementation of `GetSignBytes` for Amino JSON. This is a critical\nvulnerability in which unsigned content can now get into the transaction and signature verification will\npass.\n\n## Sign Mode Summary and Recommendations\n\nThe sign modes officially supported by the SDK are `SIGN_MODE_DIRECT`, `SIGN_MODE_TEXTUAL`, `SIGN_MODE_DIRECT_AUX`,\nand `SIGN_MODE_LEGACY_AMINO_JSON`.\n`SIGN_MODE_LEGACY_AMINO_JSON` is used commonly by wallets and is currently the only sign mode supported on Nano Ledger hardware devices\n(although `SIGN_MODE_TEXTUAL` was designed to also support hardware devices).\n`SIGN_MODE_DIRECT` is the simplest sign mode and its usage is also fairly common.\n`SIGN_MODE_DIRECT_AUX` is a variant of `SIGN_MODE_DIRECT` that can be used by auxiliary signers in a multi-signer\ntransaction by those signers who are not paying gas.\n`SIGN_MODE_TEXTUAL` was intended as a replacement for `SIGN_MODE_LEGACY_AMINO_JSON`, but as far as we know it\nhas not been adopted by any clients yet and thus is not in active use.\n\nAll known malleability concerns have been addressed in the current implementation of `SIGN_MODE_DIRECT`.\nThe only known malleability that could occur with a transaction signed with `SIGN_MODE_DIRECT` would\nneed to be in the signature bytes themselves.\nSince signatures are not signed over, it is impossible for any sign mode to address this directly\nand instead signature algorithms need to take care to reject any non-canonically encoded signature bytes\nto prevent malleability.\nFor the known malleability of the `Multisignature` type, we should make sure that any valid signatures\nwere encoded following canonical ADR 027 rules when doing signature verification.\n\n`SIGN_MODE_DIRECT_AUX` provides the same level of safety as `SIGN_MODE_DIRECT` because\n\n* the raw encoded `TxBody` bytes are signed over in `SignDocDirectAux`, and\n* a transaction using `SIGN_MODE_DIRECT_AUX` still requires the primary signer to sign the transaction with `SIGN_MODE_DIRECT`\n\n`SIGN_MODE_TEXTUAL` also provides the same level of safety as `SIGN_MODE_DIRECT` because the hash of the raw encoded\n`TxBody` and `AuthInfo` bytes are signed over.\n\nUnfortunately, the vast majority of unaddressed malleability risks affect `SIGN_MODE_LEGACY_AMINO_JSON` and this\nsign mode is still commonly used.\nIt is recommended that the following improvements be made to Amino JSON signing:\n\n* hashes of `TxBody` and `AuthInfo` should be added to `AminoSignDoc` so that encoding-level malleability is addressed\n* when constructing `AminoSignDoc`, [protoreflect](https://pkg.go.dev/google.golang.org/protobuf/reflect/protoreflect) API should be used to ensure that there are no fields in `TxBody` or `AuthInfo` which do not have a mapping in `AminoSignDoc` have been set\n* fields present in `TxBody` or `AuthInfo` that are not present in `AminoSignDoc` (such as extension options) should\nbe added to `AminoSignDoc` if possible\n\n## Testing\n\nTo test that transactions are resistant to malleability,\nwe can develop a test suite to run against all sign modes that\nattempts to manipulate transaction bytes in the following ways:\n\n* changing protobuf encoding by\n    * reordering fields\n    * setting default values\n    * adding extra bits to varints, or\n    * setting new unknown fields\n* modifying integer and decimal values encoded as strings with leading or trailing zeros\n\nWhenever any of these manipulations is done, we should observe that the sign doc bytes for the sign mode being\ntested also change, meaning that the corresponding signatures will also have to change.\n\nIn the case of Amino JSON, we should also develop tests which ensure that if any `TxBody` or `AuthInfo`\nfield not supported by Amino's `AminoSignDoc` is set that signing fails.\n\nIn the general case of transaction decoding, we should have unit tests to ensure that\n\n* any `TxRaw` bytes which do not follow ADR 027 canonical encoding cause decoding to fail, and\n* any top-level transaction elements including `TxBody`, `AuthInfo`, public keys, and messages which\nhave unknown fields set cause the transaction to be rejected\n(this ensures that ADR 020 unknown field filtering is properly applied)\n\nFor each supported signature algorithm,\nthere should also be unit tests to ensure that signatures must be encoded canonically\nor get rejected.\n\n## References\n\n* [ADR 027: Deterministic Protobuf Serialization](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-027-deterministic-protobuf-serialization.md)\n* [ADR 020](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-020-protobuf-transaction-encoding.md#unknown-field-filtering)\n* [`aminojson.proto`](../../x/tx/signing/aminojson/internal/aminojsonpb/aminojson.proto)\n* [`tx.proto`](../../proto/cosmos/tx/v1beta1/tx.proto)"
  }
]